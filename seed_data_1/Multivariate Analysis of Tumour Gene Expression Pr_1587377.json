{
    "acceptedDate": "",
    "authors": [
        {
            "name": "Zucknick, Manuela"
        },
        {
            "name": "Zucknick, Manuela"
        }
    ],
    "contributors": [
        "Gabra, Hani",
        "Richardson, Sylvia",
        "Wellcome Trust"
    ],
    "createdDate": "2012-04-12T16:31:24+01:00",
    "dataProvider": {
        "id": 105,
        "name": "Spiral - Imperial College Digital Repository",
        "url": "https://api.core.ac.uk/v3/data-providers/105",
        "logo": "https://api.core.ac.uk/data-providers/105/logo"
    },
    "depositedDate": "",
    "documentType": "research",
    "doi": "10.25560/4397",
    "downloadUrl": "https://core.ac.uk/download/1587377.pdf",
    "fullText": "MULTIVARIATE ANALYSIS OFTUMOUR GENE EXPRESSION PROFILESAPPLYING REGULARISATION ANDBAYESIAN VARIABLE SELECTION TECHNIQUESbyManuela ZucknickA thesis submitted to Imperial College London for the Degree ofDoctor of PhilosophyDecember 2008Imperial College LondonDivision of Epidemiology, Public Health and Primary CareDepartment of Epidemiology and Public HealthLondon, W2 1PG1AbstractHigh-throughput microarray technology is here to stay, e.g. in oncology for tumour classifica-tion and gene expression profiling to predict cancer pathology and clinical outcome. The globalobjective of this thesis is to investigate multivariate methods that are suitable for this task.After introducing the problem and the biological background, an overview of multivariateregularisation methods is given in Chapter 3 and the binary classification problem is outlined(Chapter 4). The focus of applications presented in Chapters 5 to 7 is on sparse binary classi-fiers that are both parsimonious and interpretable. Particular emphasis is on sparse penalisedlikelihood and Bayesian variable selection models, all in the context of logistic regression. Thethesis concludes with a final discussion chapter.The variable selection problem is particularly challenging here, since the number of vari-ables is much larger than the sample size, which results in an ill-conditioned problem withmany equally good solutions. Thus, one open problem is the stability of gene expression pro-files. In a resampling study, various characteristics including stability are compared between avariety of classifiers applied to five gene expression data sets and validated on two independentdata sets.Bayesian variable selection provides an alternative to resampling for estimating the un-certainty in the selection of genes. MCMC methods are used for model space exploration, butbecause of the high dimensionality standard algorithms are computationally expensive and/orresult in poor Markov chain mixing. A novel MCMC algorithm is presented that uses thedependence structure between input variables for finding blocks of variables to be updated to-gether. This drastically improves mixing while keeping the computational burden acceptable.Several algorithms are compared in a simulation study. In an ovarian cancer application inChapter 7, the best-performing MCMC algorithms are combined with parallel tempering andcompared with an alternative method.2DeclarationI hereby declare that this submission is my own work and that, to the best of my knowledge,it contains no material previously published or written by another person, except where dueacknowledgment has been made in the text.Manuela Zucknick December 18, 20083AcknowledgementsI thank my supervisor Prof Sylvia Richardson for her excellent guidance and advice, and forlots of encouragement and support.I would also like to thank my second supervisor Prof Hani Gabra for welcoming me into hisgroup. I thoroughly enjoyed working with him and the group, and gaining lots of experienceon the way.I thank Dr Chris Holmes, Dr Leonardo Bottolo, Dr Clive Hoggart and everybody else withwhom I had valuable discussions about issues surrounding Bayesian variable selection andstochastic search algorithms.My special thanks and gratitude go to the friends and colleagues at Imperial College and else-where who have helped me in these years, especially to my fellow PhD students at EPH formutual support and for having made this such a fun experience.Finally, I thank my family for their support and understanding, and Marc for always standingright beside me, even when we were apart.This work was funded by the Wellcome Trust through a four year PhD programme in Bioinfor-matics at Imperial College London.4Contents1 Introduction 132 Molecular biology and clinical background 163 Multivariate regularisation and dimension reduction methods 203.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203.2 Regularisation via shrinkage . . . . . . . . . . . . . . . . . . . . . . . . . . . 233.2.1 Penalised likelihood methods . . . . . . . . . . . . . . . . . . . . . . 233.2.2 Methods with derived input directions . . . . . . . . . . . . . . . . . . 343.2.3 Finding the regularisation solutions . . . . . . . . . . . . . . . . . . . 433.3 Variable selection by stochastic search . . . . . . . . . . . . . . . . . . . . . . 513.3.1 Bayesian variable selection models . . . . . . . . . . . . . . . . . . . 523.3.2 Speeding up the search algorithm: moving beyond a single Markov chain 664 Binary classification 754.1 Statistical models for binary classification . . . . . . . . . . . . . . . . . . . . 754.2 Classification based on univariate statistics . . . . . . . . . . . . . . . . . . . . 814.3 Model fitting and model assessment . . . . . . . . . . . . . . . . . . . . . . . 835 A resampling study to assess characteristics of gene expression profiles 885.1 Resampling study setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 895.2 Classification methods and software . . . . . . . . . . . . . . . . . . . . . . . 895.3 Assessing the instability of molecular profiles . . . . . . . . . . . . . . . . . . 915.4 Data sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 925.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9355.5.1 Prediction accuracy and parsimony . . . . . . . . . . . . . . . . . . . 945.5.2 Profile stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 965.5.3 External validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1015.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1036 MCMC algorithms for Bayesian variable selection in the logistic regression model1056.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1056.1.1 Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1066.2 Estimating the dependence structure . . . . . . . . . . . . . . . . . . . . . . . 1076.3 MCMC samplers for the covariate indicator γ . . . . . . . . . . . . . . . . . . 1126.3.1 Evaluation of the performance of MCMC algorithms . . . . . . . . . . 1146.4 Simulation studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1176.4.1 Simulation scenario 1: generated covariance structure . . . . . . . . . . 1196.4.2 Simulation scenario 2: covariance based on gene expression data . . . . 1306.4.3 Sensitivity analysis for prior variance parameter c2 . . . . . . . . . . . 1396.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1467 Application to ovarian cancer gene expression data 1507.1 The data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1507.2 Further analyses following on from the resampling study . . . . . . . . . . . . 1527.2.1 Five interesting genes and their biological function . . . . . . . . . . . 1537.2.2 Dependence pattern among the five genes . . . . . . . . . . . . . . . . 1567.2.3 Expression levels in independent data . . . . . . . . . . . . . . . . . . 1587.3 Bayesian variable selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1597.3.1 MCMC sampling with and without parallel tempering . . . . . . . . . 1607.3.2 Shotgun stochastic search . . . . . . . . . . . . . . . . . . . . . . . . 1667.4 Summary and discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1688 Discussion and future directions 1718.1 Further improvements in MCMC mixing . . . . . . . . . . . . . . . . . . . . . 1728.2 Incorporating biological knowledge . . . . . . . . . . . . . . . . . . . . . . . 174Bibliography 1766A Bayesian methodology 192A.1 Bayesian modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192A.2 Markov chain Monte Carlo sampling . . . . . . . . . . . . . . . . . . . . . . . 194B Sampling from the Bayesian logistic variable selection model 198B.1 Gibbs sampling algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198B.2 Metropolis-Hastings acceptance probability for sampling from p(βγ, γ|z, λ,X) 200B.3 Sampling from a tempered distribution . . . . . . . . . . . . . . . . . . . . . . 202C Comparing the characteristics of gene expression profiles derived by univariateand multivariate classification methods 205C.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207C.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209C.2.1 Classification methods . . . . . . . . . . . . . . . . . . . . . . . . . . 209C.2.2 Multiple random validation study setup . . . . . . . . . . . . . . . . . 213C.2.3 Assessing the instability of molecular profiles . . . . . . . . . . . . . . 213C.3 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216C.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218C.4.1 Prediction accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218C.4.2 Ranking genes by their profile inclusion frequencies . . . . . . . . . . 221C.4.3 Profile stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223C.4.4 Validation on independent data sets . . . . . . . . . . . . . . . . . . . 228C.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230C.6 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234C.A Similarity indices: results for Schwartz et al. (2002) . . . . . . . . . . 234C.B Extension of Jaccard index to incorporate correlation . . . . . . . . . . 2357List of Tables3.1 Outline of MCMC algorithm for the logistic BVS model by Holmes and Held(2006) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635.1 Resampling study setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 895.2 2× 2 table counting presences and absences in gene sets . . . . . . . . . . . . 915.3 Main characteristics of gene expression microarray data sets . . . . . . . . . . 925.4 Performance of sets of genes found in Schwartz et al. (2002), van’t Veer et al.(2002), when applied in logistic regression models to independent data (Luet al. 2004, van de Vijver et al. 2002) . . . . . . . . . . . . . . . . . . . . . . . 1026.1 MCMC samplers applied to two data sets in both simulation scenarios . . . . . 1186.2 Simulation scenario 1 with generated covariance structure . . . . . . . . . . . . 1206.3 Mixing performance results with respect to γ for scenario 1 over all 25 datasets (10 data sets for Full sampler, resp.) . . . . . . . . . . . . . . . . . . . . 1236.4 Mixing performance results with respect to γ for scenario 1: results for onedata set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1296.5 Simulation scenario 2 based on gene expression data by Schwartz et al. (2002) . 1306.6 Mixing performance results with respect to γ for scenario 2 over all 25 datasets (10 data sets for Full sampler, resp.) . . . . . . . . . . . . . . . . . . . . 1346.7 Mixing performance results with respect to γ for scenario 2: results for onedata set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1386.8 Results of sensitivity analysis regarding the choice of c2 . . . . . . . . . . . . . 1457.1 Histological types, FIGO stages and tumour grades for ovarian cancer samples . 15187.2 Genes which appear in at least 50% of all lasso molecular profiles applied toSchwartz et al. (2002) data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1547.3 Diagnostic measures for Markov chain mixing with respect to γ . . . . . . . . 1647.4 Shotgun stochastic search results . . . . . . . . . . . . . . . . . . . . . . . . . 1687.5 Genes, which either have marginal posterior probability estimates larger than0.0125 in BVS models, or were selected into at least 50% of profiles in theresampling study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169C.1 Resampling study setup for comparison of the characteristics of the classification meth-ods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213C.2 2× 2 table counting presences (1) and absences (0) in gene sets Z1 and Z2. . . . . . 214C.3 Main characteristics of gene expression microarray data sets used. Data sets in italicsand brackets are used as validation data. . . . . . . . . . . . . . . . . . . . . . . . 217C.4 Performance of gene sets found using data sets Schwartz et al. (2002), van’t Veer et al.(2002), when applied in logistic regression models fitted to independent data (Lu et al.2004, van de Vijver et al. 2002). One-sided permutation tests are based on 1000 randomsets of the same number of genes (significance level 0.1). . . . . . . . . . . . . . . . 2299List of Figures3.1 Gaussian and Laplace densities and corresponding shapes of densities of theirproduct with a Gaussian distribution . . . . . . . . . . . . . . . . . . . . . . . 253.2 Forms of prior densities corresponding to the bridge one-variable regressionmodels with q = 1.5 and with q = 0.1 . . . . . . . . . . . . . . . . . . . . . . 273.3 Normal-Jeffreys prior, and normal-exponential-gamma priors with a = 0.5, b =1 and a = 0.5, b = 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303.4 Solution paths for lasso and forward stagewise regression for diabetes data . . . 483.5 Spike-and-slab prior and normal mixture prior . . . . . . . . . . . . . . . . . . 544.1 Predicted class probabilities resulting from logistic and probit regression models 774.2 Linear support vector machine: separating hyperplane in the linearly separablecase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 795.1 Boxplots of predictive performances in terms of proportion of misclassifiedvalidation samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 955.2 Mean Jaccard similarity measures (± standard deviation) plotted against me-dian profile sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 985.3 Distributions of means of absolute correlations within profiles against medianprofile sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 995.4 Inclusion frequencies for genes selected into at least half of all profiles for anymethod (ovarian cancer data) . . . . . . . . . . . . . . . . . . . . . . . . . . . 1006.1 Conditional independence graph for the Schwartz et al. (2002) gene expressiondata set (random subset of 150 probe sets) . . . . . . . . . . . . . . . . . . . . 111106.2 Squared empirical correlation structure imposed on data set 1 in simulationscenario 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1216.3 Trace plots of global parameters model deviance and model size pγ , and traceplots of γ vector for data set 1 in scenario 1 . . . . . . . . . . . . . . . . . . . 1226.4 Ratio of effective sample size and CPU time, and CPU times per 104 iterations,plotted against the threshold level for data sets 1 and 2 in simulation setup 1. . . 1246.5 Posterior inclusion frequencies for variables 1, ..., 10 over all 25 replicates ofsimulation setup 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1266.6 Squared empirical correlation structure of one data set simulated according tosimulation scenario 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1316.7 Trace plots of global parameters model deviance and model size pγ , and traceplots of γ vector for data set 1 in scenario 2 . . . . . . . . . . . . . . . . . . . 1326.8 Ratio of effective sample size and CPU time, and CPU times per 104 iterations,plotted against the threshold level for data sets 1 and 2 in scenario 2 . . . . . . 1336.9 Posterior inclusion frequencies for variables 1, ..., 10 over all 25 replicates ofsimulation setup 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1366.10 Logistic BVS model: trace plots of deviance for add/delete samplers and blocksamplers with prior covariance parameters c2 = 0.5, c2 = 5, and c2 = 50 . . . . 1406.11 Probit BVS model: trace plots of model deviances and model sizes for add/deletesampler with c2 = 0.05 and block samplers with c2 = {0.05, 0.5, 5, 50} . . . . 1427.1 Plot of first versus second principal component of Schwartz et al. (2002) data . 1527.2 Expression levels of the five genes listed in Table 7.2 in Schwartz et al. (2002)and in Lu et al. (2004) data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1567.3 Part of the conditional independence graph involving the five genes in Table7.2 and their direct neighbours . . . . . . . . . . . . . . . . . . . . . . . . . . 1577.4 Trace plots of model deviances for add/delete sampler and block sampler, withand without parallel tempering applied to Schwartz et al. (2002) data . . . . . . 1617.5 Trace plots of γ vector for add/delete sampler and block sampler, with andwithout parallel tempering applied to Schwartz et al. (2002) data . . . . . . . . 1637.6 Log posterior probabilities of best 100,000 shotgun stochastic search models . . 16711C.1 Boxplots of predictive performances in terms of proportion of misclassified valida-tion samples shown for a range of tuning parameter values. The median profile sizes(averaging across the 50 resampling sets) corresponding to each parameter value areindicated for all methods below the corresponding boxplots. The orange lines representthe baseline classification rates, where all samples are assigned to the most frequent class.220C.2 Inclusion frequencies for genes selected into at least half of all profiles by any of themethods for at least one tuning parameter value (ovarian cancer data). Frequenciescorresponding to each of the parameter values are illustrated by overlaid T-bars varyingfrom light gray for the largest profiles to black for the smallest. For example, the barenclosed by the orange box shows gene X03635 being selected in all 50 univariateprofiles of size p∗ = 50 (light gray), in 47 profiles with p∗ = 10 (darker gray) and in43 with p∗ = 5 (black). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222C.3 Mean Jaccard similarity measures (± standard deviation) plotted against median pro-file sizes for univariate filtering, lasso regression, elastic net, and random forest withvariable selection (varSelRF) for the five data sets and the ovarian cancer data withrandomised response (top right). . . . . . . . . . . . . . . . . . . . . . . . . . . . 224C.4 Distributions of means of absolute correlations within profiles (mean ± standard devi-ation) plotted against median profile sizes. . . . . . . . . . . . . . . . . . . . . . . 226C.5 Mean values of similarity measures (± standard deviation) plotted against median pro-file sizes for univariate filtering, lasso regression, elastic net, and random forest withvariable selection (varSelRF) (for legend see Figure C.4). . . . . . . . . . . . . . . . 234C.6 Mean correlation-extended Jaccard similarity measures (± standard deviation) as de-fined above plotted against median profile sizes for univariate filtering, lasso regres-sion, elastic net, and random forest with variable selection (varSelRF) (for legend seeFigure C.4). The point labels indicate the corresponding tuning parameter values. . . . 23612Chapter 1IntroductionHigh-throughput technologies that can take simultaneous measurements of thousands or evenmillions of molecular biological entities have transformed the way, how scientific advancementis pursued in the biological and clinical sciences. An example for such high-throughput tech-nologies are gene expression microarrays, which allow the study of the simultaneous mRNAexpression of (tens of) thousands of genes and the comparison of these gene expressions be-tween different samples and under varying experimental conditions. Of special interest to clini-cians is the use of microarrays to find molecular profiles, which can classify tumours or predictpathological characteristics and clinical outcome of cancer or other complex diseases.The statistical analysis of such gene expression studies is challenging due to the vast num-ber p of genes, versus a relatively small number n of a few dozen or hundred samples. Classicalstatistical methodology usually requires that sample size be substantially larger than the num-ber of variables and hence it is not possible to apply standard statistical models offhand. Inmany previous microarray studies, this problem was circumvented by applying a filter beforethe actual analysis which reduces the number of genes rigorously, in most cases by using asimple univariate measure such as fold-change or correlation coefficients (e.g. van’t Veer et al.2002, Dudoit et al. 2002). Indeed, the expression levels of a relatively large proportion ofgenes will not vary much across tissue samples, either because they are not active in that typeof tissue, or because they are household genes which are not affected by the disease or experi-mental condition under investigation. And it might be a viable approach to eliminate these non-varying genes using simple univariate measures. But after such filtering, several thousand genevariables will remain for which expression levels differ between the conditions, and applying13univariate filtering techniques to further reduce the number of variables means to throw awaymuch information contained in the data. Expression levels are often correlated between genes,for example because the genes have similar functions or act in the same biological pathway.Univariate approaches do not take the correlation structure between covariates into account.It is therefore desirable to implement multivariate methods, that are capable of handling verylarge data sets with thousands of variables. Such methods usually employ dimension reductiontechniques such as variable subset selection imbedded in regression models. Another possibil-ity is to use regularisation methods, where instead of eliminating some variables completely(while retaining the others fully), the influences of all the variables are reduced, i.e. shrunk.The aim of this project is to implement and investigate multivariate regularisation andvariable selection methods, which are suitable for classification analysis with “large p, smalln” data such as gene expression array data.The “large p, small n” (p >> n) nature of gene expression data creates two problems fora statistical analysis. The first problem is that of multi-collinearity: because there are manymore covariates than samples, there are no unique parameter estimates for classical statisticalmodels. This also means that in a variable subset selection problem, many subsets of covariateswill fit the data equally well. One can either enforce a unique solution, which is done by strictlyconvex shrinkage methods like ridge regression. Or one can explore the model space to find asmany of those equally good solutions as possible. This can be done for example by performingvariable selection in a Bayesian context using stochastic search algorithms, which are able tomove between the many local modes in the model space that represent good solutions.The second problem is a computational one and is related to the exploration of the modelspace by stochastic search. Because of the large number of covariates, the estimation of modelparameters can be very demanding computationally, especially in a full Bayesian analysis,where inference about the posterior distribution cannot be done analytically so that simulationapproaches such as Markov chain Monte Carlo (MCMC) have to be used. Chapter 6 of thisthesis focusses on an approach to make posterior inference by MCMC more feasible for large-scale gene expression data by exploiting the substantial correlations among gene variables topropose blocks of (partially) correlated covariates together in a Metropolis-Hastings step withinGibbs sampling.The second main area of work, which is presented here, is concerned with characteris-14tics of gene expression profiles. Throughout this thesis, gene expression profiles - also calledmolecular profiles - are defined as small sets of genes, which have been selected, because theirexpression values are useful for the classification of samples or for predicting outcome for newsamples. So good prediction performance is one desirable characteristic of a molecular pro-file, but it is not the only one. In particular, the potential for biological interpretability is alsoconsidered important, and it is argued that in order to fulfill this potential, a molecular profileshould be both sparse and stable. Stability is a particular concern with p >> n data, becausethe multi-collinearity of the gene expression data implies, that even molecular profiles whichclassify samples perfectly, are expected to vary, when the data are split slightly differently intotraining and validation data sets. In several recent studies (e.g. Michiels et al. 2005, Ein-Doret al. 2006), it has been demonstrated that molecular profiles derived from univariate filteringmethods tend to be highly instable, not just in terms of which genes are selected into the profile,but also with respect to their predictive abilities. As part of this project, a resampling study wasperformed to compare the characteristics of gene lists derived with a variety of multivariateregularisation and variable selection methods as well as standard univariate filtering methods.The findings are described in detail in Zucknick et al. (2008) and are summarised in Chapter 5.This thesis starts with a short overview over the biological and clinical background ofgene expression studies in Chapter 2. In Chapter 3, an extensive literature review is carriedout to provide an overview of the existing multivariate regularisation and variable selectionmethodology and underlying theory. The aim is to present the available methods in a commonframework and to draw connections between them. Most regularisation and variable selectionmethods have been developed primarily in the context of linear regression models and arealso presented in this context in Chapter 3. However, the main interest here is in applyinggene expression data for binary classification. Most of the methodology can be readily castinto a binary classification framework. Some of the most commonly used binary classificationapproaches are described in Chapter 4, together with a short overview over model selectionand model assessment. The following two chapters present the two main areas of this work,which were introduced above. An example application of the investigated techniques followsin Chapter 7, where the methods are applied to an ovarian cancer gene expression data set andtheir results are compared in that context. Finally, a brief discussion of this work is given andfuture directions are outlined.15Chapter 2Molecular biology and clinicalbackgroundCancer is the cause of more than a quarter (27% in 2005) of all deaths in England and Wales,and the second most common cause of death after cardiovascular diseases according to thereport of the Office for National Statistics for the year 2005 (Office for National Statistics2006). It has long been known from family studies that the prevalence of cancer is partlydetermined by genetic factors. For a small proportion of cancers a simple relationship with asingle gene has been found. For example, about 5-10% of all female breast cancers are causedby a single mutation in either the BRCA1 or the BRCA2 gene - and the risk to have developedbreast cancer by the age of 70 is estimated to be between about 40% and 55% for a woman withsuch a germline mutation, according to population-based studies as summarised by Antoniouet al. (2003). In families with multiple cases of breast cancer, the risk to develop breast canceris even higher for a woman with BRCA1 or BRCA2 mutations; estimates from multiple-casefamily studies suggest risks of up to 87% (Antoniou et al. 2003).However, most cancers are now known to have multiple causal factors, including environ-ment and possibly many genes, where each gene on its own only has a small penetrance effect.Tumours develop through a series of mutations in a single cell. Because the human body isequipped with many mechanisms to prevent a cell from turning into a cancer cell, as manyas six or seven genes need to be faulted by mutations in the same cell in order for the cell toovercome the protective mechanisms and turn into a malign cancer cell (e.g. Strachan and Read2004). Genes which can lead to the development of cancer when being mutated are commonly16divided into two groups, proto-oncogenes and tumour suppressor genes. In their normal non-mutated state, proto-oncogenes usually promote cell proliferation. A single mutant allele canturn them into oncogenes, which favour excessive cell proliferation. Tumour suppressor genes,on the other hand, inhibit events leading to cancerous cells. They are usually involved in pre-venting excessive cell proliferation, steering potential cancer cells into apoptosis (cell death),or keeping the mutation rate low by ensuring accurate DNA replication and helping to repairfaulty DNA. Only if both alleles are deactivated due to mutation, the tumour suppressor geneloses its control function, which in turn makes the development of the cell into a cancer cellmore likely. Usually, in familial cancers with high heritability, one of the alleles of an impor-tant tumour suppressor gene is mal-functional due to a germline mutation. This means that inthis case only one somatic mutation, rather than two, is necessary to deactivate the gene in acell. The BRCA1 and BRCA2 genes are both tumour suppressor genes, which are involved inthe repair of damaged DNA. If a woman inherits a germline mutation in one of these genes, ithighly increases her life-time risk of developing breast cancer or ovarian cancer.As a consequence of de-activated DNA-repair and other tumour suppressor genes, cancercells have a drastically altered DNA structure compared to normal DNA. This is reflected in theexpression of genes, many of which are up- or down-regulated compared to their expression innormal cells. For example, the expression of oncogenes is upregulated in cancer cells, whereasthe expression of tumour suppressor genes can be down-regulated. Because of this feature oftumours, advances in the technology of high-throughput molecular diagnostics, allowing forthe simultaneous analysis of many genes at the same time, are very promising for the improve-ment of tumour classification and assessment of prognosis and management of patients withcancer. This and the high prevalence and high mortality rates of cancer in western civilisationsare the main reasons why the research of cancer using molecular diagnostics has a high pri-ority in medical research today. High-throughput technologies are available to measure manydifferent aspects of the genetics and genomics of a tumour compared to normal samples (for anoverview see Strachan and Read 2004). The altered DNA structure of a malignant tumour cellwith many deletions, insertions or inversions of big chunks of DNA can be assessed with com-parative genomic hybridisation (CGH) arrays measuring DNA copy numbers. Whether a geneis switched on or off by methylation of CpG islands is captured by methylation arrays. SNPchips can be used for high-throughput genotyping of known single nucleotide polymorphisms17(SNP’s) to find possible locations for mutations, which increase the prevalence for cancer. Atthe present time, the most common technology, however, are microarrays measuring gene ex-pression in terms of abundance of their transcribed RNA. This project is primarily focussedon the analysis of gene expression arrays, but the statistical issues are the same for all othertypes of high-throughput data. A major concern with the analysis of such data is the need fordimension reduction, because there are data available for many thousands of genes, but theyare usually measured only for a much smaller number of samples for practical and cost-relatedreasons.The importance of gene expression microarrays in today’s clinical investigations, espe-cially for cancer, has been highlighted a few years ago in a special issue of the New EnglandJournal of Medicine (Liu and Karuturi 2004, Grimwade and Haferlach 2004). The issue con-tains two articles (Bullinger et al. 2004, Valk et al. 2004) reporting on the successful applicationof gene expression arrays for the classification of primary acute myeloid leukemia (AML) intosubtypes. One of the most important applications of gene expression microarrays is to findmolecular profiles, i.e. small sets of genes, which allow the classification of samples into tu-mour and non-tumour or into different tumour subtypes on the basis of their gene expressionlevels. These tumour subtypes might otherwise not be distinguishable by classical histologicalmethods, and finding them promises to improve prognosis and helps to individualise treatment.Another common application is to derive molecular profiles, which are able to supplement andimprove existing clinical markers for disease prognosis. Liu and Karuturi (2004) point out thatthere is much confusion among researchers in molecular biology and medicine about how bestto perform such a classification analysis using microarray data. Part of the problem is thatstandard statistical methodology cannot be applied, because of the high dimensionality of thedata. Another problem is that the data are noisy, so that a careful statistical analysis is required,accounting for all sources of variation in the data. The focus of this project is the investiga-tion of statistical methods, which are able to find accurate and stable molecular profiles forclassification, using noisy and high-dimensional data such as gene expression array data.Several microarray platforms exist for measuring gene expression levels. They all have incommon that thousands of gene-specific short cDNA sequences or oligonucleotides are coatedto a chip, either directly to the glass surface (e.g. custom-made spotted cDNA chips, Agilentarrays, or Affymetrix chips) or to microscopic beads (Illumina), which are fixed to the surface18of a chip. The microarray chip is then washed with the prepared RNA sample of interest, whichhas been labelled by a fluorescent tag, so that the abundance of RNA binding to the probes canbe measured by the amount of fluorescence at that position.There are single-channel and dual-channel microarrays. In dual-channel arrays (spottedcDNA chips, Agilent chips) the fluorescence abundance of the hybridised RNA is not measuredabsolutely, but rather in comparison with a reference RNA sample, which is also hybridised tothe microarray and which is labelled with a different fluorescent dye. Usually, a red colour(Cy5) and a green colour (Cy3) are used, and the RNA abundance in a particular spot is mea-sured by the log ratio of the normalised fluorescence intensities of the sample of interest versusthe reference sample.In the single-channel technique (Affymetrix GeneChips, Illumina BeadArrays), only theRNA sample of interest is hybridised to the chip surface, so there is no external referencesample to help control for technical variability in the measured fluorescence intensities. Thedifferent platforms use different approaches to provide internal references to account for thetechnical variability. Affymetrix provides a so-called mismatch (MM) oligonucleotide probe tocomplement each perfect match (PM) oligonucleotide. While the perfect match is designed tomatch the target RNA sequence perfectly, the mismatch oligo has one mismatch nucleotide inthe centre of the 25-nucleotides-long sequence. Fluorescence measured at the mismatch (MM)spots are viewed as background noise and the signal measured at the perfect match (PM) oli-gos is “standardised” by being related to the measured background noise. However, there isno agreement on how to use the PM and MM intensities for deriving a single gene expressionvalue for each RNA molecule. In contrast, the newer Illumina BeadArray technology generateon average 30 copies of each 50mer oligonucleotide probe, resulting in 30 internal technicalreplicates on average, which can be used to reliably estimate the technical variability. In ad-dition, the probes are scattered randomly across the BeadArray chip. This is in contrast toall previous microarray technologies, where the probes were fixed to pre-specified locations,which can give rise to systematic spatial effects on the measured RNA intensities, that have tobe accounted for in the statistical analysis.19Chapter 3Multivariate regularisation and dimensionreduction methods3.1 IntroductionThe task of dimension reduction is generally approached from two different angles, which canbe best explained in the context of a linear regression modely = Xβ + ε, with ε ∼ N(0, σ2In), (3.1)where one tries to explain most of the variability in the response variable y ∈ Rn×1 by alinear combination Xβ of the input matrix X ∈ Rn×p. The goal is to estimate the regressioncoefficient vector β ∈ Rp×1. In a classical statistical situation, the sample size n is much largerthan the number of input variables p and the standard estimator for β, namely the ordinary leastsquares (OLS) estimator β̂ = (XT X)−1XT y, is well-defined. Dimension reduction or anotherkind of regularisation becomes necessary, when there are more input variables than there aresamples, i.e. when p > n.Regularisation arises from the fact that the ordinary least squares estimator does not have aunique solution if the rank of XT X ∈ Rp×p is smaller than p, which is the case for “short, fat”data sets with p >> n. In that situation, the inverse (XT X)−1 does not exist and has thus tobe replaced in the formula for the OLS estimator by the generalised inverse (XT X)−, which isnot unique. This leads to the following formulation for a generalised least squares estimator ofthe regression coefficient vector: β̂ = (XT X)−XT y. Usually, uniqueness is restored by using20the Moore-Penrose inverse (XT X)+, which, among all generalised inverse solutions, leads tothe minimum-length least squares (MLLS) estimate, which is a best linear unbiased estimatorin terms of variance, just like the ordinary least squares solution. However, it turns out that ifp >> n, there are biased estimators which are uniformly better than the MLLS estimator interms of mean squared error.One such biased estimator is the ridge estimator which was first introduced by Hoerland Kennard (1970). They proposed to replace the inverse (XT X)−1 by an approximation(XT X + λI)−1 (where I is the identity matrix), which is guaranteed to exist; thus the ridgeestimator is defined as β̂2 = (XT X + λI)−1XT y. It is biased towards zero, hence the termshrinkage estimator. The parameter λ in ridge regression acts as a penalty parameter on the log-likelihood that is proportional to the sum of squared regression coefficients∑pi=1 β2i (i.e. theL2 norm of β). Other penalised likelihood methods include lasso regression (Tibshirani 1996)with a penalty on the L1 norm of the regression coefficient vector∑pi=1 |βi| , and “bridge”regression (Frank and Friedman 1993) which is a generalisation of ridge and lasso regressionwith a penalty on the Lq norm∑pi=1 |βi|q (0 < q ≤ 2). A different generalisation is theelastic net (Zou and Hastie 2005), which combines ridge regression and the lasso method byentering both L1 and L2 penalty terms. Regularisation methods other than penalised likelihoodmethods include principal components regression (PCR) (Massy 1965) and partial least squaresregression (PLS) (Wold 1975). All methods determine the regression coefficient vector β bymaximising different criteria - subject to some constraints on β. While ordinary least squaresregression maximises the squared correlation between the response variable y and the predictorXβ (maxβ cor2(y, Xβ)), PCR is not concerned with the relationship between response andinput variables and just maximises the variance of the predictor Xβ (maxβ var(Xβ)). PLScombines these two approaches and maximises the product of the squared correlation betweeny and X and the variance within the input space: maxβ (cor2(y, Xβ)var(Xβ)). Ridge regres-sion can also be cast into this framework: maxβ(cor2(y,Xβ) var(Xβ)var(Xβ)+λ)(Frank and Friedman1993).A different approach to dealing with p > n data arises not so much from problems withmulti-collinearity, but from the wish to be able to explain the data by models which can beinterpreted in a parsimonious manner. For example, in the context of gene expression data,there is much interest in deriving molecular profiles, which discriminate well between different21conditions. This means that there is often the objective to build a model from only a smallsubset of all p variables, i.e. instead of shrinking the effect of all variables, one keeps somevariables and discards the rest. A common approach is simply to apply a filtering mechanism,where all genes are sorted according to the size of their univariate effect on the response. How-ever, this approach does not take the covariance structure between variables into account andthus does not guarantee to find the best set of variables with the largest overall explanatoryvalue. A search algorithm employed to find the optimal subset of covariates is called best sub-set selection. Optimality is often defined in terms of out-of-sample prediction accuracy, thatis, one wants to find the variable subset which results in the model with smallest predictionerror for new observations. One usually assumes that after selection only p∗ < n variablesremain in the model so that for the remaining covariates the ordinary least squares estimator β̂exists. Consequently, the focus here is not on finding alternatives to the OLS estimator, but onthe development of search algorithms. The assumption which is usually made for traditionaldeterministic greedy search algorithms like forward and backward stepwise selection, namelythat the objective function is unimodal, is not valid here because the multi-collinearity of Xassociated with p >> n implies that many different models fit the data equally well. For thisreason, stochastic search algorithms are needed which can find good models quickly and whichcan easily move between the modes of the objective function.This can be done within a Bayesian variable selection framework, where a covariate setindicator variable defines the model space. In this context, one can use Markov chain MonteCarlo (MCMC) methods as stochastic search algorithms, for example the Metropolis-Hastingsalgorithm with an appropriate proposal distribution (e.g. Brown et al. 1998b, Tadesse et al.2005). MCMC methods are designed to sample from the posterior distribution, but for verylarge model spaces, as is the case here, it is not computationally feasible to run the Markovchains long enough to cover the posterior distribution over the entire model space. Neverthe-less, one can usually be confident that most of the high-probability regions are covered by thechains (George and McCulloch 1997) and MCMC is used to identify these high-probabilityregions instead of trying to explore the entire posterior distribution.In the following, the methods which have been briefly introduced here are discussed inmore detail, some connections between methods are pointed out, and algorithms to find themodel solutions are described. In this chapter, for simplicity of notation, it is always as-22sumed that the input variables X are centered around 0 and are scaled to have unit variance.The response y is centered around 0 and coefficients β are standardised to have length unity(∑β2i = 1). In this chapter, all methods are presented in the context of a linear regressionmodel y = Xβ + ε, ε ∼ N(0, σ2In), except where stated otherwise.3.2 Regularisation via shrinkage3.2.1 Penalised likelihood methodsRidge regressionRidge regression (Hoerl and Kennard 1970) was first proposed in the context of non-orthogonalregression problems, i.e. for problems with collinear input variables. The authors showed thatin such situations their biased ridge estimator can have smaller mean squared error than thebest linear unbiased estimator, which is the ordinary least squares solution. It was much laterthat it was realised that ridge regression can also be useful in even more ill-conditioned “largep, small n” situations.The ridge estimator β̂2 = (XT X + 2σ2λI)−1XT y is the maximum penalised likelihoodsolution of the linear regression problem y = Xβ + ε (ε ∼ N(0, σ2I)), where the penalty im-posed on the log-likelihood `(β) is proportional to the value of λ > 0. For the linear regressionmodel the log-likelihood is given as`(β) = log p(y|X, β) = log(n∏j=11√2πσ2exp−(yj − xTj β)22σ2) (3.2)= −n2log 2πσ2 −n∑j=1(yj − xTj β)22σ2.The penalty term contains the sum of squared regression coefficients∑pi=1 β2i (i.e. the L2norm of β). This is an optimisation problem with constraints:maxβ(`(β)) subject top∑i=1β2i ≤ t. (3.3)The introduction of Lagrangian multipliers leads to the following equivalent formulation ofmaximising the penalised log-likelihood function `(β)∗2:β̂2 = arg maxβ(`(β)∗2) = arg maxβ(`(β)− λp∑i=1β2i ), (3.4)23where t and λ are related one-to-one. Finding the ridge regression solution is equivalent todetermining the maximum a posteriori (MAP) estimate of the Bayesian regression model withindependent and identical Gaussian priors βi ∼ N(0, τ > 0) on each parameter βi:p(βi|τ) = 1√2πτexp−β2i2τ, (3.5)and hencep(β|τ) =p∏i=11√2πτexp−β2i2τ= (12πτ)p/2 expp∑i=1−β2i2τ. (3.6)The log posterior density is proportional to:log p(β|X, y, τ) ∝ `(β) + log p(β|τ) (3.7)= `(β)− p2log(2πτ)− 12τp∑i=1β2i= `(β)− constant− 12τp∑i=1β2i ,from which it follows that the Bayesian model corresponds to the ridge regression model withλ = 1/(2τ).Lasso regressionRidge regression shrinks the coefficient estimates towards zero, but it does not favour estimatesbeing exactly equal to zero. However, in “large p, small n” situations like microarray data oneoften wants to induce sparsity in the regression solution to improve the interpretability of theresults, i.e. one prefers a solution where only a few variables have an estimated effect otherthan zero. In the penalised log-likelihood context, this can be achieved by using penalty termswhich favour sparsity. One example is lasso regression (Tibshirani 1996) with a penalty onthe L1 norm of the regression coefficient vector ||β||1 =∑pi=1 |βi| instead of the L2 norm||β||2 =∑pi=1 β2i :β̂1 = arg maxβ(`(β)− λp∑i=1|βi|). (3.8)The L1 norm corresponds to independent, identical Laplace (also called double exponen-tial) distributions with parameter λ as priors on the β parameters. They have mean 0 andvariance τ = 2/λ2:p(βi|λ) = λ2exp(−λ|βi|), λ > 0, (3.9)24and hencep(β|λ) =p∏i=1λ2exp(−λ|βi|) = (λ2)p exp(−λp∑i=1|βi|). (3.10)and solog p(β|X, y, λ) ∝ `(β) + log p(β|λ) (3.11)= `(β)− p× log 2λ− λp∑i=1|βi|= `(β)− constant− λp∑i=1|βi|.−4 −2 0 2 40.000.100.20beta−4 −2 0 2 40.000.040.08beta−4 −2 0 2 40.00.10.20.30.40.5beta−4 −2 0 2 40.000.040.080.12betaFigure 3.1: Gaussian (top) and Laplace (bottom) densities (left) with variances τ = 2 and correspond-ing shapes of densities of their product with a Gaussian distribution N(1, 1) (right), both for a singlevariable.Figure 3.1 illustrates the shrinkage behaviour of the ridge and lasso regression estimates.It shows Gaussian and Laplace distributions which correspond to priors in a Bayesian model25with equal prior variances τ = 2. The products of these distributions with a normal distributionwith mean and variance of one N(1, 1) are also shown in Figure 3.1. If the N(1, 1)-distributionwould be a likelihood, then the standard maximum likelihood estimate for beta would be β̂ = 1.Equivalently, the ridge estimator, which is the maximum a posteriori solution of the Bayesianmodel with a Gaussian prior, would be β̂2 = 0.67, since this is the mode of the product ofthe Gaussian prior and the N(1, 1) distribution (see Figure 3.1). Hence, the estimate wouldbe shrunken towards zero - but not be equal to zero. The mode of the product of the Laplacedistribution and the N(1, 1) distribution, is equal to zero. Hence, if N(1, 1) were a likelihood,then the maximum a posteriori estimate β̂1 of the Bayesian model with a Laplace prior wouldbe exactly equal to zero.Bridge regressionRidge (q = 2) and lasso (q = 1) regression both have the same general formβ̂q = arg maxβ(`(β)− λp∑i=1|βi|q) (q > 0). (3.12)Problems of this form are sometimes termed “bridge” regression problems (e.g. Fu 1998). Forproblems with q ∈ (1, 2), which are intermediate between the lasso and ridge regression, theregression estimates are shrunk more towards zero than with ridge regression, but not as muchas with the lasso. In particular, the solutions will not be sparse, i.e. while the coefficientswill usually be estimated to be close to zero, they will not be exactly zero. For problems withq < 1, however, the solutions will be even more sparse than with the lasso. For q → 0,bridge regression approaches all-subset variable selection (Frank and Friedman 1993), whichis discrete in the sense that the sum∑pi=1 |β̂i|q in the penalty term only counts the variables thatare in the model, since |β̂i|q → 1 for q → 0 and β̂i 6= 0 but |β̂i|q = 0 for β̂i = 0. Hence, thebridge regression solution for q → 0 is given by the maximum log-likelihood for a regressionmodel with the “best” variable subset, penalised by the number of variables in the model.Bridge regression solutions can again be interpreted as solutions of Bayesian regressionswith prior distributions that correspond to the penalty term. The independent, identical priordensities are of the formp(βi|λ, q) = C exp(−λ|βi|q), (3.13)26where C is a normalising constant. The left graph in Figure 3.2 shows the shape of a prior den-sity corresponding to a bridge model with one variable with q = 1.5; the shape is intermediatebetween the Laplace density corresponding to lasso and the Gaussian density of ridge regres-sion. The right graph in Figure 3.2 illustrates the shape of the prior density for the one-variablemodel with q = 0.1: most of the probability mass is either very close to zero or in the tails,favouring sparsity.betaprior density−4 2 0 2 4betaprior density−4 2 0 2 4Figure 3.2: Forms of prior densities corresponding to the bridge one-variable regression models withq = 1.5 (left) and with q = 0.1 (right).Tibshirani (1996) and Fu (1998) compared the prediction performance of lasso, ridge, andbridge regression and did not find that any of the methods uniformly dominates the others.Which model performs best, seems to depend on whether the data are consistent with the un-derlying model assumptions. For example, the lasso model performed well when there werea few covariates with large influences combined with many variables with no or little effecton the response. On the other hand, ridge regression was found to perform well in situationswhere most covariates were related to the response variable with similar effect sizes. There isalso empirical evidence (Tibshirani 1996) that, at least in the n > p situation, lasso does notperform as well as ridge regression, if there are high correlations between predictors. Lassotends to select only one variable from a group of highly correlated variables, which can be anadvantage if one is interested in finding the smallest possible set of explanatory variables forprediction.27The elastic netZou and Hastie (2005) proposed the elastic net, another shrinkage method, which is interme-diate between lasso and ridge regression. The authors wanted to preserve the sparsity propertyof lasso, but avoid a few of its disadvantages. One potential problem with lasso regression inthe p >> n situation is that it can select at most r = rank(X) ≤ min(p, n − 1) = n − 1variables (or r ≤ min(p, n) if X is not centered) due to the nature of the convex optimisationproblem. This might be too restrictive if n is very small as is often the case with microarraystudies. Elastic net regression can select at most min(p, n + p) = p variables and hence avoidsthis problem. Zou and Hastie (2005) point out, that with gene expression data one might wantto select all genes sharing one biological pathway with importance for the (clinical) response- and many of these genes can be expected to be highly correlated with each other. Contraryto the lasso, ridge regression tends to “split the evidence” across all of the correlated variablesand would estimate non-negligible regression coefficients for all of them. The elastic net triesto keep this feature of ridge regression, while at the same time keeping the automatic variableselection property of the lasso.First, the naïve elastic net is defined as a penalised log-likelihood method with both, L1and L2, penalties:β̂NEN = arg maxβ(`(β)∗) = arg maxβ(`(β)− λ2p∑i=1β2i − λ1p∑i=1|βi|). (3.14)The naïve elastic net method corresponds to a Bayesian regression problem with a prior givenbyC exp(−λ[α||β||2 + (1− α)||β||1]), (3.15)where α = λ1/(λ1 + λ2).It turns out that the (naïve) elastic net can be viewed as a lasso regression problem withredefined data (y∗, X∗) (Zou and Hastie 2005):X∗(n+p)×p =1√1+λ2 X√λ2Ip and y∗(n+p) = y0.The lasso solution for these data is given by (where β∗ =√1 + λ2β):β̂∗ = arg maxβ∗(`(β∗|y∗, X∗)− γp∑i=1|β∗|) (3.16)28This corresponds to the naïve elastic net solution for the original data, if γ = λ1/√1 + λ2:β̂NEN =1√1 + λ2β̂∗. (3.17)Zou and Hastie (2005) found that the naïve elastic net tends to over-shrink in regressionproblems when compared to the lasso, and they introduce the elastic net by scaling the naïveelastic net penalty down. This is done by replacing the penalty parameter γ = λ1/√1 + λ2 inequation (3.16) by the lasso penalty λ1 only, which leads to the following adjustment for theelastic net estimate:β̂EN =√1 + λ2β̂∗ = (1 + λ2)β̂NEN . (3.18)The Jeffreys prior and other normal scale mixture priorsThe penalised likelihood methods described so far are equivalent to a Bayesian regression prob-lem with either normal priors on the coefficients or Laplace priors - or a prior distribution whichis intermediate between those two. A penalty which is even more extreme than the lasso penaltyis induced by the normal-Jeffreys prior (Figueiredo and Jain 2001, Figueiredo 2003). The priordistribution of the regression coefficients can be expressed as a scale mixture of normal dis-tributions, that is a mixture of normal distributions which all have mean zero and variancesdistributed according to a hyper-prior distribution:βi|νi ∼ N(0, νi) (3.19)νi ∼ p(νi)for all i = 1, ..., p. Here, the hyper-prior of νi is given byp(νi) ∝ 1νi. (3.20)This so-called Jeffreys prior is an improper distribution and results in an improper prior distri-bution for the coefficient vector β of the form (e.g. Griffin and Brown 2007)p(βi) =∫p(βi|νi)p(νi)dνi ∝ 1|βi| . (3.21)Unlike the Laplace and Gaussian priors, the normal-Jeffreys prior has no adjustable hyper-parameter. It has been demonstrated by Figueiredo and Jain (2001) and Figueiredo (2003)that the normal-Jeffreys prior can induce even more sparseness than the Laplace prior. The29Gaussian, Laplace, and normal-Jeffreys priors have been compared in an application to geneexpression data by Bae and Mallick (2004). The authors performed a full Bayesian analy-sis with a probit model in order to select genes which best discriminate between two tumourtypes. Bae and Mallick (2004) performed the analysis on subsets of two well-published geneexpression data sets (Golub et al. 1999, Hedenfalk et al. 2001); in each case the 500 top geneswere used as determined by the univariate two-sample test statistics. The authors selected thosevariables with the largest posterior variance estimates; they found that all three priors performwell. However, it is not obvious, how to perform posterior inference for the model with thenormal-Jeffreys prior. The improper prior distribution results in an improper posterior distri-bution, which makes sampling from the posterior distribution via standard MCMC methodsimpossible. Bae and Mallick (2004) do not explain how they solved this problem.betaprior density−4 2 0 2 4betaprior density−4 2 0 2 4betaprior density−4 2 0 2 4Figure 3.3: Normal-Jeffreys prior (left), and normal-exponential-gamma priors with a = 0.5, b = 1(centre) and a = 0.5, b = 2 (right) for a single variable βi.Like the Jeffreys prior, the Laplace prior distribution can also be expressed as a scalemixture of normal distributions, but here the hyper-prior is a proper exponential distribution:βi|νi ∼ N(0, νi), i = 1, ..., p, withp(νi|λ) = λ22exp(−λ22νi). (3.22)Griffin and Brown (2007) point out that many more shrinkage methods can be developed frompriors for the regression coefficients which are scale mixtures of normal distributions. Theypresent several classes of such prior distributions, which induce sparseness like lasso regressionand the normal-Jeffreys prior. Among these, the authors favour the normal-exponential-gamma30(NEG) distribution. It arises from placing a gamma hyper-prior distribution on the naturalparameter λ2i /2 in the exponential hyper-prior p(νi|λi) in equation (3.22), so that now thisscale parameter can vary between coefficients. Placing a gamma distribution with parametersa and b on λ2i /2 leads to the following marginal density for the scale parameters νi:p(νi|a, b) = ab2(1 +νib2)−(a+1) 0 < a, b < ∞. (3.23)The NEG prior distribution incorporates the Laplace prior (with a, b → ∞) and the normal-Jeffreys prior (with a, b → 0) as limiting cases (see Griffin and Brown 2005, for details). InFigure 3.3 the Jeffreys prior and two examples of the normal-exponential prior are illustratedfor a single variable βi.Consistency and other properties of the lasso and related methodsRecently, there has been much work on the theoretical properties of the lasso and of relatedmethods, especially with respect to model selection consistency, that is whetherlimn→∞Pr(M̂n = M) = 1, (3.24)where M is the true sparse model and M̂n = {i : β̂i 6= 0} is the set of lasso regression coef-ficient estimates unequal to zero estimated using a data set with n samples. Meinshausen andBühlmann (2006) and Zhao and Yu (2006) show that the lasso is consistent for variable selec-tion in the n > p situation if a neighbourhood stability condition is fulfilled, which is calledIrrepresentable Condition by Zhao and Yu (2006). This Irrepresentable Condition requires thatthe predictors which are in the true model M are not too correlated with covariates which arenot in M , and that in addition the predictors in M are not too correlated with each other. Zhaoand Yu (2006) present some designs for which the Irrepresentable Condition holds, e.g. if thecorrelations Cik (i, k = 1, ..., p) are constant and positive, or if they follow a power decayrelation:Cik = (ρ)|i−k|, for any i, k = 1, ..., p and |ρ| < 1. (3.25)Unfortunately, these designs seem too restrictive for observed microarray gene expression data,and in addition, we are in the p >> n setting where these results do not apply directly. How-ever, for the “large p” situation, both Meinshausen and Bühlmann (2006) and Zhao and Yu(2006) show that the model consistency property holds under similar conditions, if p grows31with n →∞ not too fast. The growth can be polynomial in n if the noise distribution has finitemoments and even exponential in n for a normal noise distribution.In practice, the regularisation parameter λ1 is often chosen so as to minimise out-of-sampleprediction error. A prediction-optimal selection of λ1 leads to models that contain not onlythe true predictors contained in M , but also some irrelevant covariates, especially in a high-dimensional highly sparse setting (Meinshausen and Bühlmann 2006). The reason is that thelasso estimators are biased towards zero and using the λ1 value which results in only includingthe true predictors will lead to bad prediction performance. Decreasing the penalty slightlygives a trade-off between some added noise variables and larger β̂i estimates for the true pre-dictors (i ∈ M ).Recently, it has been demonstrated that the lasso is consistent in an Lq-sense (for q ∈{1, 2}) under less stringent conditions than those needed for model consistency (Zhang andHuang 2008, van de Geer 2008), even in the p >> n setting. This means that||β̂n − β||q → 0 for n →∞. (3.26)For fixed dimension p, this convergence result implies that coefficients corresponding to therelevant predictors will be non-zero with high probability. The conclusion is that the sequenceof models found using lasso with the full range of penalties λ1 contains the true model withhigh probability, along with some noise variables. This suggests that lasso should be used asa variable selection method only. When there is a very large number of predictors, a singleshrinkage parameter λ1 is not sufficient for simultaneously selecting variables and performingcoefficient estimation. The lasso may be used to select a small set of predictors, followedby a second step to estimate coefficients for those predictors (e.g. by ordinary least squaresregression), and maybe also to perform additional variable selection in some cases.The problem with lasso regarding its consistency properties is that shrinkage is too largefor large effects. Ideally, we want no shrinkage for the true predictors in M . To this end,Fan and Li (2001) have modified the lasso penalty to construct the model-selection consistentSCAD (smoothly clipped absolute deviation) estimator, where the penalty is thresholded sothat it is zero for all effects with an absolute value larger than a certain threshold:λa(|β|) = λ1I(|β| ≤ λ1) + (aλ1 − |β|)+(a− 1) I(|β| > λ1), (3.27)32with some a > 2 and for λ1 > 0. So for |β| ≤ λ1, the SCAD penalty is equivalent to the lassopenalty λ1, but for larger values of |β| the penalty depends on a and the size of |β|, until for|β| > aλ1 it becomes zero. Unfortunately, the optimisation problem for SCAD is non-convexand thus hard to evaluate numerically.An alternative method is the adaptive lasso (Zou 2006), which attempts to solve the prob-lem of over-shrunk parameter estimates for the true predictors in M by introducing weights wion the penalty which are chosen in a data-dependent wayβ̂AL = arg maxβ(`(β)− λp∑i=1wi|βi|). (3.28)The authors propose to use wi = 1/|β̂i|γ for some γ > 0, where β̂ can for example be the OLSestimator, if it exists which requires n > p. If it is indeed the OLS estimator β̂OLS and γ = 1,the adaptive lasso is nearly equivalent to the non-negative Garrote method proposed by Breiman(1995), except for a sign constraint in the non-negative Garrote that requires βiβ̂i ≥ 0 for alli. This estimator was originally proposed in the n > p setting for ordinary linear regression as(β̂NNG)i = ci(β̂OLS)i for all i = 1, ..., p, where c is the solution ofminc(||y −p∑i=1ci(β̂OLS)ixi||2 − λp∑i=1ci) subject to ci ≥ 0 ∀i. (3.29)Both, the SCAD and the adaptive lasso estimators, fulfill the oracle property that, if wewould know in advance which predictors belong to the true model M , we could not improveon the asymptotic results given by these methods (Fan and Li 2001, Zou 2006). Knight and Fu(2000) also show the oracle property for bridge regression with q < 1 for non-singular problemsrequiring n > p. However, the optimisation problem for bridge regression with q < 1 is non-trivial, because the penalised likelihood function that is to be maximised is non-convex andnon-differentiable in the local optima.Griffin and Brown (2007) argue that their Bayesian normal-exponential-gamma modelsautomatically adapt the parameter estimates in the sense of the adaptive lasso, and also usesnegligibly small penalties for large effects in accordance with the idea behind SCAD. Thisis achieved through the additional hyper-prior on λ which implies that the penalty λ is notdeterministic but can be varied continuously. While the adaptive lasso and SCAD enforcesingle solutions, this can be seen as an artificial restriction, which should not be made, andthat methods, which find many of all the potential good solutions in the multi-collinear p >>33n situation, are preferable, possibly combined with a model averaging approach to improveprediction performance. This is the philosophy adopted by Griffin and Brown (2007) and alsoin the Bayesian variable selection setup presented in Section 3.3. For this reason, the normal-exponential-gamma priors are chosen with the aim to result in non-strictly convex optimisationproblems with many solutions. The lasso is also non-strictly convex (Zou and Hastie 2005)and thus may have multiple solutions. Note, that this is generally not considered by softwareprovided to solve the lasso, and the optimisation problem is usually treated as if it were strictlyconvex with one unique solution (see Section 3.2.3).3.2.2 Methods with derived input directionsThe penalised likelihood methods presented in the previous section are one-step procedures:all input variables are entered into the model with equal weights and the solution is foundby maximising the objective function. The coefficients are shrunk compared to the ordinaryleast squares estimates according to their explicit prior distributions. A different approach is totransform the input space Z = φ(X) into a space with lower dimension. Usually, the new inputdirections zm, m = 1, ..., M , are linear combinations of the original inputs xi, i = 1, ..., p,where M << p.Principal components regressionOne example is principal components regression (PCR) (Massy 1965), where the linear com-binations zm (m = 1, ..., M ) are the first M principal components (PC) of matrix X . Everyn× p matrix X can be expressed in the form X = UDV T , where U and V are n× p and p× porthogonal matrices, with the columns of U spanning the column space of X and the columnsof V spanning its row space (e.g. Hastie et al. 2001). D is a diagonal matrix of dimension p×p,where the diagonal entries d1 ≥ d2 ≥ ... ≥ dp ≥ 0 are the singular values of X . The rank rof X is determined by the number of non-zero singular values; it is always r ≤ min(n− 1, p)(or r ≤ min(n, p) if X is not centered). In the “large p, small n” situation this implies that therank r of centred matrix X is at most n− 1. The singular decomposition of X corresponds tothe eigen decomposition of the symmetric matrix XT X:XT X = V D2V T . (3.30)34In this context, the squared singular values d2i , i = 1, ..., p, are the eigenvalues of XT X . Theeigenvectors vi of XT X are called the directions of the principal components of X (PC direc-tions); the principal components themselves are defined as Z = XV = UD, so that the samplevariance of zi = uidi is given byvar(zi) =zTi zin=d2in. (3.31)Hence, the sample covariance matrix of X can be partitioned into variance proportions alongthe PC directions, which are explained by the corresponding eigenvalues. Because the eigen-values are ordered, the sample variances of the principal components are also ordered, withthe first PC z1 = Xv1 having the largest sample variance. If p >> n, only the first at mostn− 1 PC’s differ from the zero vector (if X is centred); this implies for the “large p, small n”situation that at most n− 1 principal components determine all variability in the p-dimensionalmatrix X .Usually one hopes that there is a small number of M < n − 1 PC’s which contributemost of the variability in X , so that the subsequent PC’s can be discarded. Then the originalregression problem y = Xβ + ε, ε ∼ N(0, σ2In) can be replaced by the principal componentsregressiony = ZMγ + δ δ ∼ N(0, σ∗2In) (3.32)with ZM = (z1, ..., zM) being the column matrix of the first M principal components. ThePCR coefficient vector γ is of dimension M and represents the linear effects of the PC’s onthe response y. As the PC’s are linear combinations of the original input variables X , equation(3.32) can be expressed in terms of coefficients βPCR when y is directly expressed in terms ofX:y = ZMγ + δ = XVMγ + δ = XβPCR + δ, (3.33)where βPCR = VMγ with VM = (v1, ..., vM) being the column matrix of the first M PCdirections.The principal components are found using equation (3.31). The first principal componentdirection is the direction of greatest sample variance in X , and all subsequent principal com-ponent directions are constructed, so that they are orthogonal to all previous components andexplain as much of the remaining variability in the input data as possible. In detail, the first PC35direction v1 is defined as the unit-length vector in the direction of the maximal variance withinX:v1 = arg max||θ||=1var(Xθ). (3.34)In general, the mth (m > 1) principal component direction vm is found by solving (Stone andBrooks 1990, Frank and Friedman 1993)vm = arg max||θ|| = 1, andvTl Sθ = 0, l = 1, ..., (m− 1)var(Xθ), (3.35)where S is the sample covariance matrix of X .Thus in PCR, the input directions for the regression on y are derived solely by maximisingthe explained variance in X . However, principal component directions that correspond to smalleigenvalues d2 of XT X , might still correspond to directions which are correlated with theresponse y, and would therefore be important in the regression. One way to accommodate forsuch situations (Brown 1993) is to work backwards from the smallest to the largest eigenvaluewhen determining which principal components are to be incorporated into the regression model.One then uses the first M principal components z1, ..., zM , where M is determined by thefirst appearance of a statistically significant coefficient γM in the regression with y. As analternative, partial least squares is a dimension reduction method which uses the response y forthe construction of the regression input directions, in addition to the input matrix X (see laterin this chapter).Generalising principal components regression Principal components regression was usedfor the analysis of human breast cancer gene expression data by West et al. (2001) and West(2003). The authors used PCR for binary (probit) regression in a Bayesian context and devel-oped conjugate priors which they referred to as generalised singular g-priors. The authors foundthat by keeping all p genes in the model the computed principal components are influenced bynoise associated with the genes. West et al. (2001) solved this problem by pre-filtering thedata to identify the 100 genes which are most highly correlated with the response. West (2003)improved on this by introducing sparse latent factor models as a generalisation to Bayesianprincipal components regression, where X is viewed as an observed realisation of underlying36factors ZM = (zm)m=1,...,M that are in turn related to the response y:X = ZMBT + ∆, (3.36)y = ZMγ + δwherezj ∼ N(0, Ψ2) and ∆j ∼ N(0, Φ2), andδj ∼ N(0, σ∗2) ∀j = 1, ..., n.Here, xj , j = 1, ..., n, is the jth sample in data matrix X , zj is a vector of length M of potentiallatent factors for case j, B is a factor loadings matrix of dimension p × M , and ∆ and δ arenoise terms. Sparsity in the loadings matrix B is induced by a sparse prior, for example thespike-and-slab prior (where δ0 is the Dirac distribution at zero)p(Bim) ∼ πiδ0(Bim) + (1− πi)N(Bim|0, 1), (3.37)with small probability πi, which can differ for each variable i. Then, B will have many zeros ineach column, meaning that each underlying latent factor (which can for example be interpretedin the case of gene expression data as an underlying biological process) involves only a smallnumber of variables (i.e. genes). Also, B will have many zeros in each row, so that eachvariable is only involved in a small number of latent factors. Sparsity also appears in a differentway, by fitting a model with only a small number of latent factors, e.g. M = 25 (West 2003).In the context of gene expression analysis, the principal components are sometimes calledsuper-genes (Spang et al. 2001). One hopes that a few of these super-genes explain most of theinformation in the input matrix. However, they can be hard to interpret, because they are linearcombinations of a large number of genes. The introduction of sparse latent factors alleviatesthis problem, because here only a relatively small number of genes contributes to each latentfactor, which can be interpreted as an underlying biological factor. Another way to improvethe interpretability is to separate the input data into clusters of similar variables, which are thenrepresented by one single meta-variable per cluster. In gene expression analysis it is commonto call these meta-variables meta-genes (e.g. Wit and McClure 2004). For example, the meta-genes can be the centroids of the clusters as derived by some clustering algorithm like k-meansclustering, or they can be the first principal components of the sub-matrix X∗ constructed fromall the genes in the cluster.37One example for such a cluster-based method, which uses the first principal componentsof the clusters as meta-genes, is gene shaving (Hastie et al. 2000). This is an iterative algorithmthat attempts to identify clusters of genes with highly similar gene expression profiles andlarge variations across samples. This is achieved by iteratively constructing the first principalcomponent of a cluster and then “shaving” those genes off the cluster which are too far awayfrom the principal component according to some distance measure. The original gene shavingalgorithm is an unsupervised method, i.e. it does not relate the gene expression data to aresponse variable. But in the same paper a supervised version for response class prediction isalso proposed, where the gene clusters are created in such a way that the genes vary as muchas possible between response classes.Another example for the use of meta-genes is given by Pittman et al. (2004), where meta-genes were constructed from gene expression data by first performing k-means clustering andthen computing the first principal components of the clusters. The meta-genes are used togetherwith clinical predictor variables in order to predict breast cancer outcome by fitting multipleclassification tree models to the input data in a Bayesian analysis.Partial least squaresThe regression input directions in principal components regression are selected solely from the(orthogonal) directions of largest variability in the column space of X . However, the goal ofthe regression is to construct a model from X which best predicts the response variable y. Ittherefore makes sense to construct the input directions in such a way that they do not just covermuch of the sample variance in X , but are also highly correlated with the response. Partial leastsquares regression attempts just that: the mth (m > 1) PLS direction φm is found by solvingmax||θ|| = 1φTl Sθ = 0, l = 1, ..., (m− 1)cor2(y,Xθ)var(Xθ), (3.38)where S is the sample covariance matrix of X (Stone and Brooks 1990, Frank and Friedman1993). The first PLS direction is simply the unit-length vector θ which maximises the productcor2(y,Xθ)var(Xθ) without any orthogonality constraints.Partial least squares is based on the idea of latent variables Z = (z1, ..., zM), which under-38lie both X and y:X = ZMφT + ∆ (3.39)y = ZMγ + δ. (3.40)∆ is a residual matrix and δ a residual vector; the latent variables zm are called scores (orPLS components or simply latent factors) and the coefficients φm in (3.39) are called loadingsof X (or PLS directions) (Brown 1993). Thus, PLS forms its latent factors by a canonicalcovariance analysis of X and y, while PCR forms its latent factors (i.e. principal components)by a canonical variance analysis of X alone. In order to ensure unique solutions, additionalconstraints have to be imposed on the PLS model. Usually, either the scores are forced to beorthogonal or orthogonal loadings are required.Here, only the algorithm with orthogonal loadings, rather than orthogonal scores, is pre-sented. The orthogonal-loadings algorithm requires a series of multivariate regressions, andis more suitable for finding theoretical properties of PLS than the orthogonal-scores algo-rithm which is based on univariate regressions. For example, it becomes clear that the spacethat is spanned by the PLS directions, is equal to the space spanned by the Krylov sequence{XT y, (XT X)XT y, ..., (XT X)M−1XT y}, so that an alternative form for the PLS estimate ofβ, when expressing y directly in terms of X without the intermediate latent factors Z is givenby Butler and Denham (2000)β̂PLS =M∑m=1γ̂m(XT X)m−1XT y, (3.41)where the γ̂m, m = 1, ...,M , are the solutions of the multivariate regression of y on the PLSfactors zm. That is, γ̂m, m = 1, ..., M , minimise equation(y −M∑m=1γmzm)T (y −M∑m=1γmzm) (3.42)with respect to γm, m = 1, ..., M .Equation (3.41) allows us to express the PLS regression explicitly in terms of a regressionof y on X , without the intermediate regression equations involving the latent factors zm:ŷ = Xβ̂PLS. (3.43)39Note that because the response y is involved in finding the regression input directions, thePLS estimators β̂PLS are not linear in y. In summary, PLS estimators β̂PLS are non-linear,biased shrinkage estimators of the regression coefficients in y = Xβ + ε. The PLS and PCRsolutions β̂PCR and β̂PLS are often similar, because the correlation term cor2(y,Xβ) in the PLSoptimisation criterion maxβ (cor2(y, Xβ)var(Xβ)) is often weak compared to the varianceterm var(Xβ) (Hastie et al. 2001). The shrinkage behaviour of PLS is somewhat peculiar andit was not until the 1990’s that some light has been shed on its shrinkage properties (Frankand Friedman 1993, Butler and Denham 2000), which are further discussed in the followingsection.Continuum regression and shrinkage behaviourOrdinary least squares regression has the single goal to minimise the in-sample response pre-diction error, while the regularised regression methods like ridge regression, PCR and PLSalso account for the variation in the predictor variables X in some way. The assumption isthat directions in the predictor space that are well sampled, i.e. for which many data pointsexist, should provide better prediction for new observations, i.e. improve the out-of-sampleprediction error, than poorly sampled directions. To repeat, while ordinary least squares re-gression maximises the squared correlation between the response variable y and the predictorXβ (maxβ cor2(y,Xβ)), PCR maximises the variance of the predictor Xβ (maxβ var(Xβ)),and PLS combines these two approaches by maximising the product of the squared correlationbetween y and X and the variance within the input space: maxβ (cor2(y, Xβ)var(Xβ)). Stoneand Brooks (1990) generalised this approach to so-called continuum regression, the coefficientsβCR of which are the solution ofmax||β||2=1cor2(y, Xβ)varα(Xβ). (3.44)Here, α = 0 leads to ordinary least squares, α → ∞ numerically corresponds to principalcomponents regression and α = 1 gives the partial least squares results. In addition, Sundberg(1993) has shown that for 0 ≤ α < 1 there is a close relationship between continuum regressionand ridge regression. In fact, for a ridge regression solution with penalty λ one can find a valueα such thatβ̂CR(α) =(1 +α1− α.)β̂2(λ) (3.45)40The factor 1+α/(1−α) implies that β̂CR(α) is shrunk less than β̂2(λ). In particular, continuumregression will apply no shrinkage, if all input variables are orthogonal or if there is only asingle variable in X; in short if there is no collinearity. This has motivated Björkström andSundberg (1999) to construct an alternative definition of continuum regression by modifyingridge regression: instead of using β̂2(λ) as the final estimator, they use the solution of a simplelinear regression of ŷ2 = Xβ̂2(λ) as the single regressor on the response y. This modification iscalled least-squares ridge regression (LSRR) and is nearly identical to the continuum regressionsolution β̂CR(α) in equation (3.45).Stone and Brooks (1990) propose to choose the value of α in continuum regression jointlywith the other model parameters, e.g. by cross-validation. However, this introduces a largecomputational burden and additional variability in the estimation procedure. Even more trou-blesome is the observation by Björkström and Sundberg (1996), that continuum regression canbe discontinuous in α, a problem that is not shared by least-squares ridge regression. Neverthe-less, the formulation of continuum regression was very helpful for understanding the statisticalproperties of partial least squares by putting it in context with better understood methods likeOLS and PCR.The same can be said for another general formulation, which is especially helpful forillustrating the shrinkage behaviour of penalised likelihood methods, principal componentsregression, and partial least squares. This formulation uses the eigen decomposition (3.30)of XT X:XT X =r∑i=1d2i vivTi ,where r ≤ min(n− 1, p) is the rank of matrix X . Note that the ordinary least squares solution(or in case of p > n the minimum-length least squares solution) can be expressed in terms ofthe OLS contributions of the principal component directions (Butler and Denham 2000):β̂OLS =r∑i=1α̂ivi, (3.46)where the coefficients α̂i = 1d2i vTi XT y are the OLS estimates in the principal directions of X .Shrinkage estimators shrink some or all of these contributions towards zero, which intro-duces bias, but at the same time reduces the variance, which can lead to a smaller mean squarederror overall. The regression coefficient estimators can generally be expressed as (Butler and41Denham 2000)β̂ =r∑i=1f(d2i )α̂ivi, (3.47)where values f(d2i ) < 1 introduce shrinkage in direction vi.While PCR keeps the M first principal components, and discards the r − M last com-ponents (i.e. f(d2i ) = 1 ∀i = 1, ..., M and f(d2i ) = 0 ∀i = M + 1, ..., r), ridge regres-sion shrinks the estimator in all principal component directions by an amount which relatesto the size of the eigenvalues d2i and is inversely proportional to the constant penalty param-eter λ > 0: f(d2i ) = d2i /(d2i + λ), because V is an orthogonal matrix and D as well as(V D2V T + λI) = (D2 + λI) are diagonal matrices. Hence it is (Butler and Denham 2000):ŷ = Xβ̂2 = X(XT X + λI)−1XT y (3.48)= UDV T (V D2V T + λI)−1V DUT y= D2(D2 + λI)−1UV T V UT y= D2(D2 + λI)−1UUT y=r∑i=1uid2id2i + λuTi y.This means that the first principal components with large eigenvalues are shrunk less than thelast components with small eigenvalues. In ridge regression the penalty parameter λ has to betuned (for example by cross-validation), and in PCR the number M of principal components tobe kept in the model has to be optimised.The shrinkage behaviour of partial least squares is not as straightforward as that of PCRand ridge regression. It has been proved to be a shrinkage estimator in the sense that its Euclid-ian norm is smaller than that of the ordinary least squares (or minimum-length least squares)estimator: ||β̂PLS||2 < ||β̂OLS||2 (e.g. Goutis 1996), but in some situations some principalcomponent directions can have expanded estimates, i.e. f(d2i ) > 1 for some d2i (Frank andFriedman 1993).Using the eigen decomposition of matrix XT X in equation (3.41) leads to an explicitformulation of the shrinkage coefficient vector f (M)(d2) for PLS with M components, whichgives the amount of shrinkage in the principal component direction corresponding to eigenvalue42d2 (Butler and Denham 2000):f (M)(d2) =M∑m=1γ̂m(d2)m. (3.49)From this equation, the following shrinkage properties of PLS can be derived (Butler and Den-ham 2000). If the eigenvalues are distinct and M < r, then1. the eigenvalues can be divided into non-empty disjoint sets Rj of consecutive eigenval-ues, for which the corresponding shrinkage coefficients alternate between f (M)j > 1 andf(M)j < 1 (i.e. the shrinkage behaviour of PLS oscillates between shrinking and expand-ing).2. The last principal direction vr, i.e. the one corresponding to the smallest non-zero eigen-value d2r , is always shrunk.3. If the number of chosen partial least squares components M is even then the first principalcomponent direction v1 is shrunk, if M is odd then v1 is expanded.This behaviour implies that PLS regression should be used with caution and should not beadopted as an automatic regularisation solution. However, PLS seems to work quite well inmany practical applications. Butler and Denham (2000) suggest that one reason for this mightbe that in many real-life applications, large eigenvalues of matrix XT X have very large canon-ical covariances which then decrease fast for smaller eigenvalues. Following from equation(3.49) this means that expanded principal components are not expanded by much, whereasthose principal components which correspond to small eigenvalues are shrunk a lot.3.2.3 Finding the regularisation solutionsSolution for fixed shrinkage parameter: convex optimisation algorithmsFor a fixed amount of shrinkage (e.g. if the value of the penalty parameter for penalised likeli-hood methods is fixed), finding the regularised regression coefficient estimates can be viewedas a standard optimisation problem: we want to find the global maximum of the objectivefunction. In the case of penalised likelihood methods the objective function is the penalisedlog-likelihood (or equivalently, the log posterior density in the Bayesian interpretation). For43many regression problems, for example linear regression y = Xβ + ε with ε ∼ N(0, σ2I)and generalised linear models like logistic regression, the likelihood function - and with it thelog-likelihood `(β) - is strictly convex. Fu (1998) show that penalised log-likelihood functions`(β)∗ = `(β) − λ∑pi=1 |βi|q with any fixed q > 1 and fixed λ are strictly convex, too. Note,however, that this is not true for lasso regression (q = 1) which is only non-strictly convex andcan thus have a flat optimum, i.e. a region of connected values in β which all share the samemaximum penalised log-likelihood. Griffin and Brown (2007) demonstrate this with an exam-ple with p = 2 parameters where sometimes, one mode is at β1 = β2 = 0 and another modecan be found where one coefficient is zero and the other one is very close, but unequal, to zero.This is generally considered not to be a big problem in practice, and all algorithms describedhere which have been devised to solve lasso-type problems assume that the objective function,that is the penalised log-likelihood function `(β)∗ = `(β)− λ ∑pi=1 |βi|, is strictly convex.A wide variety of generic optimisation algorithms is available to solve strictly convex op-timisation problems, for example multivariate versions of the Newton-Raphson algorithm. Forgeneralised linear models like logistic regression, the Newton-Raphson algorithm is usually im-plemented in statistical software packages as a variant, the iteratively re-weighted least squaresalgorithm (McCullach and Nelder 1989). The algorithm has the advantage that it convergesvery fast (often within less than ten iterations). However, for data with a large number p ofvariables, like for example microarray data, it needs large amounts of memory in each itera-tion, since solving the weighted least squares problem requires the inversion of a p× p matrixby QR decomposition.As an alternative, Goeman (2008) has developed a full gradient descent algorithm forminimising the loss function, which is implemented in the R package penalized. Here,minimising the loss function is equivalent to maximising the penalised log-likelihood as theobjective function, and because of this context, we will use the term gradient ascent algorithmin the following. In each iteration the algorithm computes the full p-dimensional gradient ofthe objective function and takes a step of size ∆ν in the direction of the gradient g(ν). In orderto avoid slow convergence in the last iterations, the algorithm can automatically switch to theNewton-Raphson procedure when it gets close to the optimum. Prior to the full gradient ascentapproach, coordinate-wise gradient ascent algorithms had been proposed by several authors(e.g. Shevade and Keerthi 2003, Friedman et al. 2007, Genkin et al. 2007). These algorithms44require even less memory, since they only update one variable β̂i at a time. As an example, thecyclic coordinate ascent algorithm for logistic regression with lasso or ridge penalty by Genkinet al. (2007) is presented here, which was implemented in the C++-based “Bayesian BinaryRegression” (BBR) software.Genkin et al. (2007) base their implementation on the algorithm described by Zhang andOles (2001). The cyclic coordinate ascent (CCD) algorithm is a Gauss-Seidel-type algorithm.That is, after setting all variables to some initial value, in each iteration the algorithm finds thevalue for every variable βi (i = 1, ..., p), which maximises the conditional posterior distributionp(βi|β−i, X, y) while the other variables βk (k 6= i) are held constant. The procedure is repeateduntil convergence. The maxima of the univariate conditional posterior distributions are foundby applying the univariate Newton-Raphson algorithm. For the lasso penalty, there is a problemthough: the partial derivatives for βi of the objective function are not defined at βi = 0. Hence,the CCD update is not defined for βi = 0 and it is also invalid if it would change the sign of βi.Genkin et al. (2007) solve this problem by treating these cases separately: if the update wouldchange the sign of βi, βnewi is simply set to zero. And if βi = 0 then the update is attempted inboth directions; if both attempts fail, βnewi is kept at zero.Recently, a similar coordinate-wise ascent optimisation algorithm has been described forthe lasso and some variations of lasso by Friedman et al. (2007). The authors propose to applythe procedure for each possible value of the shrinkage parameter λ, using the solution fromeach value for λ as a warm starting value for the optimisation for the next parameter value.If there is no prior knowledge about the amount of shrinkage necessary to achieve optimalprediction, which will often be defined in terms of minimal out-of-sample prediction error, theoptimal shrinkage value has to be determined from all possible values of λ. Commonly usedmodel selection procedures to find the optimal parameter value are based on resampling, whereonly part of the data are used for training the model and the remaining samples can be used asindependent validation samples to estimate the out-of-sample error. Examples for resamplingmethods are the bootstrap and cross-validation (see Chapter 4).45Finding the complete solution path: least angle regression and other gradient ascent al-gorithmsFor shrinkage methods like lasso or PLS regression, there is an alternative to the quite genericmodel selection procedure described in the previous section. It is possible to compute the entiresolution path for all shrinkage parameter values in one go, with the computational complexity ofthe order of magnitude of ordinary least squares regression with the full set of p input variables.It has been known for some time (see e.g. Brown 1993) that the partial least squares so-lutions with 1, ..., M PLS components correspond to the solutions β̂ = GMXT y, where GMis the result of applying the first M steps of inverting matrix XT X using a conjugate gradientascent algorithm with exact line search for matrix inversion (see Gill et al. 1981, for more de-tails). If n > p and XT X is of full rank p then the OLS solution is reached after p steps. AllPLS solutions with M = 1, ..., p components lie along a path which starts at the null solutionwith 0 components, and ends at the OLS solution which corresponds to the PLS model withall p components. One might want to investigate other strategies to find a path from the simpleintercept-only model to the OLS regression coefficients (or minimum length least squares, ifp >> n). New approximations to the OLS solution can be found along such paths.Friedman and Popescu (2004) investigated gradient ascent strategies to “climb the hill” tothe ordinary least squares coefficients. Each successive point on the path is derived from theprevious one by an infinitesimal increment ∆ν > 0 in the direction of the gradient g(ν):β̂(ν + ∆ν) = β̂ + ∆g(ν) (3.50)The authors find that the gradient ascent path with squared-error loss for the linear model isvery similar to the PLS path for the linear model, but also to the path which corresponds toridge regression with penalty parameters λ ∈ (0,∞). In fact, the gradient ascent path liesin between the PLS and ridge regression paths sharing both methods’ property to shrink thecoefficients of highly correlated variables towards a common absolute value.Friedman and Popescu (2004) also investigate lasso regression and the closely related leastangle regression (LARS) algorithm (Efron et al. 2004) in this context and find that the lassopath is quite different from the solution path of the other three methods. They observe that thisis consistent with lasso behaviour towards highly correlated covariates, which is contrary to theequalising behaviour of the other three methods. In general, lasso will estimate large effects46for a small number of covariates and zero effects for the others within a set of highly correlatedvariables instead of dispersing the common effect equally among the covariates.The least angle algorithm (LARS) for squared-error loss which was introduced by Efronet al. (2004), is closely related to the lasso. The LARS method is not a gradient ascent al-gorithm, but was rather developed as a generalisation to classical forward stepwise variableselection for regression, which is not as greedy and more cautious than traditional forward se-lection. Classical forward stepwise regression works as follows. First, all possible one-variablemodels are built by regressing the response variable on each of the potential covariates. Thevariable which results in the model with the smallest residual sum of squares is chosen to be thefirst variable to be included in the model. Then, the procedure is repeated with the remainingvariables, and with the residual vector (orthogonal to the selected variable) as the new response.Thus, the second variable is selected, and so on until all variables are included in the regressionmodel. Again, this algorithm produces a solution path starting from the empty model with justa fitted intercept and ending at the full OLS model with all p input variables (always assumingthat the OLS solution exists, in particular that n > p). However, forward stepwise selectionis usually not good at finding the best regression models, especially if there is collinearity be-tween the potential input variables, because the algorithm is very greedy: once variables havebeen included, they cannot be excluded. This may keep a closely correlated covariate fromentering the model, which might otherwise result in a better fitting model together with someother variables which are also not allowed to enter.The original motivation for the LARS algorithm was to produce a similar, but more cau-tious, algorithm called forward stagewise selection, which moves towards the final model inmuch smaller steps. Forward stagewise linear regression (FSLR) starts with the empty model,like forward stepwise regression. Small steps of length ν are taken successively in the directionof the variable with the greatest current correlation with the response. This implies that thefirst step is taken in the direction of the variable which would be selected first in the forwardstepwise algorithm, but after the first step the algorithms diverge. The process is repeated untilthe final model is reached. If the current regression estimate is ŷ and the vector of correlationswith the current residual is called ĉ = XT (y − ŷ), then a forward stagewise move is describedby (Efron et al. 2004)î = arg max |ĉi| and ŷ → ŷ + ν sign(ĉi)xi, (3.51)47where choosing ν as big as |ĉi| leads to classical forward stepwise regression.** * * * ** * ** ** *0.0 0.2 0.4 0.6 0.8 1.0−5000500|beta|/max|beta|Standardized Coefficients*** ** ** ***** ** * ** ** *** ** *** ** ************ ****** ****** ******* *** ** *LASSO52110784639** * * * ** * ** * *0.0 0.2 0.4 0.6 0.8 1.0−5000500|beta|/max|beta|Standardized Coefficients*** ** * ***** ** * ** * *** ** *** * ** ********* * ******** **** ***** ** * *Forward Stagewise52110784639Figure 3.4: Solution paths for lasso (left) and forward stagewise (right) regression of the coefficientsas a function of the norm fraction |βi|/maxpj=1 |βj |. The two plots are nearly identical and only differslightly for large values of the norm fraction, see for example covariate 8. The data are from the diabetesstudy which was described in Efron et al. (2004). The plots have been created with the plot.lars()function from the R library lars.The resulting solution path is extremely similar, but not exactly equal, to the lasso regres-sion solution path (see Figure 3.4 for an example). It turns out that one can produce the solutionpaths for both methods using slight modifications of the LARS algorithm, which is computa-tionally much more efficient than the original algorithms for both lasso and forward stagewiseregression. LARS also starts off in the direction of the predictor which has strongest correla-tion with the response, but unlike forward stagewise regression the path now continues in thisdirection until some other covariate has as much correlation with the current residual. Fromthis point on, LARS proceeds in the direction which is equiangular between these two covari-ates until a third variable has as much correlation, and so on, until the final model is reached.According to Efron et al. (2004), the lasso solution path requires that the sign of any newly48added nonzero regression coefficient β̂i must agree with the sign of the current correlation ĉi.This restriction is not automatically included in the LARS algorithm, but the authors show thatthe LARS algorithm will produce the correct lasso solution path, if this additional restriction isintroduced.Connections with boosting The framework of gradient ascent (or descent) algorithms aspresented in this section allows us to investigate boosting methods, which have been developedfor classification in the machine learning community (Schapire 1990, Freund 1995) and whichhave proved very successful for prediction. The idea of boosting is to combine many, possiblyweak, classifiers to create a powerful ensemble classifier which is “more than the sum of itsparts”. The algorithm is iterative and in successive iterations puts more weight on those obser-vations, which have been hard to predict in previous iterations. Breiman (1998) was the first topoint out that the most popular boosting method AdaBoost by Freund and Schapire (1996) is agradient ascent optimisation algorithm. In the following years, the mechanisms of boosting ap-proaches were investigated extensively by statisticians (e.g. Friedman et al. 2000, Hastie et al.2001, Bühlmann and Hothorn 2007) to shed light on what used to be a black-box predictionprocedure and to understand boosting in terms of the statistical modelling culture. Friedmanet al. (2000) first noted that AdaBoost was an additive basis-expansion model and that in factboosting relies on the three key ingredients of flexible function fitting methods:• a large dictionary of basis functions to choose as increments in the additive model,• a loss function to minimise,• a regularisation mechanism (e.g. penalty term) to control the size of the coefficients inthe model.The AdaBoost.M1 algorithm for binary classification uses the unusual exponential loss func-tion, which turns out to be similar to the log-likelihood loss function for logistic models (seeChapter 4). The boosting basis functions are adaptively constructed from the data and the reg-ularisation mechanism can be seen to be a forward-stagewise procedure. Just like the lassoand the forward-stagewise linear regression (FSLR) described earlier in this section are relatedthrough the LARS procedure, boosting with the right loss function and right basis functionsis also related to the lasso. In fact, Bühlmann and Hothorn (2007) point out that the FSLR49method is a version of componentwise L2-boosting, that is boosting with the squared-errorloss function, where the components β̂i of the estimate for β are updated individually. Theonly difference between the two algorithms is that sign(ĉi) in equation (3.51) is replaced by ĉiitself:î = arg max |ĉi| and ŷ → ŷ + νĉixi. (3.52)503.3 Variable selection by stochastic searchAs we have seen in the first part of this chapter, there are sparse shrinkage methods for regres-sion which achieve “automatic” variable selection, i.e. depending of the size of the shrinkageparameter, only a small number p∗ of the input variables have an estimated regression coeffi-cient not equal to zero - and these variables can be considered as being selected by the shrink-age regression. These sparse shrinkage methods can feasibly be applied to very large data sets,which have 10, 000 or more predictor variables. However, how the variables are selected is stillnot completely understood. From the Bayesian point of view, penalised likelihood methodsonly provide the maximum a posteriori (MAP) estimates and therefore (usually) only one setof selected variables without measures of uncertainty for the inclusion of covariates. Solutionswhich are close to the MAP solution are not considered, although they might turn out to be bet-ter in terms of out-of-sample prediction. In fact, the expected value of the posterior distributioncould be considered a more preferable point estimate and summary measure for the posteriordistribution than the MAP estimate. The only reason why the penalised likelihood methodspresented in the previous section rely on MAP solutions is that these are easiest to compute.An even bigger concern is that the methods provide point estimates only. They do not readilyprovide associated measures of uncertainty like standard errors and variable inclusion prob-abilities, i.e. the variability in the posterior distribution is not accounted for. Thus, modeluncertainty will need to be assessed in an additional procedure by computationally intensiveresampling methods like bootstrapping or multiple random splits of the data into training andvalidation data sets.Bayesian variable selection (BVS) methods, however, do provide posterior probabilitiesfor models constructed from specific variable subsets and also for individual variables. Also,the Bayesian framework makes it possible to improve predictive power dramatically by usingnot just one model for prediction but an average of the top models, their influence on predic-tion weighted proportionally to their posterior model probabilities. This approach is calledBayesian model averaging (Raftery et al. 1997). While Bayesian model averaging (BMA) pro-vides a coherent framework to incorporate model uncertainty, it can be difficult to implementin practice if the number of potential input variables is very large. In this situation it becomesimpossible to evaluate the posterior probabilities of all 2p possible models, and one resolves to51use stochastic search strategies which only find the models with highest posterior probabilities(which are usually the only models of interest).3.3.1 Bayesian variable selection modelsVariable selection in a Bayesian context involves the exploration of the posterior model spacewhich is of dimension 2p if there are p potential predictor variables. Usually, the posterior dis-tribution cannot be computed explicitly, so Markov chain Monte Carlo (MCMC) methods areused for sampling from the posterior distribution. Variable selection implies that the Markovchain has to move between models of varying dimension, and several procedures have been de-veloped to make this possible. In this section, some Bayesian variable selection models are dis-cussed together with the MCMC methods which are needed to evaluate the resulting posteriordistributions. The focus will be on the implementation of a BVS model for logistic regressionrather than the linear regression model, since binary classification by logistic regression is thefocal point of the applications in Chapters 6 and 7.We start, however, by recalling the Bayesian linear model, for which the BVS modelsintroduced here were originally developed. The linear regression model is given as in equation(3.1), with conjugate prior distributions for the regression parameters β and σ2:y = Xβ + ε (3.53)ε ∼ N(0, σ2In)σ2 ∼ IG(ν/2, νλ/2)β|σ2 ∼ N(b, σ2v).I.e. the prior distribution p(β|σ2) is a normal distribution with mean vector b and variancematrix σ2v. The prior of σ2 is an Inverse Gamma distribution, where its two parameters arespecified by ν and λ, so that σ2 ∼ IG(ν/2, νλ/2) corresponds to νλ/σ2 ∼ χ2(ν) with χ2(ν)denoting a χ2 distribution with ν degrees of freedom. Assuming b = 0, the posterior distribu-52tions are given as (e.g. Lindley and Smith 1972)p(β|σ2, X, y) ∼ N(B, σ2V ) with (3.54)B = (XT X + v−1)−1XT y and V = σ2(XT X + v−1)−1,p(σ2|X, y) ∼ IG(ν̃/2, ν̃λ̃/2) withν̃ = ν + n and λ̃ =νλ + SSν + n,where SS = (y − XB)T (y − XB) + (B − b)T v(B − b) is a decomposition into the sum ofsquared errors (i.e. the distances between y and the posterior mean predictor XB) and a secondterm representing a scaled sum of squared distances between prior b = 0 and posterior meanB.A good overview of Bayesian variable selection and model averaging techniques is givenby Clyde (1999) and Chipman et al. (2001). An early implementation of BVS was publishedby Mitchell and Beauchamp (1988). The authors assume a priori that the individual regressioncoefficients βi are independent and assign a uniform “spike and slab” mixture prior distribution,i.e. the βi’s are uniformly distributed within a certain range, except for an additional probabilitymass concentrated at βi = 0 (see Figure 3.5).In later publications, the model setup is usually a Bayesian hierarchical normal mixturemodel with two components, one representing the variables included in the model and theother one the variables which are excluded. A latent indicator variable γ ∈ {0, 1}p determinesto which mixture component a variable belongs. George and McCulloch (1993) use a mixtureof two normal distributions with mean zero. The model-exclusion component has a very smallvariance τi, whereas the inclusion component has a large variance c2i τi:βi|γi ∼ (1− γi)N(0, τ 2i ) + γiN(0, c2i τ 2i ), c2i > 1 (3.55)withp(γi = 1) = 1− p(γi = 0) = πi. (3.56)The value for c2i is chosen to be large, usually the same value c2i = c2 ∀i from the range between10 and 1000. There is empirical evidence that this usually works well and that the results areinsensitive to the exact choice of c2 within this range (e.g. Smith and Kohn 1996, George andMcCulloch 1997), assuming that the input variables X are standardised to have unit varianceand zero mean.53Note that the sparse penalised likelihood methods like lasso, elastic net, and the normal-exponential-gamma (NEG) models presented in Section 3.2.1 perform variable selection forthe same reason as BVS mixture models: through imposing priors on the regression coefficientvector β which have heavy tails and are “spiky” around zero. These priors have most of theirprobability mass distributed both close to zero and far away in the tails, but not so much atintermediate values. This enforces a clear separation in the a posteriori estimates, becausethey are either very close to or equal zero (corresponding to the model-exclusion componentin mixture models) or far away from zero (the model-inclusion component). Compare Figure3.3 which illustrates NEG priors and the normal-Jeffreys prior with Figure 3.5 which showsan example for a spike-and-slab prior by Mitchell and Beauchamp (1988) as well as a normalmixture prior as described in (3.55).beta−4 2 0 2 4beta−4 2 0 2 4Figure 3.5: Spike-and-slab prior 0.4δ0 + 0.6Uniform(−5, 5) by Mitchell and Beauchamp (1988) (left)and normal mixture prior (3.55) with τ = 0.2 and c2 = 10 (right), both for a single variable βi.Equation (3.55) and the use of the inverse gamma prior for the model variance σ2 resultin the following linear regression model, where R is the prior correlation matrix of β andDγ = diag(aγ1τ1, ..., aγpτp) (aγi = 1 if γi = 0 and aγi = ci if γi = 1) determines the scaling of54the covariance matrix so that equation (3.55) is satisfied in the case where R = Ip:y ∼ N(Xβ, σ2In) (3.57)β ∼ N(bγ = 0, vγ = DγRDγ)σ2 ∼ IG(ν/2, νλγ/2)γ ∼ p(γ).A subscript γ indicates that the parameter depends on the state of γ. The prior p(γ) couldbe chosen, for example, so that the γi’s are independent with marginal distributions given by(3.56):p(γ) =p∏i=1πγii (1− πi)1−γi . (3.58)George and McCulloch (1993) call their procedure Stochastic Search Variable Selection (SSVS),because it was mainly intended as a stochastic search algorithm to identify “promising” covari-ate subsets rather than to compute the complete posterior distribution. This is true for all proce-dures presented here, especially if the number in potential input variables is large, i.e. p > 25,since then an exhaustive sampling of the entire posterior becomes practically impossible due tothe vast model space of dimension 2p.Note that in the SSVS approach all regression coefficients βi are estimated, even for vari-ables which are currently not included in the model, i.e. where γi = 0. This is an unnecessarycomputational burden when p >> n, especially since in these situations usually only a verysmall proportion of variables are expected to be included in any MCMC iteration.Holmes and Held (2006) specify an alternative model where βγ ∼ p(βγ|γ) is only deter-mined for those variables where γi = 1, so that the dimension of βγ is equal to pγ , the numberof variables included in the model. Holmes and Held (2006) implement their Bayesian variableselection procedure for a binary regression model. The full model is specified in detail in thefollowing section.The hierarchical mixture model for variable selection by George and McCulloch (1993)was extended to a wide variety of approaches by George and McCulloch (1997). The origi-nal SSVS procedure is a non-conjugate implementation of the hierarchical Bayesian mixturemodel, since the prior of β is independent of σ2. George and McCulloch (1997) present conju-gate versions which allow for the implementation of a fast-update algorithm that substantially55speeds up the evaluation of the posterior distribution. The conjugate approach also allows theexact calculation of relative posterior probabilities of γ. The setup is very similar to the non-conjugate setup given in (3.57), except that now the model variance σ2 is included in the priordistribution of β, and the prior correlation matrix R is replaced by a correlation matrix Rγwhich depends on γ:β|σ2, γ ∼ N(0, σ2D∗γRγD∗γ), (3.59)where D∗γ is a diagonal matrix as in (3.57) with diagonal elements of D∗2γ being denoted by v∗0γiif γi = 0 and v∗1γi if γi = 1. As in the non-conjugate case, (3.59) is equivalent to βi comingfrom a mixture of two normal distributions:βi|γi ∼ (1− γi)N(0, σ2v∗0γi) + γiN(0, σ2v∗1γi). (3.60)A special case of the conjugate hierarchical mixture prior is a “spike and slab” prior, where v∗0γiis zero, and the exclusion mixture component degenerates to the Dirac distribution δ0 with allprobability mass on the value zero (e.g. Raftery et al. 1997, Smith and Kohn 1996).The most important advantage of using the conjugate hierarchical mixture prior is thatit enables margining out β and σ2 from the joint posterior distribution p(β, γ, σ2|X, y). Thisreduces the computational burden, if one is only interested in inference on the posterior dis-tribution of the covariate sets γ. This is usually the case for variable selection; the regressioncoefficients β only become interesting afterwards for the final selected regression model(s).MCMC schemes, where only one variable is updated in each iteration, profit especially fromthe conjugate hierarchical prior specification, because it allows the implementation of a fast-updating algorithm (Chambers 1971). In order to draw from the posterior distribution of β,one has to invert the p × p matrix (v−1 + XT X) (see equations 3.54). The larger the numberp of potential input variables gets, the larger the burden from having to compute the inverse(v−1 + XT X)−1 becomes. Chambers (1971) provides a fast-update algorithm for regression,where the Cholesky decomposition of a matrix X is used in order to obtain updated regressioncoefficient estimates without having to re-compute the inverse of XT X , if only one variable isadded or removed. This fast-updating algorithm can also be applied to the matrix (v−1+XT X),if certain prior specifications are used. For example, it is valid if v∗0γi > 0 and Rγ = I , or ifv∗0γi = 0 and D∗1γR1γD∗1γ = c2(XTγ Xγ)−1 (where D∗1γ and R1γ correspond to those variablesfor which γi = 1) (see George and McCulloch 1997).56The conjugate hierarchical mixture prior was generalised to a multivariate setting byBrown et al. (1998a,b). The authors used the fast-updating scheme to perform Bayesian vari-able selection for data with 160 variables. If the number of variables gets much larger than that,the algorithm by Chambers (1971) cannot be used as such, because it requires that the numberof rows of input matrix X are fixed for an efficient implementation. This means that in Brownet al. (1998a,b) a new input matrix of dimension (n + p)× pγ had to be formed by adding theidentity matrix Ip of dimension p× p to the original input matrix XX∗ = XIp , (3.61)which leads to many unnecessary computations, especially if the number of selected variablespγ will generally be much smaller than p. Brown et al. (2002) make the fast-update algorithmapplicable to data sets with several thousand potential input variables by including a maximumdimension d which reduces the dimension of the new input matrix to (n+d)×pγ . The maximumdimension can be dynamically altered, so it does not restrict the model search space.The logistic Bayesian variable selection model by Holmes and Held (2006)A conjugate formulation for a Bayesian binary regression model with response y = (yj)nj=1 ∈{0, 1}n was first developed for the probit model by Albert and Chib (1993) by introducinga latent variable z = (zj)nj=1 which has a normal prior distribution and hence a conjugatenormal posterior distribution. The binary response variable is modelled by the probit link in adeterministic manner:yj =1 if zj > 00 otherwise(3.62)zj = xjβ + εjεj ∼ N(0, 1)β ∼ p(β).where p(β) is the prior distribution of regression coefficient vector β = (βi)pi=1.Usually, the conjugate normal prior distribution is chosen for β: p(β) = N(b, v), wherethe hyper-parameter b is the prior mean vector and v is the prior covariance matrix. Often,57the zero vector b = 0 is chosen and either the independence prior covariance matrix v = c2Ipor the g-prior covariance v = c2(XT X)−1. Standard techniques can be applied for inferenceof the posterior distribution of z (and hence of y), for example Markov chain Monte Carlosampling. Holmes and Held (2006) point out that, as in any Bayesian regression model, apotential problem arises from the fact that the MCMC samples of z are correlated with thesamples of β, as can be seen from the model (3.62). This over-conditioning is likely to causeslow mixing of the Markov chain, if β and z are updated separately.Often, the logistic regression model is preferred over the probit model in biostatistical ap-plications, as it is more familiar to clinicians and provides regression coefficients that are moreinterpretable, due to their connection to odds ratios (see Section 4.1). In addition, posteriorinferences in the logistic model seem to be less sensitive to the choice of the prior covarianceof β (see sensitivity analysis in Section 6.4.3). Also, while the problem of over-conditioningbetween z and β still exists in the auxiliary variable formulation of the Bayesian logistic modelthat is outlined in the following, the problem is less severe than for the probit model. Thisis because rather than being sampled from a truncated normal distribution N(xβ, 1)I(z, y) asin the probit model, in the logistic model z is sampled from a truncated logistic distributionLogistic(xβ, 1)I(z, y). Here, I(z, y) is used as a shorthand to denote an indicator function thattruncates the distribution of z to the appropriate region. As the logistic distribution has heaviertails than the normal distribution, the resulting correlation between z and β is smaller in thelogistic model than in the probit model.Here we outline the data augmentation formulation of the Bayesian logistic model devel-oped by Holmes and Held (2006), which like the probit model uses a latent variable z withconjugate normal priors:yj =1 if zj > 00 otherwise(3.63)zj = xjβ + εjεj ∼ N(0, λj)λj = (2φj)2φj ∼ Kolmogorov-Smirnov (i.i.d.)β ∼ p(β).58The auxiliary variables φj , j = 1, ..., n, are independent random variables following the Kolmo-gorov-Smirnov (KS) distribution (e.g. Devroye 1986). This leads to a normal scale mixturedistribution for εj resulting in a marginal logistic distribution, so that this model is equivalent toa Bayesian logistic regression model (Andrews and Mallows 1974). We assume from here on,that the prior distribution of β is normal with p(β) = N(b, v); then the posterior distributionof β is still normal with mean B and covariance matrix V , according to standard Bayesianmodelling theory (e.g. Lindley and Smith 1972):β|z, λ ∼ N(B, V ) (3.64)B = V (v−1b + XT λ−1z)V = (v−1 + XT λ−1X)−1λ−1 = diag(λ−11 , ..., λ−1n ).Holmes and Held (2006) extend their Bayesian logistic regression model to incorporatevariable selection by including a covariate indicator variable γ ∈ {0, 1}p, which corresponds tothe indicator variable in the hierarchical mixture model setup for variable selection describedearlier. We denote the size of the active covariate set by pγ =∑pi=1 I(γi = 1), where I is anindicator function. Then, the Bayesian logistic model for variable selection is given byyγj =1 if zγj > 00 otherwise(3.65)zγj = xγjβγ + εjεj ∼ N(0, λj)λj = (2φj)2φj ∼ KS (i.i.d.)βγ ∼ N(bγ, vγ)γ ∼ p(γ).A γ subscript indicates that the variable is only defined for those components i for which γi = 1.The prior on the model space is specified in terms of the prior distribution p(γ), which is theproduct of the Bernoulli distributions of the independent binary variables γi with individualprior probabilities πi:p(γ) =p∏i=1πγii (1− πi)1−γi . (3.66)59This means that the hyper-prior parameter vector (πi)pi=1 has to be chosen. In our applicationswe set small constant prior probabilities πi = π = p∗/p for all i = 1, ..., p so that a priori theexpected number of covariates is p∗. The model size p∗ is chosen to be small, for example fiveor ten, in order to favour sparse models a priori (see Chapters 6 and 7). Alternatively, one canchoose a more flexible Beta-Binomial distribution for γ (Kohn et al. 2001):p(γ) =∫p(γ|π)p(π)dπ, (3.67)where p(γ|π) = ∏pi=1 πγi(1− π)1−γi = πpγ (1 − π)p−pγ and with the additional hyper-priordistribution for π:p(π) = πa−1(1− π)b−1/B(a, b), (3.68)where B(a, b) is the Beta function. The parameters a > 0 and b > 0 are chosen so that mostprobability mass is on small values for π, and consequently sparse models are favoured again.The prior distribution of the regression coefficient vector βγ , N(bγ, vγ), is only defined forthose variables for which γi = 1, e.g. bγ = 0pγ×1 and vγ ∈ Rpγ×pγ . Throughout this thesis, theindependence prior is used, i.e. vγ = c2Ipγ , where Ipγ is the identity matrix of size pγ × pγ .The hierarchical logistic regression model (3.65) leads to the following joint posteriordistribution for {βγ, γ, z, λ} (Holmes and Held 2006):p(βγ, γ, z, λ|Xγ, y) ∝ p(βγ, γ, z, λ, y|Xγ) (3.69)= p(y|z)p(z|λ, βγ, Xγ)p(βγ|γ)p(γ)p(λ),wherep(λi) ∼ 14√λiKS(0.5√λi)andp(z|λ, βγ, Xγ) = N(Xγβγ, λ).In Chapters 6 and 7 the logistic variable selection model is applied in several simulationstudies and to a real gene expression data set. In a sensitivity analysis using simulated data, thefollowing equivalent latent-variable probit variable selection model is also applied for compar-60ison reasons:yγj =1 if zγj > 00 otherwisezγj = xγjβγ + εjεj ∼ N(0, 1)βγ ∼ N(bγ, vγ)γ ∼ p(γ). (3.70)MCMC samplers for the Bayesian logistic variable selection modelAn obvious Markov chain Monte Carlo algorithm in the context of Bayesian variable selectionis Gibbs sampling, where the indicator variable for the covariate sets γ can be updated sepa-rately from the other model parameters like β and any auxiliary variables, for example z and λin the logit regression model specified above. The covariate set indicator γ is updated first, andthen the other variables are sampled conditionally on the new γ. The efficiency of this genericGibbs framework can be improved in several ways. For the linear regression model there areconjugate implementations of the hierarchical mixture model for BVS, which make it possibleto integrate out all model parameters except the covariate indicator, which increases the speedof posterior inference for γ dramatically (George and McCulloch 1997) as mentioned at thestart of this section. This approach was applied by both Brown et al. (1998a) and Brown et al.(1998b). Within that framework, Brown et al. (1998a) used a Gibbs algorithm to sample theindicator vector γ, updating the entire vector γ in every iteration by sampling from the fullconditional distributions p(γi|γ−i, z) for i = 1, ..., p. Brown et al. (1998b), on the other hand,applied an add/delete/swap Metropolis-Hastings algorithm to update the γ vector instead.The add/delete/swap algorithm was first introduced by Madigan and York (1995) forBayesian graphical models. In each iteration, a swap move is proposed with probability π̃and an add/delete move is proposed otherwise. If the swap move is attempted, then two vari-ables γi and γk are selected at random and their states are swapped with the correspondingMetropolis-Hastings acceptance probability. Alternatively, in the add/delete move, a singlecovariate γi is selected at random and its state is changed from 0 to 1 or vice versa with itsMetropolis-Hastings acceptance probability.61The efficient MCMC algorithm outlined in the previous section for the linear regressionmodel can easily be adapted for binary classification via the auxiliary variable formulation ofthe probit regression model by Albert and Chib (1993) (for an application to gene expressiondata see for example Sha et al. 2004). However, Markov chain Monte Carlo inference for thelogistic model is more complex, as β cannot be integrated out, and also because the auxiliaryvariable λ needs to be sampled. This is costly, because the posterior variance matrix V of βchanges with every update of λ and hence has to be recomputed in every iteration. But it isstill possible to implement the MCMC sampler efficiently by using a blocked Gibbs sampler,where {z, λ} and {γ, βγ} are updated jointly, respectively. This is advantageous to updating allvariables individually in several ways. Firstly, variables are quite highly correlated due to theway they are constructed, especially βγ with γ and z with λ, which would lead to slow mixingof the Markov chains if they were not updated together. Secondly, this structure allows forefficient sampling from all distributions involved, see Table 3.1 below. This Gibbs algorithmwhich is proposed by Holmes and Held (2006) to sample from (3.69) is outlined in detail inAppendix B.1.As noted by Holmes and Held (2006) it is more efficient to update p(βγ, γ|z, λ, γ, X) byMetropolis-Hastings instead of Gibbs sampling, especially when using a proposal distributionwhich leads to an acceptance probability that does not involve β (see below), because thisimplies that β only needs to be updated whenever the move is accepted.Holmes and Held (2006) propose to use an add/delete proposal distribution q(γ) in theMetropolis-Hastings step for updating p(βγ, γ|z, λ, X), which is similar to the add/delete/swapalgorithm by Madigan and York (1995). That is, in each iteration, a single covariate γk isselected at random and the value of γk is proposed to be changed from 0 to 1 or from 1 to 0,respectively, so that the proposal distribution q(γ∗) is given byq(γ∗i ) =γi if i 6= k1 if i = k and γk = 00 if i = k and γk = 1for i = 1, ..., p. (3.71)Note that this impliesp(γ∗)q(γ)p(γ)q(γ∗)=1−πkπkif γk = 1πk1−πk if γk = 0, (3.72)if p(γ) =∏pi=1 πγii (1− πi)1−γi is the prior distribution for the covariate indicator variable γ.62Gibbs sampler for p(βγ , γ, z, λ|X, y) Sampling method(1) Sample from p(z, λ|β, γ, X, y) = p(z|β, γ,X, y)p(λ|z, β, γ,X)with p(zj |β, γ, X, y) =Logistic(xγjβγ , 1)I(zj > 0), yj = 1Logistic(xγjβγ , 1)I(zj ≤ 0), yj = 0and inversion methodp(λj |z, β, γ,X) ∝ p(zj |λ, β, γ, X)p(λj) = N(xγjβγ , λj) 14√λjKS(0.5√λj) rejection sampling(2) Sample from p(βγ , γ|z, λ,X) = p(γ|z, λ, X)p(βγ |γ, z, λ,X) Metropolis-Hastingswith proposal distributionq(γ∗, β∗γ) = p(β∗γ |γ∗, z, λ, X)q(γ∗) = N(B∗γ , V ∗γ )q(γ∗) whereB∗γ = V ∗γ (v−1γ∗ bγ∗ + X′γ∗λ−1z) and V ∗γ = (X ′γ∗λ−1Xγ∗ + v−1γ∗ )−1Table 3.1: Outline of MCMC algorithm for the logistic BVS model by Holmes and Held (2006): Gibbssampling p(βγ , γ, z, λ|X, y) by sampling from full conditionals p(z, λ|β, γ, X, y) and p(βγ , γ|z, λ,X).More detail is given in Appendix B.1.This results in the following acceptance probability for updating (γ, βγ), when the proposaldistribution given in Table 3.1 is used. For a detailed derivation of α(γ, βγ), refer to SectionB.2 in the Appendixα(γ, βγ) = min{1,p(β∗γ , γ∗|z, λ,X)p(βγ, γ|z, λ,X)q(γ, βγ)q(γ∗, β∗γ)}(3.73)= min1,|Vγ∗ |1/2|vγ|1/2|Vγ|1/2|vγ∗|1/2exp(0.5B′γ∗V−1γ∗ Bγ∗)exp(0.5B′γV−1γ Bγ)1− πkπk if γk = 1|Vγ∗ |1/2|vγ|1/2|Vγ|1/2|vγ∗|1/2exp(0.5B′γ∗V−1γ∗ Bγ∗)exp(0.5B′γV−1γ Bγ)πk1− πk if γk = 0.Note that βγ and βγ∗ do not occur in the acceptance probability and hence βγ∗ only needs tobe sampled if the move is accepted. The add/delete sampler is fast and efficient, but since onlyone randomly selected covariate is proposed to be updated per iteration, it results in very slowmixing of the Markov chain if the number of covariates p is large.In addition, as noted by Hans et al. (2007), in sparse situations with very small variable in-clusion probability π = p∗/p (where p∗ is the expected number of variables to be selected), theadd/delete sampler can encounter convergence problems, because the acceptance probabilityfor deleting variables can tend to zero with π → 0. This is because the algorithm will proposeto add variables much more often than to delete variables. This can lead to the sampler runningoff to include more and more variables. In sensitivity analyses presented in Chapter 6.4.3, we63found this to happen with the add/delete MCMC sampler applied to the probit variable selec-tion model (3.70) if the prior variance parameter c2 for β ∼ N(0, c2Ip) is chosen too large. Thelogistic variable selection model seems to be more robust to the choice of c2 in this respect, butnevertheless mixing remains a problem: in sparse situations only a small number of covariatesare related to the response variable while most do not carry information regarding y. In thatsituation, the randomly selected covariates are very unlikely to be related to the response andwill thus not be updated in most MCMC iterations. In addition, the sampler does not make useof the correlation structure among the covariates, which increases the likelihood of the samplergetting stuck: imagine a situation where two covariates xi and xk are moderately correlatedwith each other, and xi has a strong effect on the response while xk only has a comparablysmall effect on y. If by chance xk is included first (that is γk = 1) by the Metropolis-Hastingssampler then it might prevent xi from being included as long as it remains in the model, eventhough inclusion of xi would result in a better model fit.An alternative to the add/delete sampler is the use of an “inner” Gibbs sampler for γ,which updates all γi (i = 1, ..., p) within each iteration of the “outer” Gibbs sampler in Table3.1 by sampling from the full conditional distributionsp(γi|γ−i, z, X, λ) ∝ p(z|λ,X, γ)p(γi) = N(0, λ + XγvγX ′γ)πγii (1− πi)1−γi . (3.74)It fits in the Metropolis-Hastings algorithm outlined in Table 3.1 if viewing the full conditionaldistributions in (3.74) as proposal distributions q(γ) - which consequently leads to Metropolis-Hastings acceptance probabilities of α(γ, βγ) = 1 (see Appendix B.2).Note that here also βγ only needs to be updated once after the entire γ vector has beenupdated, which saves computation time. Like the add/delete sampler, which has been appliedto large-scale gene expression data (Sha et al. 2004, Tadesse et al. 2005), this sampler hasalso been applied to “large p” data (Lee et al. 2003). In all these applications the auxiliaryvariable model for probit regression by Albert and Chib (1993) was used. To our knowledge,the exact logistic regression setup by Holmes and Held (2006) has never been applied in alarge-scale context before. In Hans et al. (2007), the Laplace approximation has been usedto sample from a logistic BVS model with the shotgun stochastic search, a method which isintroduced in Section 3.3.2. The Laplace approximation to the marginal likelihood is p̂(y|γ) =(2π)p/2|Σ̂|1/2p(y|β̂, γ)p(β̂|γ) (DiCiccio et al. 1997), where Σ̂ is the negative inverse of the64Hessian matrix of log(p(y|β, γ)p(β|γ)) evaluated at β̂, which is the maximum a posteriori(MAP) estimate.The Gibbs sampler is more computationally intensive per iteration than the add/deletesampler, but it also results in better mixing of the Markov chains. Note that this samplerbecomes especially computationally intensive if pγ becomes large, since updating βγ requiresthe inversion of a pγ × pγ matrix when computing the posterior covariance matrix Vγ = (v−1γ +XTγ λ−1Xγ)−1 (see equation (3.64)). If pγ > n the computational burden is eased by applyingthe Sherman-Morrison-Woodbury formula (e.g. Schott 1997) V = (v−1γ + XTγ λ−1X)−1 =vγ − vγXTγ (XγvγXTγ + λ)−1Xγvγ , so that we only have to invert an n × n matrix instead.Another way to keep the required computational effort feasible is to restrict the model spaceto small models, either directly by only allowing to sample models with pγ ≤ C with a smallupper threshold C, for example C = 10 - or indirectly by setting the prior covariate inclusionprobabilities πi to very small values π = p∗/p (with p∗ = 5 or p∗ = 10), which is the approachchosen in the applications in Chapters 6 and 7.An intermediate solution between updating only one covariate in each iteration as in theMetropolis-Hastings add/delete sampler and updating the entire γ vector by the Gibbs samplerdescribed above is to create blocks of γ-components and to update one block in each iteration.In “large p, small n” applications it is often reasonable to assume a sparse dependence struc-ture between variables and that hence there is no need to update all variables together in eachiteration, but only covariates which are related. In Chapter 6, it is proposed to estimate thedependence structure prior to starting MCMC sampling and to use the estimated structure toconstruct a block sampler, where γ is updated in each iteration, e.g. by applying the “inner”Gibbs sampler only within a randomly selected block of variables.This approach only uses the dependence structure empirically to help decide which vari-ables to update together in the MCMC algorithm. Alternatively, the performance of Gibbssamplers can sometimes be dramatically improved by introducing auxiliary variables (Tierney1994) into the Bayesian model with the aim to capture the dependence structure; especially ifthere is some kind of natural structure in the predictor matrix X which introduces correlationsbetween the predictor variables. If the structure can be captured by auxiliary variables, thecorrelations within X can be reduced by conditioning on them. One successful example forthis approach is the Swendsen-Wang algorithm (Swendsen and Wang 1987) which was devel-65oped in the field of statistical physics for sampling from the Ising model (Ising 1925), whichhas a lattice structure introducing correlations between neighbouring positions. The Swendsen-Wang algorithm has been extended for general Bayesian inference by Higdon (1998) to reduceposterior dependence among covariates. An application to Bayesian variable selection wasimplemented by Nott and Green (2004), where the auxiliary variables create groups of covari-ates based on a standard multicollinearity diagnostic, the variance proportions. However, theSwendsen-Wang and similar algorithms have been developed in the context of regular corre-lation structures such as those arising in lattice structures. They do not seem to perform aswell for biological data like gene expression data, where the correlation structure is very irreg-ular (Nott and Green 2004, and personal communication with P. Green in 2005). In addition,these algorithms are more difficult to implement than our block samplers and by imposing anadditional structure on the data the model becomes less general.3.3.2 Speeding up the search algorithm: moving beyond a single MarkovchainIn the high-dimensional settings considered here, where the number of potential input variablescan be larger than 10, 000, it is not sensible to perform full posterior inference with standardMarkov chain Monte Carlo methods where only one Markov chain is run to sample from theposterior distribution of interest as the target distribution p(.). Especially regions of low pos-terior probability will be visited only rarely or never at all, making inference for these regionsimpossible. In the p >> n situation the problem is made even worse, because the posteriormodel space is multi-modal and a standard Markov chain sampler might not find all the modesand hence miss important areas containing models with comparably large posterior probability.There are several ways to improve sampling over a single Markov chain sampler. Onemethod is to force the Markov chain to mix faster, for example by modifying the target dis-tribution p(.) in a way that makes it easier to sample from, and then to re-adjust the samplesaccording to the original target distribution. Examples are importance sampling (e.g. Geweke1989) and simulated annealing (Kirkpatrick et al. 1983). Simulated annealing can be viewedas a form of importance sampling where the modified distribution p∗(.) = p(.)1/T is “heated”by a “temperature” parameter T , thus flattening the modes of the distribution and increasing66the probabilities between modes, which evens out the overall distribution and makes movesbetween modes easier. This “heating” approach can be applied with several temperatures,leading to several modified distributions according to a temperature ladder p∗k(.) = p(.)1/Tk(k = 0, ..., K) where 1 = T0 < T1 < ... < TK . The distributions with intermediate tempera-tures have the purpose of building a bridge between the “flattest” distribution with the highesttemperature TK and the target distribution of interest (where T0 = 1). The purpose is to en-able exchanges between these two distributions in situations where they themselves only sharelittle overlap. The temperature ladder can be incorporated within a single MCMC sampler,for example as in the simulated tempering method via an additional indicator variable for thetemperature (Marinari and Parisi 1992, Geyer and Thompson 1995). Alternatively, one can runseveral Markov chains in parallel, sampling from distributions at different temperatures, withoccasional information exchanges between the chains. Methods using several Markov chainsare collectively called population-based Monte Carlo methods (Liu 2001) and include paral-lel tempering which is also called Metropolis-coupled MCMC (Geyer 1991). Both, simulatedtempering and parallel tempering methods are introduced later on in this section.An alternative to these tempered MCMC approaches is to move away from the MCMCframework completely. Recall that in high-dimensional problems like microarray analysis, theobjective of a Bayesian variable selection approach if often reduced to identifying the high-probability regions only - contrary to estimating the complete posterior distribution of all pos-sible models. In this situation, the fact that MCMC is designed to explore the full posteriordistribution including the regions of very low density might become a disadvantage, since theMarkov chains spend too much time trying to sample from low-probability regions. Thus it isworthwhile to investigate other stochastic search algorithms which are specifically designed tofind the modal regions of high posterior probability. An example for such an algorithm is theshotgun stochastic search method (Hans et al. 2007), which is also described later on.Note that in combination with these approaches which move away from standard singleMarkov chain methods, other algorithms that have been developed in the context of single-chain MCMC methods can be employed in a complementary way in order to further speed upthe sampling process. This includes the MCMC block samplers which are developed in Chapter6. To demonstrate the benefit of combining these approaches, both parallel tempering (as anexample for tempering methods) and the shotgun stochastic search algorithm are applied to an67ovarian cancer gene expression data set (Schwartz et al. 2002) in Chapter 7, in addition to andcomplementing the MCMC block sampling approaches developed in Chapter 6.Tempered MCMC methodsAs outlined above, a single Markov chain sampling from a posterior distribution over a high-dimensional multi-modal model space is likely to mix poorly and to get stuck in local modes.One way to alleviate this problem is simply to run several Markov chains with different startingvalues, which might be chosen from a starting distribution which is over-dispersed compared tothe target distribution. This approach has the additional advantage that one can assess whetherthe Markov chains have converged, for example by comparing the MCMC variances withinMarkov chains to the between-chain variance in an ANOVA-like approach (Gelman and Rubin1992). However, since in the p >> n case all Markov chains are still sampling from a multi-modal distribution and are still likely to get trapped in local modes, it is still impossible toassess whether all modes and large posterior-density areas have been visited. This is wherethe idea of tempering comes in, that is the target distribution itself is “flattened”, for examplethrough the transformation p∗(.) = p(.)1/T , in order to make mixing easier.In the following, we assume that the complete states of Markov chains of different tem-peratures Tk and Tm are swapped if such a move is accepted in a Metropolis-Hastings update(exchange move). Complementary to that, it can also be useful to propose partial exchanges,where only part of the state of two chains are proposed to be swapped (crossover move), inparticular if the target distribution is high-dimensional and complex which can lead to low ac-ceptance probabilities for complete exchange moves. This framework of using several typesof moves for information exchange between parallel Markov chains has been adopted from thefield of genetic search algorithms and is commonly called evolutionary Monte Carlo (Liang andWong 2000). In addition to exchange and crossover moves, so-called snooker moves have alsobeen proposed, where the idea is to use a “good” (i.e. high-probability) Markov chain state topush another randomly selected current state towards that region of high probability (Goswamiand Liu 2007, Jasra et al. 2007). Evolutionary Monte Carlo algorithms have been extendedto models with varying dimensions by Jasra et al. (2007) and has recently been adopted forBayesian variable selection in the high-dimensional setting by Bottolo and Richardson (2008).In this framework, the moves which attempt to swap states between chains are called global68moves, whereas updates within the individual chains are called local moves. For more detailssee Liang and Wong (2000) and Liu (2001). For a general overview of tempered MCMC meth-ods, refer to the article by Iba (2001), in which this class of methods is referred to as ExtendedEnsemble Monte Carlo.Parallel tempering The two most common tempering methods are simulated tempering andparallel tempering and both are closely related (Gilks et al. 1996). Parallel tempering (Geyer1991) is also called Metropolis-coupled MCMC, because several Markov chains with differenttarget distributions are run in parallel, and at each iteration a Metropolis-Hastings step is in-cluded where it it proposed to swap the states θk and θm of two randomly selected chains k andm according to the acceptance probability:αPT = min{1,p∗k(θ(m))p∗m(θ(k))p∗k(θ(k))p∗m(θ(m))}. (3.75)If the target distributions of the Markov chains are of the form p∗k(θ) = p(θ)1/Tk , then the swapacceptance probability isαPT = min{1,p(θ(m))1/Tkp(θ(k))1/Tmp(θ(k))1/Tkp(θ(m))1/Tm}= min{1,(p(θ(m))p(θ(k)))1/Tk−1/Tm}. (3.76)For posterior inference on the (untempered) posterior distribution of interest, only theoutput from the untempered Markov chain is used (i.e. with temperature T0 = 1), and the outputfrom all other chains has to be discarded, which is a waste of computation time. However, themethod is ideally suited for parallelisation, since all chains can be run on parallel processors.The only information exchange that is necessary in each iteration is from the two chains whichare proposed for a swap. The states of these two chains only need to be exchanged in case ofacceptance of the swap move.If the target distributions are too far apart in the sense that values θ sampled from onedistribution frequently have low probability in the other distribution, then the acceptance prob-ability for swapping the Markov chain states is going to be small, leading to low mixing be-tween the chains. Therefore, the tempered distributions need to be chosen carefully to ensurea large enough overlap for those probability distributions which are often selected for a stateswap proposal. Values that have been suggested as “good” values for the expected swap accep-tance probability (Gilks et al. 1996, Goswami and Liu 2007) range from 15% to 50%, based69on simulation results and on asymptotic results for the general Metropolis-Hastings sampler(e.g. Gelman et al. 1996). More recently, there have been several publications in the field ofstatistical physics that have come to similar conclusions when examining parallel tempering invarious thermodynamic systems. In particular, for various idealised situations and using differ-ent optimality criteria, Kone and Kofke (2005), Predescu et al. (2005) and Rathore et al. (2005)found optimal acceptance probabilities of 23%, 39% and 20%, respectively. In the context ofthermodynamics, parallel tempering is also called replica-exchange Monte Carlo (Hukushimaand Nemoto 1996), and deals with exchanges between systems of different temperatures usingthe Boltzmann energy distribution:π(Ek(θ)) =exp(−Ek(θ)/(κBtk))Z(tk)=exp(−Ek(θ)/(κBtk))∫exp(−Ek(θ)/(κBtk))dEk(θ) , (3.77)where κB is the Boltzmann constant, Ek(θ) the energy of a system at temperature tk, andZ(tk) is the normalising constant. If we replace κBtk by our temperature parameter Tk andthe energy Ek(θ) by the negative log density of θk (with θk being a sample from the tempereddistribution with temperature Tk) under the target distribution of the untempered Markov chain,i.e. Ek(θ) = − log p(θk), then the Boltzmann distribution representation π(Ek(θ)) is equivalentto our representation of a tempered distribution p∗k(θk) = p(θk)1/Tk , except for the normalisingconstant.Since Markov chains, for which the temperatures are far apart, can have very low swap ac-ceptance probabilities, such pairs of chains should not be proposed for a swap too often becausethat would lead to low overall mixing between the parallel chains. It is a common proposal toonly consider swaps between chains which are direct neighbours (i.e. where temperatures areneighbours in the temperature ladder), since the target distributions of these chains will havethe largest overlap. For this case it has been suggested (e.g. Kofke 2002) to use a geometrictemperature ladder with {T0 = 1, T1 = τ, T2 = τ 2, T3 = τ 3, ..., TK = τK} (τ > 1), since thenthe acceptance probabilities for swapping any of the pairs of neighbouring Markov chains areoften similar.The disadvantage of only proposing directly neighbouring chains for swaps is that therewill be no bolder moves between chains which are further apart. Such moves could be usefulif several of the chains at the “bottom”, i.e. with low temperatures, get stuck in local modes,because then simply swapping between any of these lower-temperature chains will not help to70escape from these traps. In this situation, the all-exchange parallel tempering method proposedby Calvo (2005) can help. The author suggests to compute all Nc = K(K + 1)/2 possiblepairwise swap acceptance probabilities (pkm)k>m for all parallel chains in each iteration and tosample the pair (k,m) that is to be swapped according to this probability distribution. This iseasily feasible in typical situations where only a small number of parallel chains is considered,e.g. five or ten. In order to fulfill detailed balance, the possibility not to perform a swap is alsoincluded; the author suggests a relative probability of p0 = 1 for this, so that the probability ofno swap is equal to the probability of swapping. One specific move (either a swap between aspecific pair of chains or the rejection move) is randomly selected according to the normalisedvector of probabilities p/∑Ncc=0 pc, where p = (pc)Ncc=0 = (p0, (pkm)k>m). The sampled move isalways performed, making this a Gibbs sampler rather than a Metropolis-Hastings sampler forswapping states between chains.Simulated tempering Whereas the parallel tempering method uses K + 1 MCMC samplersrunning in parallel and sampling from different distributions p∗k(θ) (k = 0, ..., K), simulatedtempering (Marinari and Parisi 1992, Geyer and Thompson 1995) only uses a single MCMCchain, but with an added indicator variable It ∈ {0, ..., K} which indicates from which of theK + 1 distributions p∗k(θ) the sampler is sampling in the current MCMC iteration t. Hence,one iteration of the simulated tempering algorithm consists of an update of the current value θtaccording to the current state of It, followed by an update of It. For posterior inference, onlythose iterations are used where It = 0, i.e. where the untempered distribution has been thetarget distribution. So all samples from tempered distributions have to be discarded as in theparallel tempering approach. But since here, the MCMC samplers cannot be run in parallel,parallel computing resources cannot be used to save computing time.Again, one has to be careful that the acceptance probabilities for moves are not too small,because then there would not be enough flow of information between the distributions for a realbenefit in terms of improved mixing in the untempered Markov chain, especially with respectto the increased computational burden due to all the additional MCMC samples where It > 0.Hence, it is common to only allow updates of It with It+1 = It − 1 or It + 1, i.e. again onlymoves to neighbouring temperatures are permitted. This is done via the following proposal71distribution, where I∗t+1 indicates the proposed value for I in iteration t + 1.q(I∗t+1 = k + 1|It = k) = q(I∗t+1 = k|It = k + 1) = 0.5 for k ∈ {1, ..., K − 1}q(I∗t+1 = 1|It = 0) = q(I∗t+1 = K − 1|It = K) = 1.Then, the Metropolis-Hastings acceptance probability for updating the temperature indi-cator variable It is given as (Gilks et al. 1996):αST = min{1,p∗m(θt+1)q(I∗t+1 = m|It = k)p∗k(θt+1)q(I∗t+1 = k|It = m)}. (3.78)Note that for simulated tempering, unlike parallel tempering, the normalising constants, whichare given by∫∞−∞ p∗k(θ)dθ, do not cancel out in the Metropolis-Hastings acceptance probability,which can be a problem. In particular, if the normalising constants are much larger for thechains of high temperature than for the untempered chain, then the Markov chains with largeT will be sampled from more often, since their total probability mass is then larger than that ofthe untempered chain of interest. Hence here, contrary to parallel tempering, it is necessary toinclude approximations of the normalising constants in the acceptance probability, although noexact computation is needed.Tempering in the Bayesian logistic variable selection model In Chapter 7 we are going toapply parallel tempering in the context of an application to ovarian cancer gene expression datato sample from the logistic Bayesian variable selection model that was introduced earlier andspecified in (3.65). Here, a way of tempering the quite complex posterior distribution of thejoint parameter vector θ = (z, λ, γ, βγ) is proposed. Recall, that the untempered joint posteriordistribution is given asp(βγ, γ, z, λ|X, y) ∝ p(βγ, γ, z, λ, y|X)= p(y|z)p(z|λ, β, γ, X)p(βγ|γ)p(γ)p(λ),We propose to only temper p(z|λ, β, γ, X), and in the following wayp∗T (z|λ, β, γ, X) = Nz(Xγβγ, Tλ), (3.79)that is, the temperature parameter T > 1 is introduced as a scalar multiplied to the covariancematrix λ. Since p(z|λ, β, γ, X) = Nz(Xγβγ, λ) is a normal distribution, this is equivalent to72the annealing approach described above, where a distribution is “heated” by the exponent 1/T ,except that here the normalising constant Z(θk) is included to regain a probability density. Themodified posterior distribution corresponds to the following modified hierarchical modelyγj =1 if zγj > 00 otherwisezγj = Xγjβγ + εjεj ∼ N(0, Tλj)λj = (2φj)2φj ∼ KSβγ ∼ N(bγ, vγ)γ ∼ p(γ) =p∏i=1πγii (1− πi)1−γi .The Gibbs sampling approach for inference on the Bayesian logistic variable selectionmodel, as outlined by Holmes and Held (2006) and given in detail in the Appendix B.1, hasto be adjusted accordingly in order to allow sampling from the tempered posterior distribution.The adjusted sampling algorithm is detailed in Appendix B.3.Shotgun stochastic search algorithmWhen there are many more variables than samples, the data are characterised by high collinear-ity, which implies that for every “good” model there are equally good neighbouring modelswhere one or several covariates have been substituted by highly correlated covariates. The ef-ficiency of a search algorithm is improved by adapting to this feature of the data, that is if itcan evaluate all neighbouring models of a selected model simultaneously in a fast and efficientway. A Gibbs sampler does not do this at all. The addition/deletion/swap Metropolis-Hastingssampler described earlier in Section 3.3.1 addresses the situation partially by only proposingmodels which are in the immediate neighbourhood of the current model, i e. where only one ortwo variables are different, but it does not explore the entire neighbourhood simultaneously.The shotgun stochastic search (SSS) approach for “large p, small n” regression (Hans et al.2007) is a modification of the Metropolis-Hastings sampling algorithm, which was constructedto do exactly that. Each iteration in the SSS procedure starts by using the current model to73define a neighbourhood of proposal models, all of which are then evaluated, which can bedone in a parallelised way. The authors provide both serial and parallelised implementationsof their algorithm. The SSS procedure chooses one of the proposal models as the new currentmodel for the next iteration on the basis of their evaluated (unnormalised) posterior probabilitiescomputed by Laplace approximations. As in the addition/deletion/swap Metropolis-Hastingsalgorithm, the neighbourhood of a model consists of three sets of models, namely γ+ whichcontains all models with one covariate added, γ− containing all models with one covariatedeleted, and the set γ0 of models where the states of two variables have been swapped. Se-lecting one model from the entire neighbourhood at random would mean that there is a highprobability for choosing a model of the same dimension as the current model, as |γ0| is muchlarger than |γ+| and |γ−|. In the Metropolis-Hastings algorithm by Madigan and York (1995),this was solved by proposing either an addition/deletion move or a swap move according to aspecified probability. Hans et al. (2007) propose to select one model from each of the three setsand choosing the new current model from these three models. This ensures that the dimension-ality of the selected models does not get stuck. Note that, contrary to the addition/deletion/swapMetropolis-Hastings sampler, the SSS algorithm will accept every proposed move, thus movingmore quickly in the model space. This can be done as the resulting chain from the SSS pro-cedure is not used for posterior inference via Monte Carlo integration, but only as a stochasticsearch tool to explore the model space. However, even though the SSS algorithm can investi-gate a much larger proportion of the model space than can be done via conventional MCMC,especially when the potential for parallelisation is used, the exploration will still only cover asmall proportion of the model space, if there are several thousand potential input covariates.The shotgun stochastic search algorithm has been applied to high-dimensional gene ex-pression data by Hans et al. (2007). The SSS algorithm has been implemented for logisticregression using Laplace approximations to sample from the logistic distribution, contrary tothe auxiliary variable approach for exact sampling proposed by Holmes and Held (2006) whichis used throughout this thesis. Hans et al. (2007) use very similar prior specifications to the onesused throughout the applications in Chapters 6 and 7 which makes results easily comparable.In particular, the independence prior for βγ is applied, that is βγ ∼ N(0, c2Ipγ ). Also, the priorover the model space is given by the binomial prior p(γ) =∏i πγi(1− π)1−γi , and the priorprobability π was chosen to be very small in order to encourage the selection of sparse models.74Chapter 4Binary classification4.1 Statistical models for binary classificationFor reasons of simplicity, all formulae presented in Chapter 3 assumed the standard linearmodel y ∼ N(Xβ, σ2In), with the exception of the logistic and probit Bayesian variable selec-tion models developed in Section 3.3.1. However, we are mainly interested in situations wherethe response y is a binary variable, representing two classes, for example two tumour types, andwe want to use gene expression data to predict, into which class a new observation belongs.In this section, several statistical models for this binary classification problem are presented.All dimension reduction methods, which have been introduced in the previous chapter, can beapplied to all these binary classification models. The applications presented in Chapters 5 to7 focus on logistic regression as one example for all these statistical models. Throughout thischapter, all models are formulated for one observation (y, x), where x is of dimension 1×p andy is a scalar, rather than n observations as in Chapter 3. For one observation, the formulationof the linear model simplifies to y ∼ N(β0 + xβ, σ2), where β0 denotes the intercept (i.e. xand y are not required to be centered around zero here).Logistic regression Logistic regression is often the method of choice in the statistical com-munity for performing binary classification, the main reason being that it is fully probabilisticand does not make any assumptions on the distribution of X . Logistic regression attempts tomodel the class probabilities in terms of a linear function of the input variables. In order toensure that the estimated class probabilities lie within [0, 1] and sum up to 1, the linear term75β0 + xβ is connected to the class probability p = Pr(y = 1|X = x) via the logistic linkfunction:logp1− p = logit(p) = β0 + xβ (4.1)The probability for the other class q = Pr(y = 0|X = x) is then simply estimated as q̂ = 1− p̂.The estimation of class probabilities provides measures of uncertainty for class member-ship - contrary to other classification algorithms like neural networks and classical supportvector machines, which only give point estimates for class membership, but no estimates of theassociated uncertainty. The connection between logistic regression estimates and odds ratiosis another appealing feature, especially in the medical community, where logistic regressionis a relatively well-known statistical method and hence enjoys more support than other classi-fication methods. The regression coefficient estimate β̂i can be interpreted as the amount, bywhich the log odds for class 1 versus class 0 changes, if the corresponding input variable xi ischanged by one unit.Probit regression Probit regression is another example of a generalised linear regressionmodel and is very similar to logistic regression, except that the standard normal distributionfunction Φ is used to create the link between the class probability Pr(y = 1|X = x) and thelinear function β0 + xβp = Pr(y = 1|X = x) = Φ(β0 + xβ) ⇔ Φ−1(p) = β0 + xβ. (4.2)Predicted probabilities with the probit and logit link functions are often very similar (see Figure4.1 for an example) and hence regression coefficient estimates are also very close.Linear discriminant analysis (LDA) Another classical discrimination approach, which usesthe linear functions β0 + xβ for classification, is linear discriminant analysis. The originalalgorithm was proposed by R.A. Fisher and aims to optimise the linear function β0 + xβ withrespect to maximising the ratio of between-groups to within-groups variances. Fisher’s lineardiscriminant analysis (FLDA) is a non-parametric method, which has a parametric equivalentin maximum-likelihood-based LDA (ML-LDA). In the binary classification situation, FLDAgives the same solution as the maximum likelihood classifier for multivariate normal classdensities, if the two classes are required to have the same covariance matrix Σ (see e.g. Hastie76−2 −1 0 1 20.00.20.40.60.81.0xpredicted probability values Pr(Y=1|X=x)logitprobitFigure 4.1: Predicted class probabilities P̂ r(y = 1|X = x) resulting from logistic and probit regressionmodels, respectively. The training samples were simulated from X ∼ N(0, 1) and y ∼ Bernoulli(Φ(x)).et al. 2001). In this situation, the class densities are given aspk(x) =1(2π)p/2|Σ|1/2 exp(−1/2(x− µk)Σ−1(x− µk)T ), (4.3)where µk is the mean vector of class k (k = 0, 1). Bayes’ theorem relates the posterior classprobabilities Pr(y = k|X = x) with the class densities (or likelihoods) pk(x) and prior proba-bilities πk of belonging to class k:Pr(y = k|X = x) ∝ pk(x)πk. (4.4)Hence, the log-ratio of the posterior class probabilities is given by the following equation; oneoften chooses π0 = π1 = 1/2 so that the corresponding term cancels out. We again denoteP̂ r(y = 1|X = x) = p̂.logP̂ r(y = 1|X = x)P̂ r(y = 0|X = x) = logp̂1− p̂ = logp̂1(x)p̂0(x)+ logπ1π0= logπ1π0− 1/2(µ̂1 + µ̂0)Σ̂−1(µ̂1 − µ̂0)T + xΣ̂−1(µ̂1 − µ̂0)T= β̂∗0 + xβ̂∗ (4.5)77Equations (4.5) and (4.1) look the same, but the coefficients β and β∗ are estimated indifferent ways. Whereas ML-LDA assumes normal class distributions (with equal covariancematrix Σ for both classes), logistic regression assumes no particular distribution for Pr(X).This results in a loss of efficiency when using logistic regression if the ML-LDA assumptionis true, depending on the Mahalanobis distance ∆M between the two class distributions as wellas on the prior class probabilities πk. Efron (1975) assumed that a typical data situation theMahalanobis distance between classes needs to be larger than about 2.5 for logistic regres-sion and ML-LDA to have “reasonable” discriminative power. Efron (1975) showed that for∆M = 2.5 the asymptotic efficiency of logistic regression relative to ML-LDA is 78.6%, ifπ0 = π1. That is, logistic regression requires about 27% more samples to achieve the samediscriminative power as ML-LDA. With increasing ∆M the asymptotic relative efficiency oflogistic regression decreases further. On the other hand, logistic regression avoids having toestimate the covariance matrix Σ and is more robust to outliers. In practical applications, lineardiscriminant analysis and logistic regression often produce very similar solutions (Hastie et al.2001).Classical statistical classification models like probit regression, logistic regression, andmaximum-likelihood linear discriminant analysis, as they were introduced here, assume thatthe data provide many more samples than there are parameters to be estimated, that is n >> p.Hence, regularisation or dimension reduction methods such as those presented in Chapter 3need to be applied in order to make these models applicable to “large p, small n” data sets likegene expression data. Another popular method for binary classification is the (linear) supportvector machine (Vapnik 1995). Contrary to probit or logistic regression and ML-LDA, supportvector machines can deal well with situations where p >> n, because they inherently apply anL2-penalty term to the size of the coefficient vector, that is they apply ridge-like shrinkage. Thesupport vector machine framework can naturally be extended to other penalty terms as well,for example to the L1-penalty.Support vector machines (SVM) and related methods Linear support vector machinesclassify the data by finding the optimal separating hyperplane between classes in the train-ing data. The optimal separating hyperplane is the hyperplane with the largest margin, whichis defined as the sum of the distances to the hyperplane of the closest data points on either side78of it (see Figure 4.2). The data points which are closest to the hyperplane, and hence define themargin, are called support vectors. If p >> n, the data are always linearly separable, that isit is always possible to find a hyperplane which will completely separate the classes. It turns< >margin>w<− b||w||originFigure 4.2: Linear support vector machine: separating hyperplane in the linearly separable case. Bulletsrepresent data points which are coloured differently according to their class labels; big bullets representsupport vectors.out that the logistic regression and linear support vector machine models mainly differ in theunderlying loss functions (see Section 4.3). Logistic regression is based on the log-likelihoodloss L(y, p(x)) = −2` (also called deviance or cross-entropy in information theory). Contraryto the usual notation for models like logistic (or probit) regression, the SVM model usuallyassumes that the binary class variable y takes values −1 and 1 (instead of 0 and 1), to makeformulations simpler. Equation (4.6) shows the log-likelihood function for the logistic modelformulated for the situation with y taking the values −1 and +1, assuming the data x are cen-tered so that the intercept term β0 is not needed:` = − log(1 + exp(−yp(x))) with p(x) = xβ. (4.6)Support vector machines find the solution with regard to the so-called hinge loss functionL(a) = [1− a]+, where [1− a]+ = 1− a if a < 1 and [1− a]+ = 0 if a ≥ 1.The original support vector machine algorithm has the disadvantage that it only providespoint estimates for class membership but no associated measure of uncertainty. However,equivalent fully probabilistic classifiers have been developed, which are also based on the79hinge loss and which are suitable for full Bayesian analysis. A sparse Bayesian regressionmodel, which utilises the same kernel basis as the kernel-based version of support vector ma-chines introduced later, is the relevance vector machine proposed by Tipping (2001). Anotherexample for a full Bayesian approach is the implementation of a binary classification modelbased on a reproducing kernel Hilbert space by Mallick et al. (2005), which is briefly presentedhere. Mallick et al. (2005) construct Bayesian hierarchical models for classification asp(y|z) ∝ exp(−L(y, z)) (4.7)z = p(x) + εε ∼ N(0, σ2),where L can be any loss function. Using the logistic log-likelihood loss, i.e. L(y, p(x)) = −2`with ` as in equation (4.6) results in a Bayesian logistic model, whereas a Bayesian supportvector machine model is constructed when the hinge loss function L(y, p(x)) = [1 − yp(x)]+is used.Mallick et al. (2005) further generalise their class of models by the introduction of non-linear classification boundaries: they do not use the linear function p(x) = xβ, but rather afunction of reproducing kernels K. For n observations x1, ..., xn we havep(xj) = α0 +n∑j=1αjK(·, xj). (4.8)This is in the spirit of V. Vapnik, who developed support vector machines specifically withthe view to extend the linear model to a more flexible kernel-based approach. Note thatthis approach reduces the number of parameters to be estimated from the dimension of β(= p) to the dimension of α (= n + 1). Two standard choices for the reproducing kernelfunctions are the Gaussian kernel K(x, y) = exp (−||x− y||/θ) and the polynomial kernelK(x, y) = (x ·y +1)θ, where x ·y denotes the inner product of x and y. However, such flexibleclassification models require some kind of regularisation in order to control the complexity ofthe models and the resulting overfitting. Just like with any other Bayesian model, the regulari-sation is introduced by the prior distribution on the regression coefficients α ∼ N(0, σ2D−1),where D = diag(λ0, λ1, ..., λn). Usually, λ0 = ... = λn = λ, which results in the standardBayesian model with the independence prior N(0, σ2λ−1In+1). Just like with the Bayesian lo-gistic regression model, this implies that the maximum a posteriori solution of problem (4.7)80is equivalent to a ridge-like penalisation problem, with the additional generalisation, that otherloss functions than the log-likelihood loss can also be applied, in particular the hinge-loss func-tion L(y, p(x)) = [1− yp(x)]+. Mallick et al. (2005) also implemented a version with multiplepenalty parameters, i.e. varying values λ0 6= λ1 6= ... 6= λn, and demonstrated that this resultsin greater shrinkage than using the single penalty parameter.Most of the statistical models presented here estimate linear decision boundaries betweenthe classes, but in slightly different ways. An exception are the kernel-based methods, whichhave more flexible nonlinear decision boundaries. For high-dimensional data such as geneexpression data the introduction of more flexible, nonlinear models always implies an increaseddanger for over-fitting. As there are a only a few samples in a high-dimensional space, lineardecision boundaries will always exist that separate the training samples perfectly. In this sense,the introduction of nonlinear methods is not needed.In conclusion, there are many different statistical methods available for binary classifica-tion, even when we restrict ourselves to models with linear decision boundaries. These meth-ods, like logistic or probit regression, linear discriminant analysis, or linear support vectormachines and related methods, only differ in the loss function, which is used to determine theseparating hyperplane, and in addition in the assumptions made on the data. In the applicationsin Chapters 5 to 7, logistic regression is used as one example for a binary classification model.All dimension reduction methods described in the previous chapter can be applied to all binaryclassification models described here. The only requirement is that they can be specified asprobabilistic models allowing for likelihood-based and/or full Bayesian analysis.4.2 Classification based on univariate statisticsOften, studies, which have the aim to perform binary classification using gene expression mi-croarray data - for example in order to discriminate between tumour types or between goodand bad prognosis for disease outcome - do not implement binary classification models suchas those described in the previous section. Rather, univariate statistics, such as correlation ofeach gene with the outcome variable, are often used to find small sets of genes (called molec-ular profiles or gene signatures) with good discriminatory power (see for example Golub et al.1999, van’t Veer et al. 2002). In such a situation, there is no inherent statistical model frame-81work, which could be used for class prediction, and a second analysis step is needed, where aclassification model is fitted to the data using the selected genes in the molecular profile. Forexample, one could fit any of the previously mentioned binary classification models with lineardecision boundaries like logistic regression or linear discriminant analysis. If the number of se-lected genes p∗ is large compared to the sample size n, dimension reduction methods need to beapplied as described in Chapter 3. Two simple classification methods are described here, whichare particularly often used in the microarray literature in such situations. The first method isweighted gene voting, which has often been applied in microarray analyses, since it was firstintroduced in the paper by Golub et al. (1999). The second method is nearest-centroid classifi-cation, applied for example by van’t Veer et al. (2002) and Michiels et al. (2005). A close linkexists between these two popular approaches and diagonal linear discriminant analysis, whichhas been highlighted for example by Dudoit et al. (2002).Nearest-centroid classification (NC) The nearest-centroid method is often successfully ap-plied to gene expression data, and is very simple to implement. It belongs to the class ofprototype methods like k-means classification and classification based on Gaussian mixturemodels (Hastie et al. 2001), where new observations are assigned to the same class to whichtheir nearest prototype belongs to. First centroids, i.e. mean average profiles, are constructedfor each class based on the training data available for the selected genes in the molecular pro-file. New samples are then assigned to the class whose centroid is closer to the sample basedon a similarity (or distance) measure, here Pearson’s correlation r. That is, for two classes 0and 1, a sample with gene expression profile x = (x1, ..., xp∗) is assigned to class 1 iffr(x, x̄1) > r(x, x̄0), (4.9)where x̄k (k ∈ {0, 1}) is the mean expression vector (centroid) in the training samples of classk.Diagonal linear discriminant analysis (DLDA) Dudoit et al. (2002) compared various clas-sification rules for the univariate filtering approach. They selected between 10 and 200 vari-ables in several microarray data sets and found that simple classification methods generallyoutperformed more complex methods in this context. In particular, diagonal linear discrimi-nant analysis was found to perform very well. DLDA is a special case of ML-LDA presented82in the previous section, where the common covariance matrix Σ of the two class distributionsis diagonal. DLDA is very similar to the nearest-centroid method, except that here the samplevariances are taken into account. Sample x = (x1, ..., xp∗) is assigned to class 1 rather thanclass 0 iffp∗∑i=1(xi − x̄1i)2s2i<p∗∑i=1(xi − x̄0i)2s2i, (4.10)where s2i is the ith diagonal element of the pooled variance estimate of the diagonal covariancematrix Σ, which is assumed to be the same for both class populations. Classification rule (4.10)can be rewritten asp∗∑i=1(x̄0i − x̄1i)s2i(xi − (x̄0i + x̄1i)2)< 0. (4.11)Weighted gene voting Weighted gene voting is a variant of diagonal linear discriminant anal-ysis (Dudoit et al. 2002). Golub et al. (1999) interpreted the left-hand term in equation (4.11) asa sum of weighted votes∑p∗i=1 wivi over all genes, where the gene votes are the gene expressionvalues x1, ..., xp∗ observed for the new sample, centered by subtracting the mean of the averageclass profiles, i.e. vi = xi − (x̄0i + x̄1i)/2. Note that Golub et al. (1999) formulate the weightswi slightly differently than in the diagonal linear discriminant rule given in equation (4.11),with the unusual estimate si0 + si1 for the standard error of the difference between averageclass profiles, replacing the variance term s2i in equation (4.11):wi =x̄i0 − x̄i1si0 + si1. (4.12)Because a diagonal covariance matrix is assumed, the gene weights and votes can be computedfor each gene independently. Golub et al. (1999) used the univariate statistics wivi for filteringthe genes to those p∗ = 50 with the largest statistics, and only those were used for classification.4.3 Model fitting and model assessmentThe classification performance of a statistical model p(X) relates to its ability to correctlypredict the class membership y of a new sample X = x, which has not been used in the modelfitting (i.e. in the estimation of the model parameters). The predictive abilities of a fitted modelp̂θ(x) are assessed in terms of a loss function L(y, p̂θ(x)) which measures how severe an errorbetween y and p̂θ(x) is.83Loss functions Typical choices of loss functions are listed here:1. squared error loss L(y, p̂θ(x)) = (y − p̂θ(x))2, which is the usual choice for linearregression models.2. log-likelihood loss (also deviance or cross-entropy in information theory) L(y, p̂θ(x)) =−2 log Pr(y|x, θ̂) = −2`(θ̂), where θ̂ is the estimated model parameter vector and `(θ̂)is the log-likelihood of the model. The factor −2 ensures that for linear models withGaussian errors, the log-likelihood loss and the squared error loss function are equivalent.The introduction of the term −2 also allows the implementation of likelihood-ratio tests,which test for a difference in the fit of two models by using the difference between thetwo corresponding log-likelihood losses as the test statistic. The null distribution of sucha test statistic is approximately χ2-distributed.3. 0-1 loss L(y, p̂θ(x)) =0 if p̂θ(x) = y1 if p̂θ(x) 6= y, the loss function which is implicitly used ifthe prediction performance is assessed by simply counting the misclassification errors. Itis only useful if y and p̂θ(x) can only take discrete values.4. hinge loss L(y, p̂θ(x)) = [1− yp̂θ(x)]+, where [a]+ =a if a > 00 if a ≤ 0. The most promi-nent classification method which uses the hinge loss function is the support vector ma-chine, where y is binary and y ∈ {−1, 1}.5. exponential loss L(y, p̂θ(x)) = exp(−yp̂θ(x)). The AdaBoost.M1 algorithm for binarydata by Freund and Schapire (1996) relies on the exponential loss function with y ∈{−1, 1}. It leads to similar solutions as the log-likelihood loss function in the logisticregression problem, which is given as L(y, p̂θ(x)) = 2 log(1 + exp(−yp̂θ(x))).Having decided on a loss function, the predictive ability of a classification model is assessedby the expected value of the loss function for a new observation E(L(y, p̂θ(x))).Note that there are several aspects of analysis, in which the loss function is involved: inmodel fitting, it is used to estimate the model parameters. Sometimes one has fitted several(nested) models and wants to select the model with the best predictive abilities (model selec-tion). Finally, one usually wants to assess the predictive ability for the final model, also in terms84of a loss function (model assessment). These aspects should not be mixed up: if one would sim-ply assess the model performance on the same data on which it has been trained, one wouldoverestimate the predictive ability of the model for new data samples, as the model parametershave been optimised for these data. This implies that the model is likely to be over-fitted to thedata set at hand, and might hence perform badly for a new data set.In an ideal situation, one would first fit the model, i.e. estimate the model parameters,using a training data set, and then use a new data set (sampled from the same distribution) tovalidate the model performance by averaging over the observed values of the loss function.Often, however, there are not enough data samples available to split them into a training and avalidation data set, where both data sets are large enough for valid inference.Penalised loss function approaches Many procedures have been developed in order to pre-vent model over-fitting in the situation where splitting the data into training and validation datasets is not feasible. One way is to simply use all data for training and then to correct the nec-essarily over-optimistic estimate of the loss function by adding a penalty term. Since a flexiblemodel with many parameters has more potential for over-fitting than a simple model with onlya few parameters, the penalty term will depend on the number of parameters. Examples forsuch corrected loss estimates are the Akaike information criterion (AIC) and the Bayesian in-formation criterion (BIC) (e.g. Hastie et al. 2001). Both criteria are based on the log-likelihoodloss function:AIC(y, p̂θ(x)) = −2`(θ̂) + 2p = −2 log Pr(y|x, θ̂) + 2p (4.13)BIC(y, p̂θ(x)) = −2`(θ̂) + log(n)p = −2 log Pr(y|x, θ̂) + log(n)p. (4.14)The BIC tends to penalise a complex model with many parameters more than the AIC. Althoughboth criteria look very similar, they were motivated in different circumstances. The AIC wasdeveloped to aid model selection, i.e. to help decide which model among several statisticalmodels has the best prediction ability. The BIC, however, is motivated as an approximation tothe Bayes factor, which is used to compare models M1 and M2 in a Bayesian setting using theratio of their posterior probabilities, given byPr(M1|x, y)Pr(M2|x, y) =Pr(M1)Pr(M2)× Pr(x, y|M1)Pr(x, y|M2) . (4.15)85The Bayes factor is defined as the contribution of the data (x, y) towards the posterior odds(Kass and Raftery 1995):BF (M1, M2) =Pr(x, y|M1)Pr(x, y|M2) , (4.16)which equals the ratio of the posterior probabilities in the case where both models have equalprior probabilities. The Bayesian information criterion arises from the following approximationto log Pr(x, y|M):log Pr(x, y|M) = log Pr(x, y|θ̂, M)− p2log(n) + O(1), (4.17)where θ̂ is the maximum-likelihood (ML) estimate of the parameter vector θ and p is the numberof free parameters in model M . In a regression framework log Pr(x, y|θ̂,M) is equivalentto the log-likelihood function in equation (4.14) for the ML-estimated θ̂, as one is interestedin inference on y conditioned on x and x is assumed to be fixed. Note however, that theterm O(1) is independent of the sample size n and hence the term −0.5 × BIC(y, p̂θ(x)) =log Pr(x, y|θ̂, M)− p2log(n) in equation (4.17) will never converge to log Pr(x, y|M).Cross-validation as an example for resampling approaches The Akaike and Bayesian in-formation criteria depend on the log-likelihood loss function, which in linear models is equiv-alent to the squared-error loss. A different approach, which is more generally applicable to allloss functions, is taken by resampling methods, in particular cross-validation. Cross-validationmethods split the data into training and validation data sets in order to avoid the over-optimisticbias in the estimation of the loss function, which arises when using the same data set for modelfitting and loss function estimation. However, since usually there are not enough data availablein order to simply set aside one validation data set and not use it at all for model fitting, onemakes better use of the data by splitting them into many smaller subsets and averaging over theresults.In κ-fold cross-validation one divides the data randomly into κ groups of (about) the samesize. One then uses each of these groups once as the set-aside subset on which to evaluate thepredictive ability of the model, which was trained on the rest of the data. Therefore, there areκ loss function estimates and the mean of these is called the κ-fold cross-validated loss Lκ. Aspecial case of cross-validation is the leave-one-out method. Here, one trains the model withn−1 observations and tests it on the one that was left out. This procedure is performed n times,86once for each observation. The mean of all n loss function estimates is the leave-one-out lossLloo. For leave-one-out cross-validation, one uses nearly all the data to train the classificationmodel, which means that it is nearly unbiased with its expectation value being the actual errorrate of a data set with n−1 observations instead of n (e.g. Hastie et al. 2001). But a drawback ofthe leave-one-out estimator of the loss is, that it has a large variance; it also is computationallyquite expensive. So, for larger data sets it is recommended to use the 5-fold or 10-fold cross-validation estimators instead (Hastie et al. 2001). These estimators have a larger bias than Lloo,but also have a smaller variance, and are more feasible in computational terms.87Chapter 5A resampling study to assesscharacteristics of gene expression profilesThe contents of this chapter has been published in more detail in Statistical Applications inGenetics and Molecular Biology (Zucknick et al. 2008). The full paper is available via thejournal home page at http://www.bepress.com/sagmb/vol7/iss1/art7/ and isattached as Appendix C. All Figures and Tables in this chapter are adopted from the full paper.It has been pointed out (e.g. Ein-Dor et al. 2005, Michiels et al. 2005) that molecularprofiles derived from gene expression microarray data can be highly unstable, i.e. which genesget selected into a profile depends much on the choice of training data. Hence, there is need forcareful validation of the results (Simon et al. 2003, Dupuy and Simon 2007) and it is importantto assess the uncertainty associated with molecular profiles. To do this, a resampling approachis adopted here to compare important characteristics of gene expression profiles derived usingseveral univariate and multivariate methods for binary classification.In addition to classification accuracy, the biological interpretability of molecular profiles isalso important. This implies both parsimony and stability, in the sense that profiles should notvary much when there are slight changes in the training data. Within the resampling study setup,the stability is measured by adopting the Jaccard index to assess the similarity of resampledmolecular profiles.A case study on five well-established cancer microarray data sets is carried out. For twoof these data sets we have the benefit of being able to validate the results in an independentdata set. The study shows that those methods which produce parsimonious profiles generally88result in better prediction accuracy than methods which do not include variable selection. Forvery small profile sizes, the sparse penalised likelihood methods tend to result in more stableprofiles than univariate filtering while maintaining similar predictive performance. The resultsof the comparison study are summarised here. For more details, the reader is referred to thepublished article.5.1 Resampling study setupAll data sets are repeatedly randomly split into training and validation data. Fifty such randomsamplings are performed, each with a ratio of 2:1 for the training set size to validation set size.This setup is a multiple random validation setup (e.g. Michiels et al. 2005), and is outlined inTable 5.1.Table 5.1: Resampling study setup for comparison of the characteristics of the classification methods.For k = 1, ..., m (m = 50):• Divide data randomly, assigning 2/3 of the samples into a training set k and the remaining 1/3into a validation set k (optionally, fix class proportions).• For all classification methods, and for a set choice of tuning parameter values, fit classificationmodel using training data set k and find molecular profile k.• Apply fitted model to validation data k and assess prediction performance in terms of percentageof misclassified validation samples5.2 Classification methods and softwareThe binary classification methods employed here include simple univariate filtering methodsusing both the nearest-centroid classifier and diagonal linear discriminant analysis (as intro-duced in Chapter 4), the penalised likelihood methods lasso and ridge regression and elasticnet (see Chapter 3), and in addition the random forest with and without variable selection,which is briefly described here.89Random forest (RF) and varSelRF The random forest classifier (Breiman 2001) is an ex-ample of the class of ensemble classification algorithms, which combine the outputs of many“weak” classifiers, in this case classification trees, to produce a powerful ensemble. The ran-dom forest can be successful in dealing with the multi-collinearity of “large p, small n” appli-cations, because it combines two ideas to help find as many of the multiple best solutions aspossible: firstly, it uses repeated bootstraps, i.e. each tree is grown using a different bootstrapsample of the data, and secondly it also employs random subspace selection, that is to only usea random subset of all available variables to grow each tree. Because of this, for p >> n datait is likely that most or all variables will get used in node splits for some of the trees. The finalclassification is the mode of the classifications of all trees, that is the random forest chooses theclass that has been decided by the majority of trees.While random forests can deal with p >> n data, it has been found that the classificationperformance can be improved if the classifier is combined with a variable selection step so thatonly a small number of variables get used in the entire forest, see Díaz-Uriarte and Alvarez deAndrés (2006). There, the performance of random forests is compared to the varSelRF method,which implements variable selection by iteratively fitting random forests and discarding thevariables which get used as nodes least often.For most of the statistical analyses the R software (R Development Core Team 2006) wasused, in particular the affy library for data pre-processing of the Affymetrix data sets and theglm library for univariate logistic regression analyses. The R library glmpath was used forthe elastic net and libraries randomforest and varSelRF were used for the random forestwithout and with variable selection, respectively. Ridge and lasso regression analyses werecarried out with the BBR software by Genkin et al. (2007), which was chosen because duringmost of this project it was the only available software applying a computationally efficientgradient ascent algorithm in the context of logistic regression (see Section 3.2.3 for more detailson the algorithm). Note that even though the lasso can have multiple solutions if p >> n,most software packages including BBR focus on finding one solution only. We hope that thisrestriction is alleviated by the resampling study setup, as the algorithm is run m = 50 times foreach data set with different solution paths determined by the resamples of the training data.905.3 Assessing the instability of molecular profilesStability of classification methods for the construction of gene expression profiles is viewedhere in terms of whether the same genes get selected for different resamples of the data. Natu-rally, this concept of stability does not apply to those classifiers that use all the gene variables.Hence, we here only assess the stability for those methods that do incorporate feature selection,i.e. the elastic net, lasso, univariate filtering and the varSelRF method. In the microarray liter-ature, there have been many attempts to assess the stability of gene signatures, most of whichhave focussed on resampling setups such as bootstrapping (Díaz-Uriarte and Alvarez de An-drés 2006) or repeated splits into training and validation subsets (Michiels et al. 2005). A shortreview of these approaches is given in the full paper (see Appendix C). Within this context, wehave argued for the use of similarity indices, which fulfill a specific set of properties, in orderto measure how similar the expression profiles derived from all the resampled data sets are -and thus, how stable the classification methods are. A similarity measure ρ(Z1, Z2) for thecomparison of two discrete sets Z1 and Z2 is usually based on the two-by-two table countingthe presences and absences in both sets (Table 5.2).Table 5.2: 2× 2 table counting presences and absences in gene sets Z1 and Z2.Z11 0Z2 1 a b0 c dOne similarity index that fulfills all properties and in addition has an intuitive interpretationis the Jaccard index (Jaccard 1901):ρj(Z1, Z2) =aa + b + c=#(Z1 ∩ Z2)#(Z1 ∪ Z2) . (5.1)Other investigated similarity indices, which also fulfilled the required properties, were seen tobehave very similarly to the Jaccard index in all applications. Because of this, and because theJaccard index has the easy interpretation of being the ratio of set intersection size to set unionsize, it is used in this resampling study to measure the similarity of gene sets. The Jaccard91index assesses the similarity between pairs of sets. Hence, in order to compare m > 2 sets, theindices of all possible m2 combinations of pairs of sets are computed and the empiricaldistributions of these values are used to assess the overall stability of a classification method.5.4 Data setsThe main characteristics of the five publicly available gene expression microarray data setsused in the resampling study - as well as of two data sets used for validation - are described inTable 5.3.Table 5.3: Main characteristics of gene expression microarray data sets used. Data sets in italics andbrackets are used as validation data.p n Response Chip type(binary)Ovarian cancer - 7129 104 tumour type HuGeneFLSchwartz et al. (2002) (mucinous/clear-cell vs.(Lu et al. 2004) 12625 42 endometrioid/serous) U95Av2Leukaemia 7129 72 tumour type HuGeneFL(ALL/AML) - (ALL vs. AML)Golub et al. (1999)Prostate cancer - 12625 102 tumour vs. normal U95Av2Singh et al. (2002)Breast cancer - 4770 97 metastasis-free survival Agilentvan’t Veer et al. (2002) (≤ 5 yrs vs. > 5 yrs)(van de Vijver et al. 2002) 4770 87 AgilentAcute myeloid leukaemia 22283 273 normal vs. abnor- U133A(AML/karyotype) - mal karyotypeValk et al. (2004)All Affymetrix data sets are pre-processed and normalised using RMA background cor-rection (Irizarry et al. 2003) and loess regression for array normalisation (Cleveland 1979). An92exception is the ALL/AML data set where the pre-processed data provided by the R packagegolubEsets were used. All Affymetrix data are centered and scaled to zero mean and unitvariance for all gene variables in the binary classification analysis.The Agilent data are normalised in the same way as described in the original papers (van’tVeer et al. 2002, van de Vijver et al. 2002). Note that for the breast cancer data set, clinicaldata are available in addition to the gene expression data, which are known predictive factorsfor breast cancer progression (patient age, tumour grade, tumour diameter and angioinvasion),and which are hence included in all classification models. Their effects are neither allowed tobe shrunken by the penalised likelihood methods nor to be removed from the active variable setin all other methods.5.5 ResultsThe performance of a classification method to construct molecular profiles from microarraygene expression data is evaluated with respect to the following aspects of the profiles:• prediction accuracy (assessed by misclassification error),• parsimony (i.e. how many genes does a profile contain on average?),• profile stability (based on Jaccard similarity between molecular profiles from the resam-pled data sets).An additional aspect is the performance of molecular profiles in external validation, that is themisclassification error rates obtained when applying the gene sets for classification on a newindependent validation data set.The results of this resampling study with respect to these characteristics are briefly pre-sented here. Note that for each of the data sets, all classification methods are fitted to each ofthe 50 training subsets for a range of tuning parameter values, which were chosen to covera wide range of models and model sizes. For univariate filtering the model size is p∗ ∈{5, 10, 50, 100, 500}. For the Affymetrix data sets the penalty parameters in ridge and lassoregression were chosen so that they correspond to a choice of prior variances τ , ranging from0.01 to 100 on the log10 scale for the lasso, and again on the log10 scale from 10−5 to 1 for ridge93regression. Since the Agilent data sets are pre-processed and normalised in a different way,slightly different penalty parameter values had to be chosen to cover the entire range of models(τ1 ∈ {10−3, 10−2.5, 10−2, 10−1.5, 10−1} for lasso and τ2 ∈ {10−6, 10−5, 10−4, 10−3, 10−2} forridge). For the elastic net, a fixed penalty is used for the L2-term (λ2 = 1), while the size ofthe L1-penalty parameter λ1 is varied from zero (resulting in the largest possible profiles) to avalue large enough to induce maximum sparsity, i.e. so that no genes are included in the model.Because we want to compare characteristics of molecular profiles of different sizes, we do nottune the classification methods to optimise their performances in terms of minimal misclassi-fication errors, i.e. we do not attempt to choose “optimal” tuning parameter values, but ratherpresent the results for all parameter values alongside each other.5.5.1 Prediction accuracy and parsimonyThe data sets are split fifty times into a training and a validation data set. The classificationmethods are trained on the training sets, resulting in fitted models which are then applied tothe validation data and evaluated in terms of their observed misclassification rates. These mis-classification errors as summarised as boxplots in Figure 5.1 are used to assess the predictionaccuracy of the classification methods, with focus on the median values as the main measurefor comparisons between methods. In addition, Figure 5.1 also shows the median profile sizes,i.e. the median numbers of genes in the gene sets for each evaluated tuning parameter value.The boxplots in Figure 5.1 show that the proportions of misclassifications vary widelybetween the data sets. While the top three data sets (ovarian cancer, ALL/AML, and prostatecancer) are easily separable with the smallest median misclassification error rates all beingsmaller than 15%, the bottom two data sets (breast cancer and AML/karyotype) are harder toclassify, and the best median error rates are only about 30%.The sparsity-inducing classification methods, in particular the sparse penalised likelihoodmethods (lasso and elastic net) and the univariate filtering approaches, perform well in terms ofmisclassification errors for all data sets. For these methods, the tuning parameter values whichresult in the best performances, generally correspond to parsimonious models with medianprofile sizes which can be as small as five genes (lasso in prostate cancer application) or evenonly two genes (lasso in ovarian cancer example).94Ovarian cancer% misclassification010203040505 10 50 100500 5 10 50 1005007129 2 7 12 17 21 2 3 3 3 4 5 5 6 6 7 8 9 10 11 12 13 14 15 16 17 19 21 22 234823 3ALL/AML% misclassification010203040505 10 50 100500 5 10 50 1005007129 2 12 18 21 24 3 4 5 6 7 8.5 10 11 12 13 14 15 16 16 17 18 19 1919.5 20 214610 4Prostate cancer% misclassification010203040505 10 50 100500 5 10 50 10050012625 518.529.534.5 38 3 4 5 6 6.5 7 8 9 10 1213.515.5 1819.521.5 23 24 25 26 2727.5 3030.5 318518.5 12Breast cancer% misclassification010203040505 10 50 100500 5 10 50 1005004770 4 13 23 29 32 2 3 5.5 9 13 18 24 28 34 41 46 51 564669.5 14AML/karyotypemedian profile size% misclassification203040505 10 50 100500 5 10 50 10050022283 3493.5118.5129138 8 11 16 22 28 36 43 49 56 62 68 74 80 85 88 92 96 99 1011031071091111131151172058676.5NC DLDA Ridge Lasso ENet RF varSelRFFigure 5.1: Boxplots of predictive performances in terms of proportion of misclassified validationsamples shown for a range of tuning parameter values. The average profile sizes corresponding toeach parameter value are indicated for all methods below the corresponding boxplots. The orange linesrepresent the baseline misclassification rates, where all samples are assigned to the most frequent class.95The prediction accuracy of the non-sparse methods (ridge regression and random forestwithout variable selection) is sometimes comparable to that of the sparse methods with equalor slightly larger median error rates, but in some data sets they perform substantially worse. Anexample are both ridge regression and random forest when applied to the ovarian and breastcancer data and in addition the random forest applied to the prostate cancer data. Ridge regres-sion is performing particularly badly in the case of the ovarian cancer data, where the predictionerror rates are actually larger than the baseline misclassification rates for all but the smallestprior variance parameters τ = 10−4 and τ = 10−5, which induce the strongest shrinkage.The ridge regression model was also fitted for even smaller prior variances (τ = 10−6 andτ = 10−7) to see whether the prediction accuracy could be improved further, but the observedmisclassification error rates did not decrease compared to τ = 10−5. This result compared tothe good performance of very sparse classifiers like lasso might indicate, that in the ovariancancer data only a very small number of gene variables are linked to the histological response,while the large majority are noise. Since these noise variables cannot be removed completelyfrom the ridge regression equation, they might produce over-fitted ridge regression models. Inaddition, very large shrinkage also penalises the true covariates heavily, and thus any attemptto reduce the influence of noise variables by large ridge shrinkage also reduces the explanatorypower of the small number of true covariates.5.5.2 Profile stabilityProfile stability is assessed for the sparse methods, i.e. lasso, elastic net, univariate filteringand random forest with variable selection. The Jaccard similarity indices for all pairs of non-empty gene sets are summarised in Figure 5.2 by their means and standard deviation bars.The similarity values are plotted against the median gene set sizes to show how the stabilitiesof profiles constructed by the different classification methods develop with increasing profilesizes (which are induced by changing the tuning parameter values).The Jaccard index distributions for the penalised likelihood methods elastic net and lassoare generally similar. The mean Jaccard values are largest for the smallest profiles and thendecline with increasing profile sizes induced by decreasing values of the penalty parameter λ1.The Jaccard index distributions for the univariate filtering approach follow a different pattern.96They vary less across profile sizes and in general are largest for the very large profiles, withthe exception of the AML/karyotype data. This increase can in part be explained by the factthat due to the design of the resampling study (where training data sets are random samplescontaining two-thirds of all patients), any two training data sets will overlap with an expectedintersection containing 4/9 of all samples. This effect is illustrated in the top right plot inFigure 5.2, where the response variable has been randomised, so that any observed similaritybetween molecular profiles cannot be attributed to a particular set of genes which might alwaysbe selected due to their good predictive power with respect to the response.There is a link between the observed similarities and the correlation structure betweenthe genes that are included in the molecular profiles. To illustrate this, all pairwise correla-tions between all genes in each profile are computed; the mean of the absolute values of thesecorrelations is used as a summary measure for the strength of correlations in a profile. Thedistributions of these mean absolute correlations across all m = 50 resamples are illustratedby their mean and standard deviations in Figure 5.3. The plots show the differences in how theclassification methods treat correlated variables, when more and more variables are included.For very small gene set sizes, all methods include variables into one model, which are highlycorrelated, and the within-profile correlations that are observed for very small profiles (con-taining five genes or less) are similar for all methods. However when profile sizes increase, thewithin-profile correlations decrease much faster for the multivariate methods than for univariatefiltering. An explanation for the penalised likelihood methods elastic net and lasso lies in theL1-penalty term, which discourages the inclusion of two highly positively correlated variablesinto one model.In addition to within-profile correlations, there is also the aspect of possible high corre-lations between genes that are in different resampled profiles. One can conjecture that theproperty of the L1-penalty to discourage the inclusion of highly positively correlated variablesinto one model has an effect on the profile stability (as measured in the way described here) inthe following way. Imagine two highly positively correlated variables, which are also linkedto the response variable. In the resampling setup, one of the two variables might be selectedinto most of the resampled profiles, but they will rarely be selected together. This will reducethe Jaccard index for larger profiles, as indeed we have observed earlier. On the other hand,most resampled univariate filtering profiles will contain both variables, resulting in both a larger97Ovarian cancermedian profile sizeJaccard00.10.30.50.70 10 20 30 40 50 100 500Ovarian cancer (randomised Y)median profile sizeJaccard00.10.30.50.70 10 20 30 40 50 100 500UnivariateLassoElastic netvarSelRFALL/AMLmedian profile sizeJaccard00.10.30.50.70 10 20 30 40 50 100 500Prostate cancermedian profile sizeJaccard00.10.30.50.70 10 20 30 40 50 100 500Breast cancermedian profile sizeJaccard00.10.30.50.70 10 20 30 40 50 100 500AML/karyotypemedian profile sizeJaccard00.10.30.50.70 20 40 60 80 100 120 140 500Figure 5.2: Mean Jaccard similarity measures (± standard deviation) plotted against median profilesizes for univariate filtering, lasso regression, elastic net, and random forest with variable selection(varSelRF) for the five data sets and the ovarian cancer data with randomised response (top right).within-profile correlation as well as a larger Jaccard similarity measure between the univariateprofiles. However, one can argue that two highly correlated genes in two different profiles docontribute to the similarity of these two profiles, since they can replace each other without muchloss of information.In a first attempt to reflect this in the similarity measurements, the pairwise Jaccard in-dex was extend by adding a term to the numerator that summarises the contributions of genes,98Univariate filteringmedian profile sizemean absolute correlation00.10.30.50.70 20 40 60 80 100 120 140 500VarSelRFmedian profile sizemean absolute correlation00.10.30.50.70 20 40 60 80 100 120 140 500BreastALL/AMLProstateAML/karyoOvarianLassomedian profile sizemean absolute correlation00.10.30.50.70 20 40 60 80 100 120 140 500Elastic netmedian profile sizemean absolute correlation00.10.30.50.70 20 40 60 80 100 120 140 500Figure 5.3: Distributions of means of absolute correlations within profiles (mean± standard deviation)plotted against median profile sizes.which are present in one set but not the other, and which have large correlations with genesof the other set. The approach is outlined in Zucknick et al. (2008). Overall, the results weresimilar to those observed for the original Jaccard index, albeit shifted up. The effect of in-cluding the absolute correlations was observed to be similar for all classification methods anddid not affect the comparison results between methods. We concluded that a radically differ-ent approach is needed to adequately reflect highly correlated genes in a measure of similaritybetween resampled profiles.Ranking genes by their profile inclusion frequenciesIn addition to assessing the overall stability of molecular profiles, one can also look at indi-vidual gene stability in the sense of how often genes get selected into profiles. Ranking genevariables by their frequency of selection gives a measure for the relative importance of a genefor class prediction (e.g. Michiels et al. 2005, Díaz-Uriarte and Alvarez de Andrés 2006). Asan example, the selection frequencies are shown for the ovarian cancer data in Figure 5.4, for99Univariate filteringfrequencyAB000584AF000573D14662D38305D87292HG2167.HT2237HG2339.HT2435J03473J05068L03840L42379M18728M28713M59499M61853M61916M68840M80482M82809U00115U11862U14391U16799U42408U46692U62317U67963U68019U79288U85193U85707U90911X03635X60708X65614X71348X83573X86163X86809X98311Y00705Z35402Z4819901020304050LassofrequencyAB000584AF000573D14662D38305D87292HG2167.HT2237HG2339.HT2435J03473J05068L03840L42379M18728M28713M59499M61853M61916M68840M80482M82809U00115U11862U14391U16799U42408U46692U62317U67963U68019U79288U85193U85707U90911X03635X60708X65614X71348X83573X86163X86809X98311Y00705Z35402Z4819901020304050Elastic netfrequencyAB000584AF000573D14662D38305D87292HG2167.HT2237HG2339.HT2435J03473J05068L03840L42379M18728M28713M59499M61853M61916M68840M80482M82809U00115U11862U14391U16799U42408U46692U62317U67963U68019U79288U85193U85707U90911X03635X60708X65614X71348X83573X86163X86809X98311Y00705Z35402Z4819901020304050varSelRFfrequencyAB000584AF000573D14662D38305D87292HG2167.HT2237HG2339.HT2435J03473J05068L03840L42379M18728M28713M59499M61853M61916M68840M80482M82809U00115U11862U14391U16799U42408U46692U62317U67963U68019U79288U85193U85707U90911X03635X60708X65614X71348X83573X86163X86809X98311Y00705Z35402Z4819901020304050Figure 5.4: Inclusion frequencies for genes selected into at least half of all profiles for any method(ovarian cancer data). Frequencies corresponding to each of the tuning parameter values are illustratedby overlaid T-bars in shades of gray varying from light gray for the largest profiles to black for thesmallest. For example, the bar enclosed by the orange box shows gene X03635 being selected in all 50univariate profiles of size p∗ = 50 (light gray), in 47 profiles with p∗ = 10 (darker gray) and in 43 withp∗ = 5 (black).100those genes which get selected into at least half of the profiles and for at least one of the meth-ods. Note that for univariate filtering, only the smaller profiles of sizes p∗ ∈ {5, 10, 50} areshown to avoid plot overcrowding.Lasso regression always selects the same five probe sets into more than half of the m = 50resamples, for all penalty values λ1 > 0.01. The same five probe sets also get chosen veryoften into elastic net profiles. Two of these (M82809 and U11862, corresponding to genesANX4 and ABP1) are the only probe sets, that get selected into more than half of the varSelRFprofiles, which are generally very small with a median profile size of only three. While thisgood agreement is observed for the multivariate methods, different variables are included mostfrequently by univariate filtering. Only one of the five probe sets is also part of more than halfof the univariate profiles with p∗ ≤ 50 variables (X65614, equivalent to gene S100P).5.5.3 External validationFor the breast and ovarian cancer data sets, independent validation data are available. Thesedata sets seem well suited as validation data, because they describe samples from patient popu-lations, which are very similar to the original patient populations in terms of clinical and pheno-typical characteristics and disease outcome. The characteristics of the ovarian cancer data setsare described in Chapter 7. The breast cancer data sets were generated by the same lab undersimilar conditions and also reflect similar patient populations. Note that only those samplesfrom the van de Vijver et al. (2002) data set were used for validation, that were not included inthe original publication by van’t Veer et al. (2002). The gene sets, which were selected by thesparse classification methods within the resampling study setup, are here plugged into logisticregression models, which are then applied to the validation data sets and evaluated with respectto the observed misclassification rates. The tuning parameter values are chosen to give veryparsimonious profiles of comparable sizes with small misclassification errors observed for theoriginal data.Table 5.4 shows the median misclassification rates across all molecular profiles derivedfrom the 50 training subsets of the Schwartz et al. (2002) and van’t Veer et al. (2002) data sets,respectively. One-sided permutation tests are performed to assess whether these molecular pro-files show better predictive abilities on the new data than randomly selected sets of genes of101Table 5.4: Performance of sets of genes found in analyses using data sets Schwartz et al. (2002), van’tVeer et al. (2002), when applied in logistic regression models fitted to independent data (Lu et al. 2004,van de Vijver et al. 2002). One-sided permutation tests are based on 1000 random sets of the samenumber of genes (significance level 0.1).Method Tuning parameter Median Median Proportionprofile size error p-values≤ 0.1Ovarian cancerBaseline error rate - 0.3810 -Univariate p∗ = 5 5 0.2143 32/50Lasso λ1 = 4.472 7 0.1667 33/50Elastic Net λ2 = 1, λ1 = 3.986 11 0.1190 21/50varSelRF - 3 0.2143 31/50Breast cancerBaseline error rate - 0.1609 -Univariate p∗ = 5 5 0.1379 50/50Lasso λ1 = 4.472 13 0.1034 45/50Elastic Net λ2 = 1, λ1 = 7.953 18 0.0805 29/50varSelRF - 14 0.1092 43/50equal size. The proportions among the m = 50 profiles that have significantly low misclassi-fication rates are reported, i.e. where the error rates for the real profiles are smaller than the10%-quantiles of the random distributions. The significance level of 0.1 rather than the morecommon 0.05 reflects the difficulty of the task of translating a prediction rule to new data froma different population, even if the populations are very similar in their clinical and phenotypicalcharacteristics.Overall, the results show that it is possible to generate molecular profiles for binary clas-sification, such that the predictive abilities translate reasonably well to new data. It is hard tocome to a firm conclusion on which classification method performs best on a new data set basedon these two examples only.1025.6 DiscussionResults vary between the different data sets and depend much on the data structure, e.g. thecorrelation structure between those genes which are related to the response variable and that getselected into the molecular profiles. But for all data sets the results have in common that the bestpredicting gene expression profiles are small (between 3 and 80 genes). In terms of predictiveability, we observe comparable performances between those methods that incorporate variableselection, in particular univariate filtering, lasso and elastic net. Contrary to that, the methodsthat employ most or all genes for classification, i.e. ridge regression and random forest, oftenperformed worse, sometimes substantially. This conforms with the idea that usually only asmall number of genes influence any particular biological condition or disease.Interestingly, the prediction performances of simple univariate filtering methods are com-parable to those of more complex multivariate methods, even though they do not take the cor-relation structure of gene expression data into account. This has also been observed in a recentstudy by Lai et al. (2006). An explanation for this is the small sample size in most availablemicroarray data sets, due to which the correlation structure in the data cannot be estimatedaccurately enough, so that multivariate methods cannot profit sufficiently from the correlationstructure.However, if the response data to be fitted is continuous rather than binary, then the samplesize needed to give multivariate methods an edge over univariate methods becomes smaller.This is because a vector of continuous data contains more information than a binary vector ofthe same length, which makes a perfect model fit harder to achieve. In that situation, methodswhich can use more information, in particular the correlation structure between covariates, havean advantage. In a recent study comparing several methods applied to Cox proportional hazardsmodels for predicting survival from microarray data (Bøvelstad et al. 2007), the authors foundthat all multivariate methods performed clearly better than univariate approaches.For two of the data sets, independent data were available for validation. We applied themost parsimonious gene expression profiles constructed with the original data (using thoseclassification methods which incorporate variable selection) to the new data, using the genesas covariates in logistic regression models. All methods translated reasonably well in termsof their predictive accuracy achieved on both new data sets. Note that the question, whether103the predictor accuracy of existing profiles translates to new data is different from the question,whether the same genes would be found in a new analysis on the new data as has been pointedout e.g. by Somorjai et al. (2003), Roepman et al. (2006) and Simon (2006). This will gen-erally not be the case, in part due to the correlated nature of gene expression data, because ofunderlying biological processes and also due to the “large p, small n” nature of microarray dataleading to non-uniqueness of solutions.104Chapter 6MCMC algorithms for Bayesian variableselection in the logistic regression model6.1 IntroductionAlthough high-throughput technologies like gene expression microarrays provide measure-ments for a vast number of variables, it is often reasonable to assume that only a very smallnumber of all measured variables is linked to the biological condition or phenotype of inter-est. This leads to the introduction of sparse modelling where most variables are assumed notto have an effect. As outlined in Chapter 3, in a Bayesian framework this can be achievedeasily by introducing a binary indicator vector γ = (γi)pi=1 which indicates which of the vari-ables xi in the vector X = (xi)pi=1 are considered to be included in the model (γi = 1) orexcluded (γi = 0). Then one can induce sparsity by setting the hyper-prior for the probabilityof including a variable to a small value, reflecting the small expected model size. Here, weare particularly interested in applying this Bayesian variable selection (BVS) framework in thebinary regression context for modelling the effect of gene expression data on a binary responsesuch as disease versus non-disease or tumour classification. We apply the Bayesian logisticregression model (3.65) (Holmes and Held 2006), that was introduced in Chapter 3 for thereasons outlined there.There is often a complex dependence structure among markers or genes in high-throughputbiological data due to their joint involvement in biological processes and pathways. However, it105is also often reasonable to assume that the conditional dependence structure is sparse, that is thateach of the variables is only correlated with a small number of covariates when conditioningon all other variables in the data set (see West 2003). In this chapter, we propose to makeuse of the sparse conditional dependence structure to decide which variables γ in the MCMCalgorithm should always be updated together before updating all other model parameters, likethe regression coefficients β, which are taken to be strongly correlated with γ. Such a sampleris not as computationally demanding as full Gibbs sampling, which updates the entire γ vectorin each iteration before updating the other model parameters. On the other hand, with regardsto vanilla samplers using simple one-at-a-time proposals, it can result in an improvement in theMCMC efficiency in terms of mixing performance. Also, for gene expression data, many ofthe input variables are typically correlated. If variables are correlated, updating them one at atime will often lead to slow convergence of the Markov chain, so that in this situation blocksamplers are recommended instead (e.g. Gilks et al. 1996).In this chapter ways are outlined to estimate the dependence structure between covariatesand to decide for which covariates to update the γ parameter together in a block based onthe estimated dependence. Moreover, we apply several approaches for sampling within theselected blocks and evaluate and compare the mixing and convergence performances of thecorresponding MCMC samplers in two simulation studies. In the context of these simulationstudies, the sensitivity of the samplers to the choice of the variance parameter c2 in the priordistribution of β, p(β) = N(0, c2Ipγ ) is also assessed, both for the Bayesian logistic and probitregression models. For an application of logistic BVS to real gene expression data, the readeris referred to Chapter 7.6.1.1 SoftwareThe MCMC algorithms for the logistic BVS model are given in detail in Appendix B. Theyhave been implemented in MATLAB 7.3 (The MathWorks 2006) and are based on the algo-rithms for Bayesian logistic regression presented in Holmes and Held (2006). The MATLABcode for sampling from the logistic as well as the probit BVS model is available from thewebsite http://www.bgx.org.uk. The dependence structures are estimated using the Rpackage corpcor (Schäfer and Strimmer 2005). For posterior inference and mixing and con-106vergence diagnostics, both MATLAB and the R 2.4.0 software (R Development Core Team2006) have been used, in particular the R library CODA (Plummer et al. 2006).6.2 Estimating the dependence structureOne can assess the dependence of the variables in terms of their covariance matrix S =(sik)i,k=1,...,p and corresponding correlation matrix R = (rik)i,k=1,...,p. Recall that under theassumption that the covariates follow normal distributions, a correlation of zero between twocovariates implies that they are marginally independent. However, we are rather interested inthe conditional independence of variables, and so the above relationship cannot be used di-rectly. The matrix of partial correlations (ρik)i,k=1,...,p, on the other hand, can be used to inferconditional independences, as under the assumption of normal distributions of the covariates apartial correlation ρik of zero implies that variables i and k are conditionally independent givenall the other variables j 6= i, k. Note that the partial correlation matrix is related to the inverseof the standard covariance matrix S in the following way (Whittaker 1990):ρik = − s−1ik√s−1ii s−1kk, (6.1)where S−1 = (s−1ik )i,k=1,...,p is the inverse of the covariance matrix S = (sik)i,k=1,...,p. For thesake of comparison, we construct a block structure for updating γ in the Gibbs algorithm usingboth the estimated correlation and partial correlation matrices.In the p >> n paradigm, the classic maximum-likelihood and related empirical covari-ance matrix estimators ŜML and ŜE = nn−1 ŜML can be greatly improved upon by using biasedshrinkage estimators, where a small introduced bias, e.g. towards a target matrix T with im-posed restrictions, can result in a much reduced mean squared error (e.g. Stein 1956, Efron1975, Schäfer and Strimmer 2005). The restricted target matrix has assumptions imposed,which result in a smaller number of parameters to be estimated and thus in a reduced dimen-sionality. It could be for example a matrix where all off-diagonal entries are the same, resultingfrom the assumption that all variables share the same pairwise covariance. Shrinkage can beachieved by introducing a penalty on the size of the coefficient estimates which is added to thelog-likelihood function, so that the estimate is the maximum of the penalised log-likelihood,generalising the classic maximum-likelihood estimator. An example of shrinkage through pe-107nalising the likelihood function is ridge regression (Hoerl and Kennard 1970), for which thetarget matrix mentioned above turns out to be the identity matrix Ip. Here, we follow a slightlydifferent approach proposed by Schäfer and Strimmer (2005) who directly use the linear shrink-age equationŜ = (1− λ)ŜE + λT, (6.2)where the estimate is a linear combination of the unbiased empirical covariance estimate andthe target matrix T . Note that if T is the identity matrix, this is very close to the ridge estimatorwith penalty parameter λ, which results from equation (6.2) when the unbiased estimator ŜEis replaced by the maximum-likelihood estimator. Following Schäfer and Strimmer (2005)we use a slightly more general target matrix, which is also diagonal but allows for unequalvariance entries on the diagonal. This implies that only the off-diagonal elements of Ŝ areshrunken. Because of this, it is more convenient to parameterise the covariance matrix S interms of variances sii (i = 1, ..., p) and correlations withsik = rik√siiskk. (6.3)Schäfer and Strimmer (2005) propose to determine the shrinkage parameter λ analytically usingthe lemma of Ledoit and Wolf (2003). Ledoit and Wolf (2003) derive a formula for the optimalvalue of λ that minimises the risk function R(λ) associated with the mean squared error lossR(λ) = E(p∑i=1(ŝi − si)2), (6.4)where ŝi and si are the column vectors of the shrinkage estimator Ŝ in equation (6.2) and thecovariance matrix S, respectively. In the case of our target matrix T this results in the followingoptimal value for λ:λ∗ =∑i6=k Var(r̂ik)∑i6=k r̂2ik, (6.5)where r̂ik is estimated from the empirical covariance matrix ŜE = (ŝEik) plugged into equa-tion (6.3). In practice, Var(r̂ik) is being substituted by an unbiased estimate V̂ar(r̂ik) (Schäferand Strimmer 2005). The R package corpcor was used for the estimation of correlation andpartial correlation matrices. Due to restrictions in working memory size we found that appli-cability of the algorithm is restricted to data sets containing up to about 4000 covariates, whenapplied on the 32-bit operating system Windows XP with a maximum memory size of 4GB.108After estimating the correlation and partial correlation matrices in this way, they are usedto determine which variables should be updated together in the MCMC algorithm. This couldbe done by testing rik = 0 (or ρik = 0) for all pairs of variables xi 6= xk. All coefficiententries in the correlation matrix R = (rik)i,k=1,...,p, which are not considered to be significantlydifferent from zero, can be interpreted as implying marginal independence between the corre-sponding variables xi and xk when we assume that all variables follow a normal distribution.Under the same assumption, all partial correlation coefficients ρik, which are not significantlydifferent from zero, can be seen as conditionally independent. However, this approach posesa huge multiple testing problem as there are p(p − 1)/2 pairwise correlation coefficients fora large number of variables p. A possible solution is to control the false discovery rate FDR(Benjamini and Hochberg 1995) or estimate the positive false discovery rate pFDR (Storey2002).An alternative approach to sparse covariance matrix estimation has for example been pre-sented by Dobra et al. (2004) and Meinshausen and Bühlmann (2006). Instead of estimatingthe covariance or correlation matrix and then inferring the partial correlations by using equa-tion (6.1), they use the fact that partial correlations can also be estimated directly by linearlyregressing each variable on all others. This results in a very large set of regression equations,effectively one for each partial correlation coefficient. Sparseness can be introduced by com-bining the regression analysis with variable selection. Dobra et al. (2004) have implemented aBayesian variable selection approach, while recently Meinshausen and Bühlmann (2006) haveused lasso (Tibshirani 1996) to reduce the number of non-zero coefficient estimates.Note that when doing multiple testing, the order of coefficients is always the same, nomatter whether they are ordered by the size of unadjusted or adjusted p-values, or by the q-values which replace p-values in case of Storey’s method to control pFDR. Hence, the onlydifference between all approaches is the threshold value, or equivalently the number of coeffi-cients from the top of the ordered list that are called significantly different from zero. Here, wedo not attempt to set such a threshold, but instead we apply several threshold values coveringa range of average block sizes in our simulation studies in the following section to allow forcomparisons of performances of MCMC samplers with varying block sizes. Indeed, we usethe correlations and partial correlations only to guide our block proposals, and not to determinestatistical significance. This gives us an insight into how the average block size relates to the109mixing performance of the Markov chain relative to CPU time per iteration. We characterisethe threshold values C in terms of percentiles of the distributions of correlation or partial cor-relation coefficients. All pairs of variables for which the absolute value of the pairwise partialcorrelation coefficient value |ρik| is below the threshold, are treated as if they were condi-tionally independent, and the corresponding γ values are not updated together in the MCMCalgorithm. In addition, for comparison, the pairwise absolute correlation values |rik| are alsoused to construct the blocks, although they only relate to marginal rather than conditional in-dependence. By replacing all those correlation or partial correlation matrix entries, for whichthe absolute values are below the threshold C, by zero, a sparse matrix is created. Finally,in one of the simulation scenarios, we will also construct blocks simply by randomly draw-ing variables into blocks, matching the block sizes with the mean block sizes observed for thepartial-correlation-based and correlation-based block structures for comparison, in order to seewhether the structure of the blocks influences mixing, rather than block size alone.A sparse matrix can be illustrated by a graph where all non-zero entries represent edgesbetween the nodes which represent the variables. A graph corresponding to a sparse covarianceor correlation matrix is commonly referred to as a relevance network and a graph representing apartial correlation matrix is known as a conditional independence graph (e.g. Whittaker 1990).Figure 6.1 shows the conditional independence graph of a partial correlation matrix estimatedwith the method described above, where the threshold C is set so that 50% of all coefficientsare considered to be zero. The data used for Figure 6.1 is a random subset of 150 genes ofthe gene expression data set analysed in Chapter 7. Note that there are some larger subgraphs,while the majority of nodes is only connected to one other node or none at all. This is a typicalstructure for gene expression data.110HG658.HT658 HLA-AGCLCINSM1USP7INPP5AKHSRPAESLARP5ISGF3G BIRC4BPHG3991.HT4261CDKN2BCYP19A1RBBP7HDHD1ASORDZNF9KCNMA1FSHBGYPEUSP9YLTBRDYRK4SPIBABCB1ANK1HG2730.HT2828L43576ACTA1C4orf9CAP1ZYXNR4A3DSCR1ATP5G3 TIMM17AHG3925.HT4195HG3733.HT4003MYH11DRD3HG896.HT896FXYD3AAMPGSTA1HG4157.HT4427 UGT2B7PRMT2LIG3HIST1H2ACC1SCOX7CQARSDESFHITBRCA2GPD1LPCSK7GPLD1FASGARSUBE2SX72475 PTPRCSMR3ACCL19SEPHS2 ALDOASMARCC2NFE2RYR2MUC5ACCDKN2DSNTA1IL2RATHOP1MEIS1PXDNLAMB1RBPMSRPE65U32331AFFX.M27830ACTR1ADFFAPSME4IQCB1PTK6CPSTRN3 KIAA0232ITGB8DUSP6UBE2L3 EIF2B2KRR1FEZ1GTF3C2MYO1ESTIP1DLG7HG2190.HT2260EMDSUMO2ZNF592ID1ATMU60269TRIM21BNC1HG2479.HT2575C18orf1HG3437.HT3628CPT1BACTA2MMP15RPL13AITGA3SARSTHBDNME3HG2417.HT2513KRTHB1IL8RBHG4128.HT4398SLNMPIPRRT1CREBL1HDLBPSDC1S82075TFDP1UGCGC1orf61CASP4KRT19MAP7PRPF4BAK1EYA1CRYGCC6orf32PMS2L1LCN2SETHG4102.HT4372X00237SLC9A3R2UGP2Figure 6.1: Conditional independence graph for the Schwartz et al. (2002) gene expression data set(random subset of 150 probe sets), based on a sparse shrinkage estimate of the partial correlation matrix.Only the partial correlations with absolute values larger than the C = 50% percentile are consideredsignificantly different from zero and shown as edges. The nodes represent probe sets, which are labelledby the corresponding gene symbols if known, otherwise they are identified by their Affymetrix probe setID’s.1116.3 MCMC samplers for the covariate indicator γBased on the dependence structure estimated in the way described above, the covariate indica-tor vector γ in the Bayesian variable selection model is updated in each MCMC iteration by firstselecting a variable at random and then updating this variable and in addition all those in thesame block, that is the variables which are considered to be related based on the estimated de-pendence matrix. In a straight-forward implementation of the graph structure described above,one could use all separate sub-graphs as blocks, which would produce a natural block struc-ture. This is especially the case when constructing the graph based on the partial correlationsρik, since then all nodes (i.e. gene variables), which are not connected through edges, can beconsidered conditionally independent. However, these conditional independence graphs con-structed from gene expression data tend to consist of a few large sub-graphs (blocks) and manyvery small blocks, most of them singletons. This would mean that whenever a gene in one ofthe largest sub-graphs is selected for sampling, this iteration would take quite long and genesin these sub-graphs would be covered by the MCMC algorithm much more often than genes,which are in small sub-graphs. Based on the results of preliminary test runs where we assessedMarkov chain mixing relative to required CPU time, an alternative approach for block-buildingis preferred here: only the direct neighbours of a variable, defined as all nodes to which it isdirectly connected via an edge in the graph, are considered to be in a block (or neighbourhood)with this variable. Note that this implies that there is no fixed structure of non-overlappingblocks; the “blocks”, that we consider, rather represent a definition of neighbourhood for theselected variable. In the course of this project, other variations of this neighbourhood approachwere implemented, in particular the possibility to use not only the first-order neighbours butalso a random selection of up to k second-order neighbours. Since preliminary test runs did notyield promising results, this was not pursued further.For each MCMC iteration, the elements of γ within the selected block of variables i ∈ Iare proposed to be updated. This can be done by any MCMC sampler. Here we propose theunivariate Gibbs sampler, updating each γi by sampling from its full conditional distributionp(γi|γ−i, z,X, λ). In addition, one can argue that a joint update for all γi (i ∈ I), samplingfrom the joint conditional distribution p(γI |γ−I , z,X, λ), might be advantageous, especiallyhere, where the variables within a block are selected because they are considered to be related.112Hence, in the simulation studies in this chapter, the following MCMC algorithms are assessedand compared with respect to mixing and convergence performances relative to CPU time periteration:1. Block samplers: select γk randomly, find the set of neighbours nb(k) and within blockIk = {k} ∪ nb(k) propose to update using:(a) Univariate Gibbs proposal (Gibbs): for each γi ∈ Ik sample from its full condi-tional distribution p(γi|γ−i, z, X, λ).(b) Restricted joint Gibbs proposal (Joint < d >): for vector γIkd (Ikd ⊆ Ik) sam-ple from joint full conditional distribution p(γIkd |γ−Ikd , z,X, λ) . The size of Ikdis restricted to d for computational reasons, by randomly sampling min(d, #(Ik))variables from the set Ik, where #(Ik) denotes the size of Ik.(c) Restricted univariate Gibbs proposal (RGibbs < d >): like univariate Gibbsproposal, but only considering γi with i ∈ Ikd in order to allow direct comparisonwith Joint < d >.2. Vanilla samplers for comparison:(a) Add/delete move (AD): select one γi at random and propose to change state(b) Full Gibbs sampler (Full): updating entire vector (γi)pi=1 in each MCMC iteration.For ease of notation, the indexing of k is suppressed for the block definitions Ik and Ikdfrom here on. Throughout this chapter, N denotes the overall number of iterations for whichan MCMC sampler was evaluated; B is the length of the burn-in period, i.e. the number ofMCMC iterations in the initial period where the sampler has not yet converged to the targetdistribution. For posterior inference, only the M = N − B iterations after burn-in are used,where the MCMC samples are considered to be from the target distribution. In this context wedenote by (θi,m)Mm=1 the vector of MCMC samples (after burn-in) of any variable θi. In order tosimplify the notation, the vector (θi,m)Mm=1 is also sometimes written as θi. The meaning shouldalways be clear from the context.1136.3.1 Evaluation of the performance of MCMC algorithmsThe main aim here is to improve the mixing performance of MCMC samplers with respectto γ. The mixing of candidate MCMC samplers is assessed visually by plotting the traces ofthe model deviance (i.e. −2× log-likelihood), of the current size of the model pγ , and mostimportantly of the γ vector itself. Also, mixing is measured by the effective sample sizesESS(γi) (Neal 1993, Kass et al. 1998) of the indicator variables γi.The effective sample size is based on the autocorrelations between MCMC steps and in-tends to assess to what sample size the observed MCMC sample size would correspond to, interms of information contained in the sample, if the samples were independent observationsfrom the target distribution rather than highly dependent MCMC samples. For each γi it isdefined asESS(γi) =Mτ(γi), (6.6)where M is the number of MCMC iterations after the burn-in period andτ(γi) = 1 + 2∞∑κ=1%κ(γi) (6.7)is the integrated auto-correlation for estimating γi using the Markov chain, with %κ(γi) denotingthe auto-correlation at lag κ. This definition is motivated by the fact, that τ(γi) is equal to one iffall auto-correlations %κ(γi) are equal to zero, that is if the samples were independent. Usually,an MCMC sampler will provide strongly positively correlated samples, resulting in a reductionof ESS(γi) compared to the sample size M . Note, that it is possible to construct samples withnegative auto-correlation, which will result in an effective sample size which is larger than M .The effective sample sizes are estimated using the R package CODA (Plummer et al. 2006).In CODA, in order to provide robust estimators of the integrated auto-correlation, the Markovchain is viewed as a time series and an autoregressive model AR(k) of order k is fitted, as-suming the following relationship between the MCMC sample of γi in iteration m and its kprevious MCMC samples:γi,m = αi1γi,m−1 + ... + αikγi,m−k + εim. (6.8)The auto-correlations are then estimated from the fitted AR(k) model and plugged into (6.7) inorder to estimate τ(γi) with τ̂(γi) = 1+2∑kκ=1 %̂κ(γi). The order k of the autoregressive model114is determined via Akaike’s Information Criterion AIC (4.14). However, the maximum possibleorder that can be fitted is restricted to 10 log10(M), as is suggested by Plummer et al. (2006), toreduce the computational burden as well as reduce the variance of the estimator by removingthe small and highly instable auto-correlation estimates of high lag κ. Note that the stochas-tic process (εim)Mm=1 is assumed to be a white-noise process and autoregressive processes arecommonly used to model continuous normally distributed data. Hence, the AR(k) process isnot completely appropriate for modelling a Markov chain of samples for the binary indicatorvariable γi. However, we are not interested in the autoregressive model itself but rather in us-ing it to estimate the effective sample sizes. For this purpose our approach is found to workwell, although in extreme situations ESS(γi) can take values which can be counterintuitive tothe understanding of mixing. In particular, for an MCMC sample (γim)Mm=1, which consists ofM − 1 entries of value 0 and one entry 1, then ESS(γi) = M , although one might expect amuch smaller effective sample size value. In reality, such extreme cases are very rare though.In addition, we use the median - rather than for example the mean - as a summary measure torepresent the mixing properties of the MCMC chains for the entire γ vector, since the medianis robust to such outliers. Also note that the effective sample size measures ESS(γi) are onlyused to compare mixing effectiveness of various MCMC samplers which are all applied to thesame data set using the same prior specifications. Hence, the same posterior distributions areinvestigated as target distributions for the MCMC samplers, which ensures that the ESS valuesof the various MCMC algorithms are comparable.In large-scale applications such as gene expression microarray data analysis it can easilyhappen that the majority of the genes is never selected by an MCMC algorithm sampling from asparse model, i.e. that γim = 0 ∀m for more than half of the variables. Then, the straightfor-ward medianpi=1ESS(γi) is zero, because ESS(γi) = 0 for all variables i, that were not selectedat all during the run of the Markov chain, i.e. for which (γmi)Mm=1 = (0)Mm=1. This makescomparisons of the mixing properties between samplers impossible based on this measure. Wehence prefer a weighted mean, averaging over the median of all variables that get selected atleast once and the median of those which never get included in the model (which is equal to115zero)ESS∗(γ) =#Iγp×mediani∈Iγ ESS(γi) +p−#Iγp×mediani 6∈Iγ ESS(γi)=#Iγp×mediani∈Iγ ESS(γi), (6.9)where Iγ := {i : ||γi|| > 0}.There is a trade-off between the mixing performance of a Markov chain and the com-putational complexity of the MCMC algorithm. Because time constraints are a problem forlarge-scale applications with several thousand variables, there is an interest in assessing themixing performances relative to the CPU time required to run the various Markov chains for aspecific number of iterations. Thus, we also compare the ratios R of average effective samplesizes to CPU times t required for the M MCMC iterations after burn-inR(γ) =ESS∗(γ)t. (6.10)Global convergence of the Markov chains to their target distribution is monitored by plot-ting the traces of univariate summary statistics such as the model size pγ =∑pi=1 γi and modeldeviance −2 log p(y|X, β) = 2 ∑nj=1 log(1 + exp(−yjxTj β)), which take a specific range ofvalues under the target distribution for the model p(γ, βγ|z, λ,X). Also, the trace of the indica-tor variable vector γ is plotted by indicating variables which are included in the model as points;variables which are excluded from the model are not shown. Finally, in simulation studies theplots of marginal posterior probabilities p(γi|X, y) can be used to check how consistently the“true” model is found by the MCMC sampler.Since we are mostly interested in finding the most frequently selected models and vari-ables, we focus on regions of high posterior probability regions, while keeping in mind thatit is likely that convergence has not yet been reached in low probability tails of the posteriordistribution. A bigger problem here is that the posterior distribution is multi-modal becausep >> n, and that the chains might not have visited all the modes. That is why good mixingand the ability of the chains to move freely is more important here than elusive convergence tothe target distribution.1166.4 Simulation studiesIn the following the results of two simulation studies are presented. For both studies, 25 datasets have been simulated according to a scheme specified below. In an initial step, for bothsimulation scenarios a variety of possible implementations of the block sampler as outlined inSection 6.3 are applied to two selected data sets only out of all 25 sets. The purpose of theseinitial runs is to determine whether there are notable differences between the performances ofthe various block sampling implementations, and which setting is doing best. In these initialruns the threshold values C ∈ {99%, 97.5%, 95%, 90%, 80%, 60%} are tested in both simula-tion scenarios, corresponding to sparse estimated dependence structures where variable pairsare only considered to be related if their pairwise estimated absolute correlation or partial cor-relation is larger than (or equal to) the Cth percentile of all pairwise coefficients. A thresholdvalue of C = 0 means that all variables are updated in each iteration, i.e. that the full Gibbssampler is applied. An overview over the MCMC samplers is given in Table 6.1, as well astheir labels, which are used to refer to them throughout this chapter, and their overall iterationnumbers and burn-in lengths. The full Gibbs sampler and the block sampler with the Joint10updates within blocks are run for a smaller number of MCMC iterations than all other sam-plers because these samplers are extremely slow. Note that all Markov chains are started fromrandomly sampled starting values for all variables, sampled from their prior distributions.After the initial runs on two data sets in each simulation scenario, the block samplerswhich are found to perform best are applied to all simulated data sets in order to allow amore precise assessment and better comparison between these MCMC block algorithms andthe vanilla samplers, i.e. the add/delete Metropolis-Hastings and full Gibbs algorithms. Theadd/delete sampler is also applied to all simulated data sets, but the full Gibbs algorithm is onlyrun for 10 out of all 25 data sets in both simulation scenarios because of its extreme CPU timerequirements. MCMC iteration numbers are the same as for the initial runs listed in Table 6.1.The MCMC total run lengths of 200, 000 (scenario 1) and 250, 000 (scenario 2) as well asa burn-in length of 50, 000 for add/delete and all block samplers except Joint10 were chosenso that even the slow mixing add/delete MCMC sampler achieved convergence in terms ofthe global parameters model deviance and model size (pγ) - as assessed via their trace plots(see Figures 6.3 and 6.7) - and to allow for a sufficiently long post-burn-in period. Because117Label MCMC sampler MCMC run length N (burn-in length B)Simulation 1 Simulation 2AD add/delete Metropolis-Hastings 200, 000(50, 000)250, 000(50, 000)Full Gibbs update of all (γi)pi=1 90, 000(10, 000)110, 000(10, 000)block samplersblocktypeUpdate within block IPcor< C >partial cor-relationUnivariate Gibbs update of alli ∈ I200, 000(50, 000)250, 000(50, 000)Corr< C >correlation Univariate Gibbs update of alli ∈ I200, 000(50, 000)250, 000(50, 000)Random< C >random se-lectionUnivariate Gibbs update of alli ∈ IN/A 250, 000(50, 000)Rgibbs4 partial cor-relationUnivariate Gibbs update of sub-set of I of size 4200, 000(50, 000)250, 000(50, 000)Joint4 partial cor-relationJoint Gibbs update of subset of Iof size 4200, 000(50, 000)250, 000(50, 000)Rgibbs10 partial cor-relationUnivariate Gibbs update of sub-set of I of size 10200, 000(50, 000)250, 000(50, 000)Joint10 partial cor-relationJoint Gibbs update of subset of Iof size 1090, 000(10, 000)110, 000(10, 000)Table 6.1: MCMC samplers applied in initial runs to two data sets out of all 25 sets in both simulationscenarios.118the per-iteration running time for the full Gibbs sampler and the Joint10 block sampler isexceptionally long, these two samplers were run for a shorter number of iterations N = 90, 000(scenario 1) and N = 110, 000 (scenario 2). This includes a burn-in period of only B = 10, 000iterations, since global convergence in terms of model deviance and model size is achieved wellwithin this period for these two samplers. Inference on mixing and convergence performancewas adjusted for the shorter run lengths. Throughout, all post-burn-in samples were used forposterior inference and assessment of MCMC performance, i.e. no thinning was performed.6.4.1 Simulation scenario 1: generated covariance structureSimulation setupThe algorithm in Table 6.2 is used to simulate 25 data sets (X, y), so that the input data sets Xhave p = 500 variables and n = 100 samples, and p∗ = 5 variables (x1, ..., x5) are related to thebinary response y via a logistic link. The variables are simulated - in a way similar to example4.2 in George and McCulloch (1993) - such that there are five blocks of 100 variables each,with moderately strong correlations between the variables within blocks which are induced byadding the same standard normal variable z to 100 independent standard normals x∗1, ..., x∗100. Inaddition, correlation is also introduced between blocks by using the same variables x∗1, ..., x∗100for generating the five blocks (but with different variables z added to them). The correlationstructure that is imposed by this data-generating scenario is illustrated by an image triangularplot of an the squared empirical correlation matrix of one example data set in Figure 6.2. Thevariables linked to the response, i.e. (x1, ..., x5), are all in the same block. They are thuscorrelated with each other and with all other variables in their block. They are also correlatedwith the first five variables in all subsequent blocks, that is x1 with x101, x201, x301, and x401,etc. In such a scenario it is harder for a sampling algorithm to find the correct model with allfive true covariates (x1, ..., x5) than if they were unrelated.The Bayesian logistic variable selection model outlined in Section 3.3.1 is fitted to eachof the 25 data sets (X, y). As mentioned above, for two of these data sets only, all MCMCsamplers listed in the previous section are applied to sample from the posterior distribution. Forthe remaining 23 data sets, only one of the block samplers is selected in addition to the vanillasamplers (add/delete and full Gibbs). Based on the convergence and mixing performances119Table 6.2: Simulation scenario 1 with generated covariance structure.1. x∗1, ..., x∗q iid ∼ N(0, 1) with q = 1002. For m = 0, ..., 4 do(a) zm ∼ N(0, 1)(b) xm×q+i∗ = x∗i∗ + zm (i∗ = 1, ..., q)3. yj ∼ Bernoulli(exp(xjβ)1+exp(xjβ))(j = 1,...,n), β = (2, 2, 2, 2, 2, 0, ..., 0)observed in the first two data sets, and as shown in the following, the Pcor90 sampler is chosen,i.e. the block sampler based on the partial correlation matrix with threshold C = 90%. In theBayesian logistic variable selection model, the prior parameter c2 in the independence priordistribution p(β) = N(0, c2Ipγ ) is set to c2 = 5, which guarantees a good coverage of the rangeof values expected for β. The prior probability for γi = 1 is set to π = p∗/p = 0.01 so that theprior expected number of selected variables is equivalent to the true number, namely 5.Markov chain mixing performanceFigure 6.3 shows the traces of the global parameters model deviance and model size pγ forthe add/delete, the Pcor90 block sampler and the full Gibbs algorithm for simulated data setnumber 1. As expected, Figure 6.3 indicates that mixing is much slower for the add/deletesampler than for the block (Pcor90) and full Gibbs samplers. In particular this is also the casefor the γ vector, where for the add/delete sampler the trace plot shows long “lines”, where pointsare plotted for each iteration over a long period (indicating that a variable stays in the modelfor a long time), and equivalently long stretches of no plotted points (showing variables that arenot included for a long time). This is confirmed when measuring the mixing performances ofthe three samplers in terms of the effective sample sizes ESS∗(γ) (see Tables 6.3 and 6.4 at theend of this section). After adjusting for the reduced post-burn-in run length MFull = 80, 000(compared to MAD = MPcor90 = 150, 000), the effective sample size is nearly 60-fold forthe block sampler compared to the add/delete algorithm and even about 400-fold for the fullsampler (for generated data set 1, see Table 6.4). However, the AD sampler is also much faster120Figure 6.2: Squared empirical correlation structure imposed on data set 1 in simulation scenario 1. Theplot was created using MATLAB code by Leonardo Bottolo (Bottolo and Richardson 2008).than the Pcor90 and full Gibbs samplers.We adjust for the computation time by computing the ratios R(γ) of effective samplesizes and computation times. These ratios, relative to the ratio RFull(γ) observed for the samedata set but the full Gibbs algorithm, are displayed in the left-hand side plot in Figure 6.4for all block samplers for the first two simulated data sets. In addition, the values RPcor90(γ)are shown for the Pcor90 samplers applied to all 10 generated data sets for which the fullGibbs sampler was run. The Pcor90 sampler is chosen for computation in all data sets and forcomparison with the full Gibbs and add/delete samplers, because the comparison of a range ofthreshold sizes in the first two data sets indicates, that the 90% percentile is within the rangeof threshold values C, for which the block samplers are at their highest efficiency in termsof effective sample size per CPU time (see Figure 6.4 and Table 6.4). The right-hand sideof Figure 6.4 shows the evolution of computation times (per 10, 000 MCMC samples) for theblock samplers with increasing block sizes.The effective sample size relative to CPU time R(γ) is larger for all block samplers than1210 0.5 1 1.5 2x 105020406080100120MCMC iteration−2 log likelihood0 0.5 1 1.5 2x 105020406080100120MCMC iteration−2 log likelihood0 2 4 6 8x 104020406080100120MCMC iteration−2 log likelihood0 0.5 1 1.5 2x 10505101520MCMC iterationp γ0 0.5 1 1.5 2x 10505101520MCMC iterationp γ0 2 4 6 8x 10405101520MCMC iterationp γFigure 6.3: Trace plots of global parameters model deviance (top) and model size pγ (middle), as wellas trace plots of γ vector (bottom) for add/delete sampler (left), one block sampler (Pcor90) (centre),and for the full Gibbs sampler (right) for data set 1 in scenario 1.122Table 6.3: Mixing performance results with respect to γ for scenario 1 over all 25 data sets (10 datasets for Full sampler, respectively) with n = 100 samples and p = 500 variables (incl. p∗ = 5 truepredictors): median values and inter-quartile ranges.MCMC CPU time ESS∗(γ) R(γ) #I] # FP† # FN†sampler t (min)AD 38 59 1.58 267 10 1(38, 38) (55, 65) (1.45, 1.72) (263, 271) (8, 14) (1, 2)Block 168 3024 18.28 500 7 1(Pcor90) (166, 169) (2699, 3345) (16.13, 20.11) (500, 500) (5, 12) (0, 1)Full‡ 672 10780 17.63 500 8 1(664, 676) (7791, 13700) (16.07, 21.52) (500, 500) (4.5, 10.75) (0.25, 2)‡ For Full it is M = 80, 000, compared to M = 150, 000 for all other samplers] #I = #{i : ||γi|| > 0}, i.e. number of variables for which γi = 1 in at least one MCMC iteration†false positives and false negatives if cut-off at ratio of posterior to prior probability > 5, i.e. ifp̂(γi = 1|X, y) > 0.05for the add/delete sampler and increases with decreasing threshold level (corresponding tolarger average block sizes). It is also larger than the value for the full Gibbs sampler, forall but the smallest block sizes (induced by the largest threshold levels C). There is no obviousdifference between the block samplers constructed using partial correlations and those builtfrom estimated correlation matrices. Variation exists between the two displayed generated datasets, with R(γ)/RFull(γ) ratios generally being larger for data set 2 than for data set 1. AllRPcor90(γ)/RFull(γ) ratios (for those ten data sets for which RFull(γ) is available) are largerthan one, implicating that in this simulation scenario the Pcor90 block sampler leads to largereffective sample sizes relative to CPU time requirements than the full Gibbs sampler. If weconsider the observed values RPcor90(γ) for those 10 data sets for which RFull(γ) is available,to be samples from underlying populations that have distributions FR(γ), a two-sided Wilcoxontest for paired samples will reject the null hypothesis at significance level α = 0.05, that thedistribution of RPcor90(γ)−RFull(γ) is symmetric about zero (p-value = 0.004), indicating thethe difference between the distributions is significant. Note however, that we have to be care-ful when interpreting the observed ratios R(γ) for the sample data sets as an estimator for thepopulation ratio, as ratios of estimates are generally unstable and biased estimators of a ratio.123Threshold c−th percentileESS*/t (relative to full Gibbs)full Gibbsadd/delete (run 1)add/delete (run 2)99 95 90 80 6000.10.30.50.70.911.11.31.5Run 1: Pcor           CorrRun 2: Pcor           CorrRuns 1−10: PcorThreshold c−th percentileCPU time t (per 10^4 iterations)add/deletefull Gibbs (run 2)full Gibbs (run 1)99 95 90 80 6001020304050607080901006 26 51 101 201Mean block sizeFigure 6.4: Ratio of effective sample size and CPU time R(γ) = ESS∗(γ)/t (left), and CPU times per104 iterations (min) (right), plotted against the threshold level C for the block samplers for data sets 1and 2 in simulation setup 1. In addition, for threshold C = 0.9, R(γ) is plotted for simulated data sets1 to 10.The results for all 25 generated data sets are summarised in Table 6.3. The median of theRPcor90(γ) values median10k=1RPcor90(γ) = 18.28 is larger than the median of the full Gibbsvalues RFull(γ), which is equal to median25k=1RFull(γ) = 17.63, and although the inter-quartileranges overlap, we have seen from Figure 6.4 that for each pairwise comparison within a gen-erated data set it is RPcor90(γ) > RFull(γ). Note that the inter-quartile range of RFull(γ) isexpected to be larger than those of RPcor90(γ) and RAD(γ), because the number of availabledata points is smaller (10 compared to 25).An additional indicator of Markov chain mixing, particularly in a high-dimensional set-ting, is the proportion of all variables that are visited by the Markov chain at least once. Whilethe add/delete algorithm only visited 265 of all 500 variables in the application to simulateddata set 1 (see Table 6.4), this number is larger for all block samplers and increases with de-creasing threshold values C. Comparing the Pcor and Corr samplers with the smallest averageblock sizes, i.e. with the largest values of C, the partial-correlation based samplers visit morevariables than the corresponding correlation based block samplers.124Posterior variable inclusion probabilitiesFigure 6.5 shows the medians and inter-quartile ranges of the MCMC estimates of the posteriorvariable inclusion probabilities p̂(γi = 1|X, y) for variables (x1, ..., x10), over all 25 generateddata sets. These include the p∗ = 5 “true” predictors (x1, ..., x5) which were generated as beinglinked to the response variable y. In particular, the individual plots in Figure 6.5 illustrate theevolution of the MCMC estimates p̂(γi = 1|X, y), when the number of post-burn-in MCMCiterations M increases. The results shown for variables (x6, ..., x10) are representative of theposterior inclusion probability estimates that we find for all variables which are simulated notto be linked to the response y. As expected, the median posterior inclusion probability esti-mates of these variables are close to zero for all values of M and both samplers, with verysmall associated inter-quartile ranges. For the five “true” covariates, the inter-quartile rangesof p̂(γi = 1|X, y) (i = 1, ..., 5) initially include all values in the interval [0, 1], when only asmall number of post-burn-in samples are available (M = 1, 000). This reflects the randomstarting points of the Markov chains, which imply that all five predictors are found early on inthe MCMC run for some of the generated data sets, but also that none of them is always foundearly on. Because in the add/delete sampler individual variables are visited and proposed for astate change so rarely, even after M = 10, 000 post-burn-in iterations the median posterior in-clusion frequencies are still either zero or very close to one for the five “true” predictors. Onlyafter all M = 150, 000 iterations do the add/delete sampler median values of the estimates startto move away from the extreme values at which they were fixed simply due to slow mixing ofthe Markov chain.The Pcor90 block sampler does not seem to have this problem, with median values of theestimates being different from the extremes zero and one even for M = 1, 000 iterations. Infact, for all M ∈ {1000, 10000, 50000, 150000} the median posterior inclusion frequencies aresimilar, with inter-quartile ranges becoming narrower with increasing sample sizes, reflecting aconvergence to the true posterior variable inclusion probabilities p(γi = 1|X, y) on the level ofthe individual generated data sets. Overall, the inter-quartile ranges are narrower for the blocksampler than for the add/delete sampler.Note that the median values of the estimated posterior inclusion probabilities are con-siderably smaller than one, and in fact converge to values between around 0.2 and 0.4 with1251,000 iterations after burn-in0 2 4 6 8 1000.20.40.60.81Variablesp(γi=1|y,X)  Block (Pcor90)Add/deleteSimulated model10,000 iterations after burn-in0 2 4 6 8 1000.20.40.60.81Variablesp(γi=1|y,X)  Block (Pcor90)Add/deleteSimulated model50,000 iterations after burn-in0 2 4 6 8 1000.20.40.60.81Variablesp(γi=1|y,X)  Block (Pcor90)Add/deleteSimulated model150,000 iterations after burn-in0 2 4 6 8 1000.20.40.60.81Variablesp(γi=1|y,X)  Block (Pcor90)Add/deleteSimulated modelFigure 6.5: Posterior inclusion frequencies (median and inter-quartile ranges) for variables 1, ..., 10over all 25 replicates of simulation setup 1 (after burn-in period).increasing MCMC run lengths. In individual data sets, on average one of the five variables haseven an estimated posterior inclusion probability smaller than 0.05. These cases are labelledas false negative in Tables 6.3 and 6.4. This is linked to the fact that in individual data sets,other variables (xi with e ∈ {6, ..., 500}) are sometimes found to have high posterior inclusionprobability estimates. These variables are counted as false positive in the tables, again usinga cut-off at p̂(γi = 1|X, y) > 0.05. These results can be explained by mixing or convergenceproblems of the MCMC algorithm, but also by the presence of multi-collinearity in the inputdata matrix X which necessarily arises when the number of variables p is larger than the samplesize n.126Alternative Gibbs updates within the blocksAll block samplers discussed so far used a univariate Gibbs updating scheme within the blocksof variables. An alternative is to use multivariate Gibbs updates, in particular since the variableswithin the blocks are considered to be (partially) correlated. However, joint sampling of thevector γId (which represents the covariate set inclusion indicator of d variables in set Id) from itsmultivariate full conditional distribution p(γId |γ−Id , z,X, λ) quickly becomes computationallyexpensive. The computational effort is exponentially related to the set size d, as in each iterationthe conditional probabilities have to be computed for each possible realisation of the vectorγId ∈ {0, 1}d in order to sample a new state γ∗Id according to these probabilities. Since theprobability for the current state has already been computed in the previous iteration, there are2d − 1 computations remaining. As illustrated on an example in Figure 6.1, blocks that areconstructed from estimated partial correlation or correlation matrices can vary widely in theirsize, so that even a very conservative choice of C resulting in small average block sizes can leadto a neighbourhood structure with a few large blocks. For this reason, we restrict the number ofvariables to be updated jointly within a block I to a fixed size d, so that the number of variablesfrom block I that are updated in a multivariate Gibbs step is equal to min(#I, d).As part of this simulation study, we apply multivariate Gibbs sampling with d ∈ {4, 10}to two of the generated data sets for both simulation scenarios. In addition to these multivariateGibbs samplers denoted Joint4 and Joint10, we also apply corresponding restricted univariateGibbs samplers Rgibbs4 and Rgibbs10, where the maximum possible number of variables tobe updated within an MCMC iteration is also restricted to min(#I, d) with d ∈ {4, 10}. Forall these samplers, the Pcor90 mechanism is used to determine the underlying block structure,because the above comparison of block samplers with different threshold sizes C has indicatedthat the Pcor90 sampler (where C = 90%) performs well in terms of the ratio of effectivesample size and computation time.The results for one data set of simulation scenario 1 are summarised in Table 6.4. Whilethe computation time needed for the Joint4 run is with 88 minutes only about half the timeneeded for the univariate Gibbs run (Pcor90), the time required to run the Joint10 sam-pler explodes to nearly 24 hours for only NJoint10 = 90, 000 MCMC iterations in contrastto NPcor90 = 200, 000 iterations. At the same time, the effective sample sizes ESS∗(γ) are127only 9% of ESS∗Pcor90(γ) for the Joint4 sampler, and only 31% for the Joint10 algorithmwhen adjusting for the differences in post-burn-in MCMC run lengths (MPcor90 = 150, 000vs. MJoint10 = 80, 000) by assuming a linear relationship between ESS∗(γ) and M . Withincreasing set sizes d in multivariate-Gibbs-within-block samplers Joint < d >, the requiredcomputation time increases too quickly and outweighs the improvement achieved in mixing asmeasured by ESS∗(γ). Also, the effective sample sizes of the multivariate samplers are onlymodestly larger than those of their corresponding restricted univariate Gibbs samplers (Rgibbs4and Rgibbs10): the ratios of the effective sample sizes are about 1.2 for both d = 4 and d = 10,again when adjusting ESS∗Joint10(γ) for the reduced post-burn-in MCMC run length.We conclude that joint moves, that update a fixed number of variables d jointly, are notuseful as the main sampling move. However, it might be useful to include such updates ina portfolio of moves, if there are covariates which are strongly correlated. Such a flexiblesampler, which could select updating moves randomly from a portfolio of possible updates,might benefit from occasional joint updates of strongly correlated covariates.128Table 6.4: Mixing performance results with respect to γ for scenario 1: results for one data set (run 1)with n = 100 samples and p = 500 variables (incl. p∗ = 5 true predictors).MCMC CPU time ESS∗(γ) R(γ) #I] # FP† # FN†sampler t (min)AD 40 65 1.65 265 7 1Full‡ 704 13910 19.77 500 7 1Block samplerPcor99 56 558 9.88 484 12 1Pcor97.5 74 1193 16.06 497 10 1Pcor95 107 2094 19.57 500 10 1Pcor90 170 3802 22.39 500 7 1Pcor80 293 6559 22.39 500 8 1Pcor60 564 13160 23.34 500 7 1Corr99 56 380 6.80 411 14 1Corr97.5 74 967 13.07 481 8 1Corr95 107 1528 14.33 499 9 1Corr90 166 3309 19.89 500 9 1Corr80 293 7357 25.07 500 8 1Corr60 544 11580 21.27 500 7 1Rgibbs4 56 298 5.29 463 12 1Joint4 88 354 4.03 455 10 0Rgibbs10 71 971 13.64 499 7 1Joint10‡ 1423 620 0.44 474 10 1‡ For Full and Joint10 it is M = 80, 000, compared to M = 150, 000 for all other samplers] #I = #{i : ||γi|| > 0}, i.e. number of variables for which γi = 1 in at least one MCMC iteration†false positives and false negatives if cut-off at ratio of posterior to prior probability > 5, i.e. ifp̂(γi = 1|X, y) > 0.051296.4.2 Simulation scenario 2: covariance based on gene expression dataSimulation setupIn this second simulation scenario (see Table 6.5) we use a real gene expression data set(Schwartz et al. 2002) to generate the covariance structure between variables. For that pur-pose, p = 500 variables are selected at random from all 7129 probe sets available in the ovariancancer gene expression data set provided by Schwartz et al. (2002). This data set is describedin more detail and analysed in Chapter 7. All n = 104 samples of non-mixed histology areused for generating the simulated data sets. Again, 25 data sets (X, y) are generated, so thatp∗ = 5 variables (x1, ..., x5) are related to the binary response y via a logistic link. The naturalcorrelation structure among the 500 randomly selected variables is illustrated by the image tri-angular plot of the squared empirical correlation matrix of one of the 25 generated data sets inFigure 6.6. The correlations are not as regular as in the generated data in scenario 1, but thereare covariates which are strongly correlated, as expected with gene expression data. Pairwiseempirical correlations range from −0.7 to 0.9.Again, in the Bayesian logistic variable selection model, the prior parameter c2 in theindependence prior distribution for β p(β) = N(0, c2Ipγ ) is set to c2 = 5. The prior probabilityfor γi = 1 is set to π = p∗/p = 0.01.Table 6.5: Simulation scenario 2 based on gene expression data by Schwartz et al. (2002).1. X (input matrix): select p = 500 gene variables at random from pre-processed andnormalised gene expression microarray data set X̃ of dimension p̃ × n = 7129 × 104by Schwartz et al. (2002) (described in Chapter 7), standardise to zero mean and unitvariance.2. yj ∼ Bernoulli(exp(xjβ)1+exp(xjβ))(j = 1,...,n), β = (2, 2, 2, 2, 2, 0, ..., 0)Markov chain mixing performanceWe follow the structure of analysis outlined for the simulation scenario 1 in the previous section.Figure 6.7 shows the trace plots of global parameters model deviance (top) and model size130Figure 6.6: Squared empirical correlation structure of one data set simulated according to simulationscenario 2. The plot was created using MATLAB code by Leonardo Bottolo (Bottolo and Richardson2008).(middle) and the individual traces for all γi with i = 1, ..., 500 (bottom) for the add/deleteMetropolis-Hastings sampler, the Pcor90 block sampler and the full Gibbs algorithm for onegenerated data set. The conclusions are much the same as for simulation scenario 1, that ismixing with respect to sampling γ is much slower for the add/delete sampler than for thePcor90 and full Gibbs algorithms. In addition, there is also an obvious improvement in mixingfor the full Gibbs sampler compared to the block sampler, when viewing the trace plots of γ.In terms of the effective sample sizes ESS∗(γ) (see Table 6.7 at the end of this section forthe results for data set 1), values increase about 40-fold for the block sampler compared toadd/delete algorithm and more than 220-fold for the full Gibbs sampler after adjustment for thereduced post-burn-in run length MFull = 100, 000 (compared to MAD = MPcor90 = 200, 000),which is a slightly smaller improvement than what we had observed in simulation scenario 1.In terms of convergence for the global parameters model deviance and model size, thetrace plots of the add/delete sampler indicate that convergence might not yet have been reached1310 0.5 1 1.5 2 2.5x 105020406080100120MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 105020406080100120MCMC iteration−2 log likelihood0 2 4 6 8 10x 104020406080100120MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 10505101520MCMC iterationp γ0 0.5 1 1.5 2 2.5x 10505101520MCMC iterationp γ0 2 4 6 8 10x 10405101520MCMC iterationp γFigure 6.7: Trace plots of global parameters model deviance (top) and model size pγ (middle), as wellas trace plots of γ vector (bottom) for add/delete sampler (left), one block sampler (Pcor90) (centre),and for the full Gibbs sampler (right) for data set 1 in scenario 2.132for this data set after the designated B = 50, 000 burn-in iterations. Comparisons of these traceplots of all 25 generated data sets show, however, that this is an exception in data set 1.Threshold c−th percentileESS*/t (relative to full Gibbs)full Gibbsadd/delete (run 1)add/delete (run 2)99 95 90 80 6000.10.30.50.70.911.11.3Run 1: Pcor           Corr           RandomRun 2: Pcor           Corr           RandomRuns 1−10: PcorThreshold c−th percentileCPU time t (per 10^4 iterations)add/deletefull Gibbs (run 2)full Gibbs (run 1)99 95 90 80 6001020304050607080901006 26 51 101 201Mean block sizeFigure 6.8: Ratio of effective sample size and CPU time R(γ) = ESS∗(γ)/t (left), and CPU times per104 iterations (min) (right), plotted against the threshold level C for the block samplers for data sets 1and 2 in scenario 2. In addition, for thresholds C = 0.9 and C = 0.8, R(γ) is plotted for simulated datasets 1 to 10.The ratios R(γ) of effective sample sizes and computation times, relative to the ratioRFull(γ) for the full Gibbs algorithm, are displayed in the left-hand side plot in Figure 6.8 forall block samplers for the first two simulated data sets. Also, the ratios RPcor90(γ)/RFull(γ)and RPcor80(γ)/RFull(γ) are shown for the Pcor90 and Pcor80 samplers applied to those 10generated data sets for which the full Gibbs sampler has been run. As before, the right-hand sideof Figure 6.8 shows the linear evolution of computation times (per 10, 000 MCMC samples)for the block samplers with increasing block sizes.The effective sample sizes relative to CPU time R(γ) are larger for all block samplersthan for the add/delete sampler and increase with decreasing threshold level (corresponding tolarger average block sizes), until leveling off around C = 0.9 to C = 0.8. Contrary to sim-ulation scenario 1, the partial-correlation based block samplers now have considerable largereffective sample sizes and hence larger values of R(γ) than the samplers using correlation esti-mates for block construction. In fact, now the Corr samplers do not outperform the full Gibbs133sampler in terms of R(γ) for the two displayed data sets, while the Pcor algorithms do resultin better mixing than full Gibbs sampling if the threshold is large enough. As in the previ-ous simulation scenario, there is variation between the two displayed generated data sets, withR(γ)/RFull(γ) ratios generally being larger for data set 2 than for data set 1. Seven out of tenRPcor90(γ)/RFull(γ) ratios, for which RFull(γ) is available, are larger than one. A two-sidedWilcoxon test for paired samples, applied to the R(γ) values of the ten data sets for whichRFull(γ) is available, will not reject the null hypothesis that there is no location shift betweenthe distributions FRPcor90(γ) and FRFull(γ) at the significance level α = 0.05 (p-value = 0.1602).Table 6.6: Mixing performance results with respect to γ for scenario 2 over all 25 data sets (10 datasets for Full sampler, respectively) with n = 104 samples and p = 500 variables (incl. p∗ = 5 truepredictors): median values and inter-quartile ranges.MCMC CPU time ESS∗(γ) R(γ) #I] # FP† # FN†sampler t (min)AD 53 103 1.96 316 10 1(53, 53) (100, 105) (1.88, 1.99) (303, 321) (6, 15) (0, 2)Block 241 5148 21.46 500 8 0(Pcor90) (239, 243) (4773, 6000) (20.11, 24.89) (500, 500) (4, 9) (0, 1)Full‡ 931 16690 17.94 500 9 0(928, 932) (12320, 23170) (13.20, 25.00) (500, 500) (5, 9.75) (0, 2)‡ For Full it is M = 100, 000, compared to M = 200, 000 for all other samplers] #I = #{i : ||γi|| > 0}, i.e. number of variables for which γi = 1 in at least one MCMC iteration†false positives and false negatives if cut-off at ratio of posterior to prior probability > 5, i.e. ifp̂(γi = 1|X, y) > 0.05The results for all 25 generated data sets are summarised in Table 6.6. The median of theRPcor90(γ) values median25k=1RPcor90(γ) = 21.46 is larger than the median of the full Gibbsvalues RFull(γ) median10k=1RFull(γ) = 17.94, although again the inter-quartile ranges overlap,with the IQR of the full-Gibbs algorithm being larger, partly because of the smaller number ofdata points.In terms of the number of variables visited by the MCMC algorithms, the picture is thesame as for simulation scenario 1. While the add/delete algorithm only visited 323 of all 500134variables at least once in the application to simulated data set 1 (see Table 6.7), this number islarger for all block samplers and increases with decreasing threshold values C. Comparing thePcor and Corr samplers with large thresholds C, the partial-correlation based samplers visitmore variables than the correlation based block samplers.Posterior variable inclusion probabilitiesFigure 6.9 shows the median values and inter-quartile ranges of the MCMC estimates of theposterior variable inclusion probabilities p̂(γi = 1|X, y) for variables (x1, ..., x10), over all25 generated data sets. These include the p∗ = 5 “true” predictors (x1, ..., x5) which weregenerated as being linked to the response variable y. As in the previous simulation scenario,the individual plots illustrate the evolution of the MCMC estimates p̂(γi = 1|X, y), whenthe number of post-burn-in MCMC iterations M increases. The results shown for variables(x6, ..., x10) are representative for all variables which are simulated not to be correlated with theresponse y, and again, the median posterior inclusion probability estimates of these variablesare close to zero for all values of M and both samplers. We also see, as observed before,that when only a small number of post-burn-in samples are available (M = 1, 000), the inter-quartile ranges of p̂(γi = 1|X, y) (i = 1, ..., 5) for the five “true” predictors include all valuesin the interval [0, 1] for the add/delete sampler and the IQRs of the block sampler also covermost of the [0, 1] range. After M = 200, 000 post-burn-in iterations the median estimates havemoved away from the extremes {0, 1} and for the block sampler converge at values of around0.8.Overall, the inter-quartile ranges are again narrower for the block sampler than for theadd/delete sampler, but seem to be wider after M = 200, 000 than in simulation scenario 1after M = 150, 000 iterations.The median values of the estimated posterior inclusion probabilities are again smaller thanone, and are between ca. 0.7 and 0.9 for the block sampler and 0.3 and 0.9 for the add/deletealgorithm at M = 200, 000. This are larger probabilities than observed in simulation scenario1. In individual data sets, the median number of false negatives is zero for the block and fullGibbs samplers, but one for the add/delete algorithm (Tables 6.6 and 6.7). The median numbersof false positives range between 8 (Pcor90) and 10 (AD), when using the same cut-off as thatused for marking the false negatives, at p̂(γi = 1|X, y) > 0.05.1351,000 iterations after burn-in0 2 4 6 8 1000.20.40.60.81Variablesp(γi=1|y,X)  Block (Pcor90)Add/deleteSimulated model10,000 iterations after burn-in0 2 4 6 8 1000.20.40.60.81Variablesp(γi=1|y,X)  Block (Pcor90)Add/deleteSimulated model50,000 iterations after burn-in0 2 4 6 8 1000.20.40.60.81Variablesp(γi=1|y,X)  Block (Pcor90)Add/deleteSimulated model200,000 iterations after burn-in0 2 4 6 8 1000.20.40.60.81Variablesp(γi=1|y,X)  Block (Pcor90)Add/deleteSimulated modelFigure 6.9: Posterior inclusion frequencies (median and inter-quartile ranges) for variables 1, ..., 10over all 25 replicates of simulation setup 2 (after burn-in period).Alternative Gibbs samplers within the blocksThe same restricted joint Gibbs samplers were applied in addition to univariate Gibbs samplerswithin the blocks as in the previous simulation scenario, i.e. Joint4 and Joint10, and inaddition the corresponding restricted univariate Gibbs samplers Rgibbs4 and Rgibbs10. Again,the Pcor90 mechanism is used to determine the underlying block structure.The results of data set 1 of simulation scenario 2 are summarised in Table 6.7. While thecomputation time needed for the Joint4 run is with 112 minutes less than half the time neededfor the univariate Gibbs run (Pcor90), the time required to run the Joint10 sampler is 30 hoursfor only NJoint10 = 110, 000 MCMC iterations instead of NPcor90 = 250, 000 iterations. At thesame time, the effective sample sizes ESS∗(γ) are only 11% of ESS∗Pcor90(γ) for the Joint4sampler, and only 30% for the Joint10 algorithm when adjusting for the differences in post-136burn-in MCMC run lengths (MPcor90 = 200, 000 vs. MJoint10 = 100, 000). We draw thesame conclusions as before, i.e. that any improvement in mixing with increasing set sizes din multivariate-Gibbs-within-block samplers Joint < d > is outweighed by the exponentiallyincreasing computation time. Also, the effective sample sizes of the multivariate samplers areonly modestly larger than those of their corresponding restricted univariate Gibbs samplers(Rgibbs4 and Rgibbs10), with ratios of the effective sample sizes being about 1.1 for bothd = 4 and d = 10 (when adjusting ESS∗Joint10(γ) for the reduced post-burn-in MCMC runlength).Random construction of blocksIn addition to blocks constructed by means of estimated correlation and partial correlationmatrices, for comparison reasons we here also construct blocks simply by randomly drawingvariables into blocks, matching the block sizes with the mean block sizes observed for thepartial-correlation-based and correlation-based block structures for the given threshold valuesC. These Random < C > block samplers are applied to the first two of the 25 generated datasets. The results for data set 1 are listed in Table 6.7, and the curves of the ratios RRandom(γ)values relative to RFull(γ) are shown in Figure 6.8. The effective sample sizes and consequentlythe ratios R(γ) = ESS∗(γ)/t of the Random samplers are similar to the Corr samplers for thetwo observed data sets. This suggests that for this simulation scenario, where the correlationstructure of the data corresponds to that of a real gene expression data set, the Corr samplersare not doing better in terms of mixing relative to computation time than block samplers withthe same mean block sizes, where the “blocks” are just random selections of genes. The Pcorsamplers, on the other hand, are consistently more efficient than the Random samplers. Theseobservations conform with the idea that the dependence structure in gene expression data canbetter be explained by a sparse partial correlation structure than by a sparse correlation ma-trix, i.e. that the sparsity is observed in terms of conditional dependence rather than marginaldependence.137Table 6.7: Mixing performance results with respect to γ for scenario 2: results for one data set (run 1)with n = 104 samples and p = 500 variables (incl. p∗ = 5 true predictors). Diagnostic measures forMarkov chain mixing with respect to γ.MCMC CPU time ESS∗(γ) R(γ) #I] # FP† # FN†sampler t (min)AD 53 104 1.96 323 14 0Full‡ 932 11640 12.50 500 10 0Block samplerPcor99 82 618 7.54 473 11 0Pcor97.5 108 1399 12.94 491 14 0Pcor95 153 2120 13.84 499 12 0Pcor90 259 4090 15.80 500 11 0Pcor80 433 6564 15.16 500 9 0Pcor60 845 13100 15.49 500 11 0Corr99 80 268 3.35 377 11 1Corr97.5 109 574 5.26 422 12 0Corr95 156 1196 7.67 463 12 0Corr90 258 2473 9.59 494 11 0Corr80 425 5163 12.14 500 10 0Corr60 972 11460 11.79 500 13 0Random99 77 187 2.41 498 13 0Random97.5 109 554 5.07 500 12 0Random95 153 1340 8.75 500 13 0Random90 247 2877 11.65 500 10 0Random80 457 5447 11.92 500 11 0Random60 846 10312 12.20 500 13 0Rgibbs4 75 412 5.53 479 11 0Joint4 112 448 3.98 482 8 0Rgibbs10 94 1099 11.67 499 8 0Joint10‡ 1777 611 0.34 485 12 0‡ For Full and Joint10 it is M = 100, 000, compared to M = 200, 000 for all other samplers] #I = #{i : ||γi|| > 0}, i.e. number of variables for which γi = 1 in at least one MCMC iteration†false positives and false negatives if cut-off at ratio of posterior to prior probability > 5, i.e. ifp̂(γi = 1|X, y) > 0.051386.4.3 Sensitivity analysis for prior variance parameter c2In all previous analyses, the covariance parameter c2 in the prior distribution of the regressioncoefficient parameters β ∼ N(0, c2Ip) was set to 5. This value was chosen to provide a rel-atively flat prior across the expected range coeffient values, and in particular to comfortablyinclude the “true” regression coefficient values β1 = ... = β5 = 2, which were used to simu-late the five covariates being linked to the response. To see, how much this choice of c2 = 5has influenced the posterior distributions not just of β but also of the main parameter of inter-est γ, a range of different values for c2 has been applied in this section. Both the add/deleteand Pcor90 block samplers have been applied to one of the data sets generated according tosimulation scenario 2.In several previous publications, where the probit model was used for variable selectionin binary regression rather than the logistic model (e.g. Brown et al. 1998b, Lee et al. 2003,Tadesse et al. 2005), the authors had warned that the posterior inference about the covariateindicator variable γ can be influenced by the choice of the prior covariance parameter c2. Forg-priors, i.e. β ∼ N(0, c2(X ′X)−1), the suggestion by Smith and Kohn (1996) to use largevalues of c2 ranging between 10 and 100 is often followed. For the independence prior, whichis used here, Brown et al. (2002) and Sha et al. (2004) suggest values which are small relativeto typically expected regression coefficient values β and are chosen in order to allow for goodinference about γ (rather than β). In particular, Sha et al. (2004) argue for using a value c2 whichimplies a ratio of prior to posterior precision of between 0.1 and 0.005. The prior precision is1/c2 for all variables xi and the posterior precision is 1/c2 +ei where the vector (ei)i=1,...,n−1 ofeigenvalues of the precision matrix is equal to the inverse of the vector of non-zero eigenvaluesof the empirical covariance matrix. Consequently, the range of c2 is given byc(ē, 0.1) < c2 < c(ē, 0.005), (6.11)where c(e, p) = (1− p)/(pe) and ē denotes the mean eigenvalue. This criterion is proposed bySha et al. (2004) for data matrices where the condition number, i.e. the ratio of maximum andminimum eigenvalues is not too large.We will compare the sensitivity of both, probit and logistic, BVS regression models withregards to the influence of c2 on Markov chain mixing and convergence behaviour, and posteriorinference about γ and β. The probit regression model is implemented in the auxiliary variable139formulation described by Albert and Chib (1993) as given in equation (3.62). The MCMCsampling algorithms are based on the algorithms for Bayesian probit regression presented inHolmes and Held (2006) and are similar to the algorithms for the logistic BVS model detailedin Appendix B.0 0.5 1 1.5 2 2.5x 105020406080100120140MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 105020406080100120140MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 105020406080100120140MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 105020406080100120140MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 105020406080100120140MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 105020406080100120140MCMC iteration−2 log likelihoodFigure 6.10: Logistic BVS model: trace plots of deviance for add/delete samplers (top) and blocksamplers (Pcor90) (bottom) with prior covariance parameters c2 = 0.5 (left), c2 = 5 (centre, standardvalue chosen throughout simulation studies in previous section), and c2 = 50 (right).Starting with the logistic variable selection model, the trace plots of the model deviancesin Figure 6.10 are used to visually monitor MCMC convergence and mixing. In terms of modeldeviance, the Markov chains mix better and converge faster, if the prior covariance parameterc2 is chosen to be small, for both add/delete and block sampling algorithms. This is also true forchain mixing at the level of the individual covariate indicators γ as indicated by the effectivesample sizes ESS∗ and the numbers of variables #I visited by the chains at least once (seeTable 6.8 at the end of this section). This behaviour is not unexpected, since decreasing thesize of c2 restricts the posterior parameter space so that it is easier for Markov chains to coverthe entire posterior distribution and find the regions of high density quickly. Note that in thelogistic variable selection model, for all choices of c2 ∈ {0.5, 5, 50} Markov chains generatedby the Pcor90 block sampler mix better than the corresponding add/delete Metropolis-Hastings140Markov chains. This is not just apparent for the global parameter model deviance as indicatedby the trace plots, but also in terms of the individual covariate indicators γ as measured by theeffective sample sizes ESS∗(γ) and the number of variables #I visited by the Markov chains(see Table 6.8).In addition to monitoring the mixing and convergence properties of the Markov chains,we also look at the posterior estimates of βγ and γ, where for each variable xi the regres-sion coefficient βγi is estimated using only those MCMC iterations where γi = 1. Rememberthat the “true” underlying vector of regression coefficients used to simulate the data set is β =(2, 2, 2, 2, 2, 0, ..., 0) and the “true” value of βγ for the model defined by γ = (1, 1, 1, 1, 1, 0, ..., 0)would be βγ = (2, 2, 2, 2, 2). So in Table 6.8, the estimates β̂γ from the posterior distribution aresummarised in terms of the ranges (minimum and maximum values) of the variables β1, ..., β5on the one hand, and of β6, ..., β500 on the other hand. While we expect the estimates of theformer to be close to the value two, the latter should vary around zero. Indeed, the estimatesβ̂γi for i = 6, ..., 500 vary around zero, with the ranges becoming larger with increasing valuesof c2. The values of β̂γi for i = 1, ..., 5 also depend on the choice of c2, with those posteriorestimates obtained with the prior covariance paramater c2 = 5 being closest to the expectedvalue 2 (although being slightly too large with ranges of (2.13, 3.03) for the add/delete samplerand (2.22, 3.16) for the block sampler). Note that the β̂γi estimates are the marginal estimatescomputed by averaging over all MCMC iterations, where γi = 1, not taking into account whichother variables are included in the model at each iteration. Hence, the estimates are not condi-tioned to the “true” model, where γ = (1, 1, 1, 1, 1, 0, ..., 0).More important for the variable selection problem is the posterior inference about thecovariate indicator vector γ. The results for γ are summarised in terms of false positives andfalse negatives, defined using the threshold p̂(γi = 1|X, y) > 0.05 as before in this chapter. Theadd/delete sampler with c2 = 50 is the only MCMC run, where not all five “true” predictorsare detected at that level, with variable 3 never even having being visited by the Markov chain.There is no obvious difference in the numbers of false positives selected by the samplers atdifferent values of c2. In summary, the logistic variable selection model is quite robust to thechoice of the prior covariance parameter c2 in terms the covariate indicator vector γ. Thisallows to use the samplers for inference about variable selection and model selection withoutneed for fine-tuning c2.1410 0.5 1 1.5 2 2.5x 105050100150MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 10505101520MCMC iterationp γ0 0.5 1 1.5 2 2.5x 105050100150MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 1050510152025MCMC iterationp γ0 0.5 1 1.5 2 2.5x 10500.511.52x 104MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 10501020304050607080MCMC iterationp γ0 0.5 1 1.5 2 2.5x 10500.511.522.5x 104MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 10501020304050607080MCMC iterationp γ0 0.5 1 1.5 2 2.5x 10500.511.522.53x 104MCMC iteration−2 log likelihood0 0.5 1 1.5 2 2.5x 105010203040506070MCMC iterationp γFigure 6.11: Probit BVS model: trace plots of model deviances (left) and model sizes (right) foradd/delete sampler with prior covariance parameter c2 = 0.05 and block samplers (Pcor90) with c2 ={0.05, 0.5, 5, 50} (from top to bottom).142The probit variable selection model is much more sensitive to the choice of c2, especiallythe add/delete algorithm, which does not even converge if c2 is chosen too large (see Table6.8). Instead, the samplers start to include more and more variables until the number of vari-ables in the model became larger than the sample size n = 104. Consequently, the samplersslowed down significantly, due to the necessity to invert large matrices of size k × k withk = min(pγ, n) in every iteration. At that point, the sampling process was stopped manu-ally due to convergence problems. As mentioned previously in Chapter 3.3.1, this problemis related to the fact, that in sparse situations with very small variable inclusion probabilityπ = p∗/p, the acceptance probability for deleting variables tends to zero with π → 0 in theadd/delete Metropolis-Hastings algorithm. This in turn means that the algorithm proposes toadd variables much more often than to delete variables, leading to the sampler running off toinclude more and more variables. The convergence problem could only be avoided by choos-ing a very small prior covariance parameter value of c2 = 0.05, which resulted in very smallposterior estimates β̂γi, but good posterior inference on the probabilities p̂(γi = 1|X, y) > 0.05for variable inclusion (see Table 6.8). Incidentally, c2 = 0.05 does not fit within the range ofvalues suggested by Sha et al. (2004), as the values in equation (6.11) correspond to the range3.33 < c2 < 73.70 for the data set used in this example, where the mean eigenvalue is ē = 2.70.The Gibbs sampler using Pcor90 blocks does not break down like the add/delete algo-rithm, if c2 is chosen larger, although the trace plots of the model size pγ (Figure 6.11) illustratethat large models are often visited with many more variables being included than in the logisticvariable selection models with the same value of c2. In addition, the model deviance trace plotsshown in Figure 6.11 indicate that the sampler frequently moves into regions of low posteriordensity, and the posterior estimates of βγ run off with magnitudes up to 1012 for c2 = 50 (Table6.8). However, the posterior inference about variable inclusion probabilities p(γi = 1|X, y) isstill quite robust, as reflected by the number of false negatives and false positives (Table 6.8).Most samplers still find all five “true” predictor variables, but the number of false positives isslightly larger than was observed for the corresponding logistic models for some values of c2,in particular for c2 = 0.5.One could circumvent this problem of having to fine-tune c2 in a probit BVS model byintroducing a hyper-prior distribution for c2. Possible hyper-prior implementations p(c2) forthe g-prior c2(X ′X)−1 have been presented by Bottolo and Richardson (2007). The viabil-143ity of possible hyper-prior distributions p(c2) for the independence prior c2Ip would dependon whether sampling from the conditional posterior distribution p(βγ, γ, z, λ|c2, X, y) remainsfeasible.Finally, in terms of the binomial prior distribution p(γi) = π (i = 1, ..., p) for the covariateindicator γ, it should be mentioned that the strategy to choose the prior so that π corresponds tothe expected fraction of true predictors among all variables (which is what we have done in thischapter by setting π = p∗/p = 5/500), might not be the best strategy, if the main interest liesin finding the “true” predictors rather than the overall “true” model. In that situation, choosinga binomial prior probability π, which is larger than the expected proportion p∗/p, would meanthat the models which are visited by the Markov chain will tend to be larger than the expectedsize p∗, which will increase the chance that all “true” variables of interest will be included inthat model. This would increase the posterior probability estimates of the individual “true”predictors p̂(γi = 1|X, y).144Table 6.8: Results of sensitivity analysis regarding the choice of c2. MCMC samplers are evaluated ondata set 1 in simulation scenario 2.\\c2 abort due to (min β̂γi, max β̂γi) (min β̂γi, max β̂γi)‡ # FP† # FN† ESS∗(γ) #I]convergence for i = 1, ..., 5 for i = 6, ..., 500problems?Logistic BVS modelAdd/delete sampler0.5 no (0.92, 1.52) (-0.85, 0.87) 13 0 177 4375 no (2.13, 3.03) (-2.34, 1.88) 15 0 84 33050 no (1.61, 3.51)[ (-1.42, 1.88) 10 2 46 147Block sampler (Pcor90, univariate Gibbs within blocks)0.5 no (0.89, 1.49) (-0.83, 0.92) 14 0 8699 5005 no (2.22, 3.16) (-1.82, 1.96) 12 0 4195 50050 no (4.59, 8.95) (-4.90, 5.65) 18 0 1629 500Probit BVS modelAdd/delete sampler0.05 no (0.36, 0.60) (-0.39, 0.42) 11 0 185 4780.5 yes N/A N/A N/A N/A N/A N/A5 yes N/A N/A N/A N/A N/A N/A50 yes N/A N/A N/A N/A N/A N/ABlock sampler (Pcor90, univariate Gibbs within blocks)0.05 no (0.35, 0.60) (-0.38, 0.43) 14 0 10020 5000.5 no (103.38, 6.00e4) (-5.64e5, 1.09e9) 49 0 9116 5005 no (2815.56, 1.59e8) (-1.42e9, 2.02e12) 25 0 8873 50050 no (236.82, 4.13e5) (-6.60e10, 2.17e12) 13 1 8246 500] I = {i : ||γi|| > 0}, i.e. number of variables for which γi = 1 in at least one MCMC iteration†false positives and false negatives if cut-off at ratio of posterior to prior > 5 (i.e. p̂(γi = 1|X, y) > 0.05)‡only for variables, which were visited at least once by the Markov chain[does not include βγ3, which was never visited by the Markov chain\\N/A = not applicable.1456.5 DiscussionMCMC sampling from the posterior distribution of a Bayesian variable selection model iscomputationally very demanding for large-scale p >> n applications. In previous publications,both the Gibbs sampler (e.g. Brown et al. 1998a, Lee et al. 2003) and the add/delete(/swap)Metropolis-Hastings sampler (see Brown et al. 1998b, Sha et al. 2004) have been used forsampling the indicator variable γ that determines the model space. As we have seen, fullGibbs sampling is computationally very demanding. For example, for the second simulationscenario with generated data for only 500 variables and with a very sparse prior, the Gibbssampler ran for 15.5 hours for 110,000 iterations on a standard desktop computer. And whilethe add/delete sampler is much faster (53 minutes for 250,000 iterations), very slow mixing isa problem, not just in terms of how many iterations it takes to convergence, but also becausethe sampler can get seriously stuck, see for example the application to a gene expression dataset in the following chapter. Note that with the logistic regression model we did not experiencethe problem reported for the add/delete Metropolis-Hastings sampler by Hans et al. (2007) andothers, that the acceptance probability for deleting variables can tend to zero for small priorvariable inclusion probabilities, but we did observe this problem for probit regression models,which were affected if the prior covariance parameter c2 was chosen too large.In this chapter we explored a simple way to account for most of the dependence structureamong covariates to create a block sampler which improves mixing and reduces the probabilityof the sampler getting stuck in a local optimum, but which is not as computationally demandingas a full Gibbs sampler. In two simulation studies we compared the block samplers derived fromboth correlation and partial correlation matrices with several choices of cut-off levels to findout, which of these choices improves mixing most relative to computation time.We compared the mixing performances as assessed by the effective sample size measureESS∗(γ) and its relation to the required computation time. In both simulation scenarios theadd/delete sampler performed worst. The performance of our block samplers improved withincreased values of C, until the threshold reached about C = 90% when the ratio of effec-tive sample sizes and required computation time started to level off. Note that since in bothsimulation studies the total number of variables was p = 500, a block threshold C = 90%implies mean block sizes of about 50. In simulation scenario 1, both correlation-based and146partial-correlation-based block samplers outperformed full Gibbs sampling for most simulateddata sets for C = 90%, while in scenario 2 only the samplers with block construction basedon partial correlations outperformed the full Gibbs sampler. Note that none of the MCMC al-gorithms are optimised with respect to computation time and that results might change withoptimised samplers.In summary, our block sampling method is simple to implement, and the partial-correlation-based sampler in particular is successful in speeding up mixing relative to required computationtime compared to standard full Gibbs sampling and the add/delete Metropolis-Hastings sam-plers. A further advantage is that is does not impose any structure on the data, because theblocks are only used as a guide for the MCMC sampler and are not part of the model. A poten-tial disadvantage is that it is quite heuristic, meaning that it is not known in advance how bigthe threshold for correlation or partial correlation values should be to achieve optimal mixingimprovements. However, some improvement is easily achieved if the threshold C is not cho-sen too small. If available, prior knowledge about the average expected number of neighboursfor the variables can be used. For gene expression data, such knowledge can be available forexample from known biological networks or from previous studies. Then the threshold C canbe set to a value so that the average block size equals that expected block size to ensure thatenough of the dependence structure is captured in order to improve mixing sufficiently.The performance of Bayesian variable selection methods is not just influenced by thechoice of the MCMC algorithm which has been the focus here. Other factors are the choiceof prior for the regression coefficients β and also the prior for the indicator variable γ. Herewe used the independence prior forβ p(β) = N(b = 0, v = c2Ip) throughout, with c2 =5 to create a relatively flat prior across the range of expected β values. In addition to theindependence prior, the g-prior N(0, c2(X ′X)−1) is often used (e.g. Lee et al. 2003, Bottoloand Richardson 2007). Arguments for both priors can be found in Brown et al. (2002) andBottolo and Richardson (2007).A sensitivity analysis for the choice of c2 has shown that while the estimates of βγ areinfluenced by the choice of c2, the estimates of γ - which is what we are most interested in here- are not influenced much in the logistic BVS model. So, with respect to the main interest offinding variables and models with high posterior probability for being linked to the response,the logistic BVS model can be applied without the need for extensive fine-tuning of the prior147covariance parameter c2. Contrary to that, in the probit regression model posterior inferenceof γ was highly sensitive to the choice of c2, and if c2 was large then the add/delete MCMCsampler broke down completely.For γ, the binomial prior p(γ) = πγ(1−π)1−γ was used here with small prior probabilitiesπ chosen so that the a priori expected number of selected genes was five for all examples. Asmentioned above, instead of fixing the prior probabilities, one can estimate π by using a beta-binomial prior.The simulation examples in this chapter were designed based on the simulation studies inGeorge and McCulloch (1993), in particular example 1. In both examples, a sparse scenariowas simulated, where most regression coefficients are zero and only p∗ = 5 variables are linkedto the response, all with the same regression coefficient value β = 2. Alternative possibilitiesinclude (a) choosing a different number p∗ of input variables being linked to the response vari-able, and/or (b) using different values βi for the variables for which βi 6= 0. Point (b), i.e. thevalues chosen for β in the simulations, was not investigated in much detail, because the maininterest here lies in assessing the ability of the MCMC algorithms to find the correct covariateindicator γ using the Bayesian variable selection models, and not the correct regression coef-ficient vector β. With respect to point (a), i.e. the number p∗ of “true” covariates linked tothe response, we found in preliminary simulation studies that in the binary classification sit-uation with correlated covariates the MCMC samplers are hard to assess, unless p∗ is chosenvery small relative to the sample size n. Otherwise there is considerable redundancy in the p∗“true” covariates and not all of them will be needed to completely explain the binary responsevector y, in which case one cannot say whether a “true” covariate was not found because ofpoor performance of the MCMC sampler or because it is redundant in the model. Even withonly p∗ = 5 covariates generated to be related to y in the two simulation scenarios that weredescribed in this chapter (with sample sizes n = 100 and n = 104), in some simulated datasets 4 or even only 3 of these covariates were found to be sufficient to completely explain thebinary response vector.In the first simulation scenario, all large dependencies between variables are generated aspositive correlations. This is not realistic for gene expression data. A more realistic scenariowas generated in the second simulation, where the empirical correlation matrix of a randomsubset of a real gene expression data set was used to generate the correlation structure. The148blocks for the MCMC samplers are built using absolute (partial) correlations, i.e. positivelyand negatively correlated variables are treated equally.As long as the covariate indicator variable γ is sampled by componentwise Gibbs steps, thechoice of variables to be updated together in a block has no direct influence on the samplingdistribution of γ itself. However, if an input variable xi is related to the response y, othercovariates which are (partially) correlated with xi are likely to be linked to the response as well.Once, one of these variables is visited by the MCMC sampler, the block sampling algorithmmakes it likely that most of the others are visited as well in the same iteration if the choiceof block structure is appropriate. Thus, “interesting” variables will be visited more often. Inaddition, since γ and the regression coefficient vector β are updated jointly in a Metropolis-Hastings step (where the Gibbs-within-blocks sampler for γ is simply the proposal distributionq(γ)), the choice of blocks influences the sampling distribution of β indirectly - by changingthe underlying γ vector for which β is updated.Joint updates of the γ parameters within a block, based on their joint conditional samplingdistribution rather than componentwise Gibbs steps, would make more direct use of the corre-lation structure. As we have seen, however, joint Gibbs samplers of fixed block sizes (d = 4and d = 10) do not improve Markov chain mixing enough compared to single Gibbs samplingwithin blocks to compensate for the increased computation time. An alternative would be aflexible joint Gibbs sampler within the blocks, where the number of γ components to be up-dated jointly would be determined adaptively. For example, in future work, one could includeadditional moves proposing to update pairs or triplets of strongly correlated variables jointlyusing multivariate Gibbs steps.Finally, note that the block updates proposed in this chapter can be readily combined withother methods for improving Markov chain mixing, e.g. parallel tempering or evolutionaryMonte Carlo. In an application to a gene expression data set in the following chapter, blocksampling will be combined with a parallel tempering algorithm involving five Markov chainstempered at different temperatures.149Chapter 7Application to ovarian cancer geneexpression dataOvarian cancer is the most lethal form of gynaecological cancer, mostly since the cancer is veryoften only detected when it is already in an advanced metastatic stage. Also, many ovarian can-cers develop resistance to the most effective known chemotherapeutic treatment, with drugsbased on platinum. It is therefore of great importance to investigate the process of platinumresistance at the molecular level and also to develop molecular markers that are able to pre-dict tumour development and clinical outcome. Prof Gabra, the co-supervisor for this project,and his group at the Ovarian cancer action (HHMT) Research Centre and Section of Molec-ular Therapeutics in the Department of Oncology at Imperial College London are involved insearching for molecular profiles for ovarian cancer that are linked to platinum resistance andother tumour characteristics as well as clinical outcomes for the patients. For this reason, anovarian cancer gene expression data set was chosen for a comprehensive analysis in which wecan compare the results of the various classification methods investigated in the context of thisthesis.7.1 The dataThe ovarian cancer gene expression data set was first published by Schwartz et al. (2002).Oligonucleotide microarrays (Affymetrix HuGeneFL gene chips) were used in the study; thegene chips contain 7129 probe sets, 7069 of which are non-control probe sets representing hu-150man transcripts - they are called genes from here on. Data are available for 104 ovarian cancertissue samples including 53 serous, 33 endometrioid, 10 mucinous, and 8 clear-cell samples.Additional 9 samples of mixed histology are not considered here, because their histologicalclass is not clear, and the aim of this analysis is to classify tumours according to their histology.Table 7.1: Histological types, FIGO stages and tumour grades for ovarian cancer samples (n = 104)Histology Grade FIGO stageI II III 1 2 3 4 UnknownClear cell (N=8) 8∗ 3 3 2Endometrioid (N=33) 10 10 13 15 5 9 3 1Mucinous (N=10) 7 3 5 2 3Serous (N=53) 3 21 29 2 2 41 6 2∗ Classified as grade III as recommended by the NCCN Practice Guidelines for Ovarian Cancer(Morgan et al. 2000).In addition to gene expression data and histology, the tumour stages (1 to 4) as definedby the International Federation of Gynecology and Obstetrics (FIGO) are available. The FIGOstage is a known prognostic factor for ovarian cancer survival and incorporates both surgicaland pathological findings. Tumour grade (I = well differentiated, II = moderately differentiated,III = poorly differentiated) data are also available as assigned by Schwartz et al. (2002). Notethat all clear-cell tumours were classified as grade III as recommended by the NCCN PracticeGuidelines for Ovarian Cancer (Morgan et al. 2000). For the gene expression analysis infor-mation on grade and stage are not taken into account. Therefore, the results will be specific forthe distribution of FIGO stage and tumour grade that is observed in this data set which con-tains mostly high-grade and high-stage tumours. However, the distribution of histological type,grade and FIGO stage observed in this data set reflects the general population of ovarian cancerpatients presenting at U.S. hospitals (see Schwartz et al. 2002), where grade III and high-stagetumours are also most commonly observed.The gene expression data were pre-processed and normalised in the same way as describedin Chapter 5. In Figure 7.1, the first two principal components of the data are plotted againsteach other, labeled according to the histological subtypes of the samples. The plot is very sim-ilar to the corresponding Figure 1A in the original publication (Schwartz et al. 2002). Some1510.096 0.097 0.098 0.099−0.2−0.10.00.10.2PCA component 1PCA component 2///123456789101112131415161718192021222324252627282930313233343536373839414243444546474849505152 5354555657585960616263646566676869707172737475767778798081828384858687888990919293949596 97989910010110210310440/clear cellserousmucinousendometrioid/ often misclassifiedFigure 7.1: Plot of first versus second principal component of background corrected and loess nor-malised Schwartz et al. (2002) data, with data points being labelled according to the histological sub-type of the samples. The three samples which are misclassified most often by the sparse multivariateclassification methods in the analysis in Chapter 5, have their sample IDs crossed out.differences arise because the data were pre-processed in a slightly different way than describedin the original article. Figure 7.1 shows that, when the data are represented by the first two prin-cipal components, clear-cell, mucinous, and serous samples are separated quite clearly, whilethe endometrioid samples (displayed as yellow circles) overlap with all three other subtypes.7.2 Further analyses following on from the resampling studyThe ovarian cancer data set was part of the resampling study performed in Chapter 5. Follow-ing on from the study, the data set was analysed further, as the gene sets that emerged were ofspecial interest to Prof Gabra and his group, particularly with respect to the interesting result152of the Schwartz et al. (2002) paper that gene expression data can be linked to the histology ofovarian cancers. Because of the known inherent platinum resistance of clear-cell and mucinouscarcinomas, a re-analysis of these data is performed here with the aim to identify parsimoniousmolecular profiles which discriminate between clear-cell/mucinous and serous/endometrioidsamples. Hence, the 18 mucinous and clear-cell samples are grouped together and the 86endometrioid and serous samples are also grouped together for the classification analyses. Be-cause the class sizes are very unbalanced, the class proportions were fixed in the resamplingstudy. That is, in each resample, of the 18 clear-cell/mucinous samples, 12 are assigned to thetraining set and 6 to the validation data set.In the course of the resampling study, groups of genes were found, which can accuratelydistinguish intrinsically platinum-resistant histologies from the others. In particular, five un-related genes were identified, which may indicate underlying separate pathways that differbetween these histological groupings. Here, these five genes are investigated further and de-scribed in some detail. The expression levels of these five genes are evaluated in an independentgene expression data set (Lu et al. 2004).7.2.1 Five interesting genes and their biological functionAs reported in Chapter 5, sparse molecular profiles, that discriminate well between intrinsicallyplatinum-resistant and more responsive histologies, can be found by univariate filtering as wellas multivariate dimension reduction methods (here lasso regression and the elastic net, as wellas random forests combined with variable selection varSelRF). For the range of tuning param-eter values given in Chapter 5, we observed minimum prediction misclassification error ratesof two out of all 68 training samples (2.9%).A small set of genes associated with these molecular profiles is very stable. In particular,lasso regression selects the same five genes into more than half of the m = 50 resamples, forall penalty values λ1 > 0.01, as seen in Figure 5.4. These are the ANX4, ABP1, CYP2C18,SPINK1, and S100P genes listed in Table 7.2. These genes also get chosen very often intoelastic net profiles. Two of these (ANX4 and ABP1) are the only variables that get chosen intomore than half of the very sparse varSelRF molecular profiles. While we observe this goodagreement for the sparse multivariate methods, different genes are included most frequently by153Table 7.2: Genes which appear in at least 50% of all molecular profiles resulting from applying thelasso to the Schwartz et al. (2002) data (where λ1 > 0.01).Genbank Gene symbol Description (NCBI Genbank definition and keywords) In Table 2 inaccession original paper?M82809 ANX4 Human annexin IV (ANX4) mRNA (keywords: annexin IV; yeschromobindin 4; placental anticoagulant protein II.)U11862 ABP1 Human clone HP-DAO1 diamine oxidase, nocopper/topa quinone-containing mRNAM61853 CYP2C18 Human cytochrome P4502C18 (CYP2C18) mRNA, clone 6b. no(keywords: cytochrome P450; cytochrome P450 2C18)Y00705 SPINK1 Homo sapiens pstI mRNA for pancreatic secretory inhibitor no(expressed in neoplastic tissue)(keywords: pst1 gene; trypsin inhibitor)X65614 S100P H.sapiens mRNA for calcium-binding protein S100P no(keywords: calcium binding protein; S100 protein)Gene KEGG pathways Gene Ontology - biological processsymbolANX4 none GO:0006916 anti-apoptosisGO:0007165 signal transductionABP1 Amino acid metabolism pathways GO:0008152 metabolismCYP2C18 hsa00590 Arachidonic acid metabolism GO:0006118 electron transporthsa00591 Linoleic acid metabolismhsa00980 Metabolism of xenobioticsby cytochrome P450SPINK1 none noneS100P none GO:0043542 endothelial cell migration154univariate filtering. Only one of the five genes is also part of more than half of those univariateprofiles where p < n, that is with p∗ ≤ 50 (S100P). This is reflective of the fact that variablesget selected independently in the univariate filtering approach, while multivariate methods takeinto account the correlation structure between genes. When applying the univariate method tothe complete data set and ordering the variables by the size of their absolute estimated effects,the five genes appear in places 13 (S100P), 60 (ABP1), 354 (ANX4), 501 (CYP2C18), and 540(SPINK1).Only one gene, namely ANX4, was also found to be up-regulated in clear-cell carcinomasin the original publication (Schwartz et al. 2002) based on a fold-change of two. It is knownto be involved in anti-apoptosis and signal transduction pathways, which is reflected by itsassociated Gene Ontology biological processes (The Gene Ontology Consortium 2000), whichare listed in Table 7.2. Two of the five genes are associated with metabolic pathways accordingto the KEGG pathways database (Ogata et al. 1999): ABP1 is known to be involved in aminoacid metabolism, and CYP2C18 belongs to the class of cytochrome P450 enzymes, which formthe most prominent group of drug-metabolising enzymes in humans (see Table 7.2). Mutationsin cytochrome P450 genes or deficiencies of the enzymes are responsible for several humandiseases. For example, the expression of some P450 genes is known to be a risk factor inseveral cancers since these enzymes can convert procarcinogens into carcinogens. The genefound here is part of the CYP2 group of enzymes, which are known to be involved in steroidmetabolism and is hence a likely candidate for ovarian cancer, which is influenced by estrogenlevels. Several studies have been conducted to examine a potential role of CYP2C18 (amongother cytochrome P450 genes) in human breast cancer (e. g. Modugno et al. 2003, Knüpfer et al.2004), but in none of these studies has it been found to have an elevated level of expression inbreast tumours. To our knowledge CYP2C18 has not yet been studied in the context of ovariancancer. Another gene listed in Table 7.2, which is of interest is the calcium-binding proteinS100P. It has been shown to have elevated expression levels in several cancers, for examplepancreatic cancer (e. g. Ohuchida et al. 2006) and breast cancer (Guerreiro Da Silva et al.2000). There is also evidence that the SPINK1 gene is up-regulated in several cancers, forexample in pancreatic gland, breast, thyroid, stomach, oesophagus and gall bladder cancers(for an extensive review see Paju and Stenman 2006). In addition, some papers report highconcentrations of the SPINK1 gene in the urine of patients with mucinous ovarian cancer (e. g.155Turpeinen et al. 1988, and references therein). We also observe that the expression of theSPINK1 gene is elevated in mucinous tumours, but not in clear-cell or other carcinomas (seeFigure 7.2).gene expression levelANX4 ABP1 CYP2C18 SPINK1 S100P456789101112clear cellmucinousendometrioidserousgene expression levelANX4 ABP1 CYP2C18 SPINK1 S100P678910111213clear cellmucinousendometrioidserousFigure 7.2: Expression levels of the five genes listed in Table 7.2 in Schwartz et al. (2002) data (left),and in an independent data set by Lu et al. (2004) (right). The lines represent the mean expression levelsfor the intrinsically platinum-resistant clear-cell/mucinous class (orange) and the serous/endometrioidclass (blue).7.2.2 Dependence pattern among the five genesDue to the nature of the L1-penalty imposed on the size of the regression coefficients in thelasso method, lasso tends to select variables that are less related than would be found by othermethods (Tibshirani 1996), as we have observed in Chapter 5. This is in agreement with thefact that the five probe sets listed in Table 7.2 are not directly connected in the conditionalindependence (CI) graph, part of which is shown in Figure 7.3, except for one joint neighbour(TM4SF4), which is shared by S100P and SPINK1. Note that two gene symbols appear twiceeach: ERBB2 as two neighbouring nodes of SPINK1, and PTHLH as a neighbour of both S100Pand SPINK1. This is due to genes being represented by several probe sets on the Affymetrixgene chips, and in the case of the ERBB2 and PTHLH genes, two of these probe sets are156ABP1FXYD2MAD1L1DAPK1CYP2C18CYP2C9SPINK1CKMT2CRPRARACASC3SLC39A14ALDOCAFFX.HUMRGE.M10098STAT5BSERPINA1NEBKRT10ERBB2PGK1PTHLHSTAT3HG1862.HT1897GSTT2RPL27DNASE1HG4533.HT4938GNLYACLYAMELXX94629ADH1AERBB2KRTHA5TPM1VIL1HABP2HG4264.HT4534AP2B1TM4SF4 S100PPTHLHCSTAPTPRRPPARGCOL17A1ANX4Figure 7.3: Conditional independence graph with edges being drawn between two gene nodes if theirpartial correlation has a 95% posterior probability for being different from zero. Only the five genesin Table 7.2 (coloured in blue) and their direct neighbours are shown. For computational reasons, only4000 genes selected by univariate filtering could be considered here. Nodes are labelled by the genesymbols or by the Affymetrix probe set IDs if gene symbols are not available.157depicted in the graph. In the graph in Figure 7.3, the SPINK1 node looks like a hub with a totalnumber of 33 directly neighbouring nodes.Due to computational restrictions, it was not possible to construct the CI graph from theentire data set, because the algorithm implemented in the R library corpcor is restricted toa maximum number of nodes of about 4000. Hence, the top 4000 genes were selected forgraph construction, based on the univariate filtering method applied to the complete data set.Note that these are the same 4000 genes, which get used in the Bayesian variable selectionapplication in Section 7.3. Figure 7.3 only shows the five genes and their direct neighbours,not the complete CI graph, which would contain all connections between the 4000 genes andwould be too complex for a clear picture. This implies that more remote connections betweenthe five genes, involving more than one intermediate gene node, are not shown in the graph. Inthe conditional independence graph, two gene nodes are connected by an edge if their partialcorrelation conditioning on all other genes in the data set has a 95% posterior probability forbeing different from zero (see Schäfer and Strimmer 2005).7.2.3 Expression levels in independent dataWithin the context of the resampling study presented in Chapter 5, the molecular profiles de-rived for the Schwartz et al. (2002) data were also applied to an independent data set publishedby Lu et al. (2004) for validation. In Lu et al. (2004), the Affymetrix GeneChip Human GenomeU95 set of oligoarrays was used to obtain gene expression data, which is a newer chip set thanthe HuGeneFL oligoarray used in Schwartz et al. (2002) and which contains the five genesof interest. Data are available from ovarian carcinomas for 42 patients including 17 serous, 9endometrioid, 7 clear-cell and 9 mucinous tumours. The distribution of samples in terms oftumour grade and FIGO stage is similar to the data set by Schwartz et al. (2002). For moreinformation about clinical characteristics see the paper by Lu et al. (2004). The right plot inFigure 7.2 shows the expression levels of the five genes in the new data set. They are similarto the levels observed in the Schwartz et al. (2002) data set, albeit with less marked differencesbetween the two classes, especially in the case of CYP2C18.When a logistic regression model is applied to the Lu et al. (2004) data with the fivegenes as covariates, a misclassification error rate of 14% (6/42) is achieved, which is clearly158worse than the error rates observed in the resampled validation data subsets of the Schwartzet al. (2002) data (there for the lasso logistic regression model, the median misclassificationerror rate was 3%). On the other hand, the error rate of 14% achieved by the the five genes issignificantly smaller than expected under the null hypothesis of a random set of five genes: aone-sided permutation test is performed by comparing the observed misclassification rate withthe five-percent quantile of the misclassification error distribution of 1000 random sets of fivegenes sampled from the Lu et al. (2004) data set with replacement in a bootstrap procedure.The five-percent quantile is with 19% (8/42) larger than the observed error rate indicatingsignificance.7.3 Bayesian variable selectionIn addition to an analysis of the ovarian cancer data set in the context of the resampling studythat was described above, the same data set was also used as an example application of theblock MCMC algorithms developed for Bayesian variable selection in Chapter 6, as well asan application of the vanilla add/delete Metropolis-Hastings sampler. Both these samplers arealso combined with a parallel tempering algorithm, and the results are presented here. Finally,a comparison with the shotgun stochastic search algorithm by Hans et al. (2007) is also per-formed.Of the 7129 available probe sets, only p = 4000 are used here. These variables are se-lected based on the absolute size of their estimated coefficients in univariate logistic regressionmodels applied to the complete data set. The restriction to 4000 variables is not required bythe MCMC algorithms themselves, but rather by the computational restrictions of the R codein the corpcor library, that is used for sparse estimations of the correlation and partial corre-lation matrix, and on which the decisions are based, for which variables the covariate indicatorparameter γ should be updated together in the same block.In the logistic Bayesian variable selection model, the sparsity-inducing hyper-parameterin the prior of γ is set to π = 5/p = 0.00125, so that 5 variables are expected to be selected apriori. The prior covariance parameter c2 in the prior distribution of β is set to c2 = 10 for arelatively flat prior distribution in the expected range of values for β. The data are standardisedto zero mean and unit variance for all gene variables, and hence even extremely large effect159sizes are unlikely to be outside the range covered with high prior probability. The value c2 = 10is larger than in the simulation examples in Chapter 6 to account for the fact that now thetrue |βi| values are unknown and not set within the range [−2, 2], as it was the case with thesimulation data.7.3.1 MCMC sampling with and without parallel temperingWe here compare the performances of four MCMC algorithms for sampling from the logisticBVS model: the vanilla add/delete Metropolis-Hastings sampler, a block MCMC sampler asdeveloped in Chapter 6 and in addition both samplers in combination with a parallel temperingalgorithm.In Chapter 3.3.2, two possible implementations of the parallel tempering method weredescribed. In one scenario, only neighbouring Markov chains in the temperature ladder areproposed for state swaps in a Metropolis-Hastings algorithm. This scenario is applied here,with five parallel Markov chains and a geometric temperature ladder {1, τ, τ 2, τ 3, τ 4} withτ = 1.2. An alternative scenario, that has not been applied here, is the all-exchange paralleltempering scheme by Calvo (2005), where all possible pairwise swap acceptance probabilitiesare computed for all parallel chains in each iteration and the pair of chains, that is to be swapped,is sampled according to this probability distribution. This algorithm has also been implementedin MATLAB and the code is available on http://www.bgx.org.uk.In the simulation studies in Chapter 6, we found that block samplers, where blocks arebased on the partial correlations, are preferable to block samplers based on correlations, espe-cially in the simulation scenario where the data were simulated using real gene expression data.Of all these Pcor block samplers, we found the Pcor90 sampler to perform particularly well.In the simulation examples in which the data sets had p = 500 variables, the threshold levelC = 90% resulted in mean block sizes of 51 variables. The computation time per iteration of ablock MCMC sampler is determined by the absolute block sizes, i.e. the numbers of variablesfor which the γ parameter is updated together in an MCMC iteration. In the gene expressiondata set considered here, where the total number of gene variables is p = 4000, a thresholdvalue of C = 99% will lead to mean block sizes similar in magnitude to those observed for thePcor90 sampler in the simulation studies. Hence, the Pcor99 sampler is applied to the ovarian160cancer data set.All samplers are applied with one long MCMC run with a total number of N = 1, 100, 000iterations including B = 100, 000 burn-in samples, leaving a total of M = 1, 000, 000 MCMCiterations for posterior inference. The parallel tempering algorithm is implemented with fiveparallel Markov chains at different temperatures determined by the geometric temperature lad-der {1, τ, τ 2, τ 3, τ 4} with τ = 1.2. To improve chances for the acceptance of proposed stateswaps, only neighbouring Markov chains are proposed for swaps as follows. One pair of neigh-bouring chains with temperatures τ k and τ k+1 (k = 0, ..., 3) is selected at uniformly randomand a swap is proposed between them. Note that all parallel Markov chains are run un-coupled,i.e. without state swaps, for BPT = 50, 000 iterations before starting the parallel temperingalgorithm proper. This is done in order to allow the Markov chains to move towards their targetdistribution before starting state exchanges between chains.0 2 4 6 8 10x 105050100150MCMC iteration−2 log likelihood0 2 4 6 8 10x 105050100150MCMC iteration−2 log likelihood0 2 4 6 8 10x 105050100150MCMC iteration−2 log likelihood0 2 4 6 8 10x 105050100150MCMC iteration−2 log likelihoodFigure 7.4: Trace plots of model fit in terms of deviances (-2log-likelihood) for add/delete sampler(left) and block sampler (Pcor c = 99%) (right), with (bottom) and without (top) parallel tempering inapplication to gene expression data (Schwartz et al. 2002).161Figure 7.4 shows the trace plots of the model deviances for the Markov chains of allfour MCMC samplers. According to these plots, all samplers, including the simple add/deletesampler, manage to converge to the regions of the model space with the highest observed likeli-hoods within the burn-in period of B = 100, 000 iterations. Interestingly though, the add/deletesampler leaves that region again after a total of about 400, 000 iterations and seems to get stuckin an area of the model space which fits the data less well, i.e. with increased model de-viances. The sampler is unable to leave that area before the total number of MCMC iterationsN = 1, 100, 000 is reached. This is an example of a Markov chain getting stuck in a local modewhich does not represent the best-fitting models. Posterior distribution estimates based on thesamples from such a Markov chain may be seriously biased.In this particular case, the add/delete sampler in that local mode selected models, whichconsistently contain the two variables with IDs 354 (ANX4) and 1232 (TFF1), so that over-all these are the only two variables for which the add/delete-sampler-based estimates of themarginal posterior probability are larger than p̂(γi = 1|X, y) = 0.5. The other three MCMCalgorithms all find that two other variables are the only ones with marginal posterior inclu-sion estimates which are larger than 0.5, namely the genes with ID 501 (CYP2C18) and 540(SPINK1). Gene ANX4 is also frequently included in models selected by the other three sam-plers, but gene TFF1 does not play a role. So it seems, that in this situation the TFF1 geneis a false positive due to poor mixing of the add/delete sampler. Interestingly, genes ANX4,CYP2C18 and SPINK1 are all in the set of five genes consistently found by the sparse mul-tivariate classification methods applied in the context of the resampling study (see previoussection and Chapter 5).The traces of the individual covariate indicator variables γi for all variables i = 1, ..., p =4000 are shown in Figure 7.5. The trace plots illustrate the extremely slow mixing of theadd/delete sampler at the level of individual γi variables. Mixing improves when adding theparallel tempering algorithm, and also when replacing the add/delete sampling algorithm bythe block sampler. Based on the trace plots, mixing performance is best for the MCMC blocksampler combined with parallel tempering.Diagnostic measures for Markov chain mixing listed in Table 7.3 confirm this impressiongained from the trace plots. The effective sample size is largest for the parallel tempering al-gorithm when combined with the block sampler with ESS∗(γ) = 41, 985, and about half that162Figure 7.5: Trace plots of γ vector for add/delete sampler (left) and block sampler (Pcor c = 99%)(right), with (bottom) and without (top) parallel tempering in application to gene expression data(Schwartz et al. 2002).with ESS∗(γ) = 19, 900 when combined with the add/delete sampler. That is, one million postburn-in MCMC samples correspond to about 42,000 (block sampler) and 20,000 (add/delete)independent samples, respectively. Compared to this acceptable result, the effective samplesize is only ESS∗(γ) = 8 for the vanilla add/delete sampler without parallel tempering, whichis clearly not large enough for valid posterior inference about the γ vector. Thus, the im-provement in effective sample size from the introduction of parallel tempering is huge for theadd/delete Metropolis-Hastings algorithm. The improvement is not as large for the block sam-pler, but the effective sample size still increases about eleven-fold from ESS∗(γ) = 3, 793 forblock sampling without parallel tempering, which means that it is still advantageous to performthe parallel tempering algorithm, since the computation time only increases about five-fold dueto having to run five Markov chains rather than just one. Note that our parallel tempering imple-mentation is serial, but could of course be done in parallel if parallel processors are available.163Table 7.3: Diagnostic measures for Markov chain mixing with respect to γ; results for M = 1, 000, 000post burn-in MCMC iterations (CPU time for total iteration number N = 1, 100, 000).MCMC CPU time ESS∗(γ) #I] # can- # not can-sampler t (min) didates† didates‡Add/delete 315 8 198 1 23Blocks 1356 3,793 2856 3 6Parallel temperingwith add/delete 1726 19,900 1091 4 15with blocks 6601 41,985 3752 4 5]I = {i : ||γi|| > 0}†How many of the five genes consistently selected by lasso and other multivariate methods in resampling studyare recovered by BVS, if cut-off at ratio of posterior to prior > 10, i.e. p̂(γi = 1|X, y) > 0.0125?‡How many genes are consistently selected besides the five candidate genes?In a parallel implementation, the computation time would only increase slightly, so that addingparallel tempering to the MCMC algorithm can increase chain mixing dramatically with nearto no cost in terms of increased computation time.The improvement in mixing by introducing block sampling and parallel tempering is alsoseen in the number of γi variables, which are visited at least once by the MCMC samplers. Theparallel tempering with block sampling approach visits #I = 3752 variables out of all 4000which corresponds to 94%. The block sampler without the added parallel tempering schemealready results in good mixing and visits #I = 2856 variables (71%), whereas the mixing ofthe vanilla add/delete sampler is very poor and visits only 5% of all variables (#I = 198). IfMarkov chain mixing is interpreted in terms of the number of variables being visited at leastonce, rather than by ESS, then the parallel tempering add/delete sampler is not doing as wellcompared to the single-chain block sampler, since it only visits 1091 variables (27%).Of the five genes, that were consistently found by the multivariate classification methodsin the resampling study as described in the previous section, four genes are retrieved by theparallel tempering algorithm (with both add/delete and block sampling), namely ABP1, ANX4,CYP2C18, and SPINK1. The only gene not recovered, is the S100P gene, which has a posteriorinclusion probability of 0.001. A total of 145 genes had larger posterior inclusion probabilitiesestimated by the parallel tempering block sampler. Individual gene variables are considered to164be selected when their marginal posterior probabilities p̂(γi = 1|X, y) are sufficiently large.The cut-off value was set to p̂(γi = 1|X, y) > 0.0125 for all variables, which corresponds toa ratio of posterior to prior variable inclusion probability of 10. The block sampler withoutparallel tempering manages to retrieve three out of the five genes (ANX4, CYP2C18, SPINK1),while the vanilla add/delete sampler only finds the ANX4 gene. Note that overall, at the samecut-off for the marginal posterior variable inclusion probability, more variables are selectedby the add/delete MCMC algorithms (with and without parallel tempering) than by the blocksamplers (Table 7.3).Recall that the prior variable inclusion probability was chosen to reflect an expected modelsize of p∗ = 5, i.e. π = p∗/p = 5/4000. As was noted before, it might have been of advantageto choose a larger prior variable inclusion probability, e.g. π = 10/4000, especially with respectto the analysis aim of finding variables with large marginal posterior inclusion probabilities. Alarger value for π would have resulted in larger models, providing more opportunities for genesto get selected. It also results in more flexible models, which makes mixing easier for theMarkov chains. In summary, it would have improved the chances to quickly find all importantgenes.Here, the parallel tempering algorithm was implemented with the geometric temperatureladder {1, τ, τ 2, τ 3, τ 4} with τ = 1.2. The value τ = 1.2 was selected based on the observedacceptance probabilities α for swaps of neighbouring parallel chains. These were chosen tobe within the suggested range of acceptance probabilities between 15% and 50% as detailed inSection 3.3.2 in the Methods chapter 3. In fact, the overall mean acceptance frequencies for astate swap are 44% (Pcor99 sampler) and 39% (add/delete). The geometric temperature ladderscheme works well and swap frequencies between all chain pairs agree with the suggestedrange of acceptance probabilities. However, the scheme is not optimal, as the neighbouringchains with the lowest temperatures are swapped more frequently (33% of all swaps for bothadd/delete and block samplers) than the pair of chains with the highest temperatures (18% ofswaps for block MCMC and 19% for add/delete MCMC). One could improve the temperingscheme by adopting the temperatures in the burn-in phase of the MCMC sampling rather thanusing the geometric scheme, as has been done for example by Bottolo and Richardson (2008).1657.3.2 Shotgun stochastic searchIn Chapter 3.3.2, the shotgun stochastic search (SSS) algorithm (Hans et al. 2007) was de-scribed as an alternative to tempered MCMC methods for speeding up the stochastic searchprocess to find variable selection models with large posterior probability. Here, the SSS algo-rithm is applied to the ovarian cancer data set. We used the serial version of the SSS 2.0 soft-ware (http://www.stat.duke.edu/research/software/west/sss). To makecomparisons with the tempered MCMC approach easier, the prior specifications are kept assimilar as possible. This is aided by the fact, that Hans et al. (2007) have implemented thelogistic Bayesian variable selection model with the same prior distributions for β and γ as wehave done. That is, p(γ) = πpγ (1 − π)p−pγ , where pγ is the number of variables in the modeland the prior inclusion probability π = 5/4000 = 0.00125 is chosen to be the same as in theprevious section. For β the independence prior is applied, that is p(β|γ) = N(0, c2Ipγ ). Un-fortunately, at the time of submission of this work, it is not yet possible to specify the priorvariance parameter c2 for the serial logistic regression version of SSS, so that the pre-specifiedvalue c2 = 1 had to be used rather than c2 = 10, as in the MCMC application. However, as wasillustrated in the sensitivity analysis in Chapter 6, a smaller value of c2 is unlikely to have a biginfluence on the posterior inference about γ in the logistic BVS model. For computational rea-sons, only models with 20 or less covariates are evaluated during the run of the SSS algorithm.As none of the MCMC samplers in the previous section ever selected a model with more than15 variables, this is not considered to be a serious restriction to the model space that is to besearched.The shotgun stochastic search algorithm is run for N = 110, 000 iterations, which is 1/10of the total number of MCMC iterations in the previous section. But since SSS evaluations acomplete neighbourhood of models in each iteration, the total number of models that are evalu-ated by SSS is much larger than 110, 000. The total number of evaluated model is unknown asit is not part of the output of the SSS software, which makes it hard to compare the efficiencyof the shotgun stochastic search algorithm with our MCMC samplers. In total, the algorithmran for more than 90 hours (Table 7.4), which is similar to the computation time required forthe run of the parallel tempering algorithm with block sampling (Table 7.3).For the posterior inclusion probability estimates p̂(γi = 1|X, y), we use the best 100,0001660 2 4 6 8 10x 104−40−35−30−25modellog posterior probabilityFigure 7.6: Log posterior probabilities of best 100,000 logistic Bayesian variable selection models(in decreasing order) as evaluated by shotgun stochastic search algorithm. The black horizontal lineindicates the log prior probability of a model with five variables.evaluated models, that is the 100,000 models with largest posterior probabilities. The posteriorprobabilities of the models were evaluated using the Laplace approximation for the marginallikelihood (DiCiccio et al. 1997). Figure 7.6 shows the log posterior probabilities of the best100,000 models. The black line in the plot indicates the log prior probability of a model withpγ = 5 covariates, which is pγ log π + (p − pγ) log(1 − π) ≈ −38.42. Only models, whichare among the first approximately 60,000 models, have log posterior probabilities which arelarger than the log prior probability, which suggests that all models that are not among those≈ 60,000, do not contribute much to the total posterior probability mass anymore, particularly,because all the best 60,000 models have extremely low dimensionality: model sizes pγ rangefrom 1 to 4; the large majority of models has 3 variables (89.01%). The fact that the priorand posterior model probabilities are of the same order of magnitude reflects the large impactthat the prior has on the posterior compared to the likelihood function. This is typical for ap >> n data situation and shows again that the choice of prior distributions drive the posteriordistribution and what kinds of models are going to be selected; in particular the model size isaffected.167Table 7.4: Shotgun stochastic search results.CPU time t (min) #I] # candidates† # not candidates‡5435 4000 5 13]I = {i : ||γi|| > 0}†How many of the five genes consistently selected by lasso and other multivariate methods in resampling studyare recovered by BVS, if cut-off at ratio of posterior to prior > 10, i.e. p̂(γi = 1|X, y) > 0.0125?‡How many genes are consistently selected besides the five candidate genes?The shotgun stochastic search algorithm retrieves all five genes, that were consistentlyfound by the multivariate classification methods in the resampling study as described in theprevious section (Table 7.4). As in the MCMC applications, a gene is singled out if its observedposterior-to-prior ratio of the marginal variable inclusion probabilities is larger than 10. Forthe marginal posterior probability, this implies that p̂(γi = 1|X, y) > 0.0125. In total, 18genes have a marginal posterior inclusion probability that large. For one gene, this probabilityis even larger than 0.5, namely for the gene with ID 354 (ANX4). Note, that this gene wasalso selected very often by the vanilla add/delete MCMC sampler, but not by any of the otherMCMC samplers.7.4 Summary and discussionAll sparsity-inducing multivariate methods, that were applied to the ovarian cancer gene ex-pression data set by Schwartz et al. (2002), agree on a core set of genes, which are consistentlyselected by all methods. This is perhaps not surprising, since all these methods, except therandom forest method (varSelRF), are based on the logistic regression model. In all methods,sparsity is induced by the prior distributions. This is either directly the case in the Bayesianvariable selection applications, or indirectly for the penalised likelihood methods lasso andelastic net, where the penalised maximum likelihood solutions are equivalent to maximum aposteriori solutions of Bayesian models with sparse priors.In Table 7.5, all genes are listed, for which their marginal posterior probability was es-timated to be larger than 0.0125 either by the shotgun stochastic search algorithm, or by theMCMC algorithm, which arguably performed best in terms of mixing, i.e. the block samplercombined with parallel tempering. In the absence of posterior probability estimates for the168Table 7.5: Genes, which either have marginal posterior probability estimates larger than 0.0125 (onlyestimates by block parallel tempering or shotgun stochastic search considered), or were selected into atleast 50% of all profiles in the resampling study (by lasso or elastic net, for at least one tuning parametervalue).Gene ID Gene symbol Sampling algorithm Penalised likelihoodblock PT SSS Lasso Elastic net8 KIAA0513 X10 FGFR4 0.040 X13 S100P (0.001) 0.042 X X23 TFPI 0.026 0.02149 CYB5R3 0.050 X52 BDKRB2 0.040 X60 ABP1 0.028 0.183 X X66 ID4 0.02087 CYP2C9 0.041108 GGT1 0.015129 GPLD1 0.029150 CEACAM6 0.041 X171 PTPN12 0.016297 C4BPB 0.036354 ANX4 0.201 0.844 X X367 CEACAM7 X455 LBP 0.017501 CYP2C18 0.774 0.089 X X540 SPINK1 0.722 0.049 X X760 FXYD2 0.1391007 TCN1 X1232 TFF1 0.160 0.0421360 LGALS4 0.138 0.024non-Bayesian multivariate methods employed in the resampling study, the frequency of beingselected into the resampled molecular profiles is used to highlight, which genes are assignedthe highest confidence by these methods. So Table 7.5 also lists the genes, which are selectedinto at least 50% of all resampled profiles - using the sparse penalised likelihood methods, that169is either lasso or the elastic net.In total, there are 23 genes listed in Table 7.5. Of these, 12 genes have been singledout by at least two of the methods listed in the Table, which overall indicates a good agree-ment between the methods. Of the remaining 11, 6 genes are only selected by the shotgunstochastic search algorithm, all with relatively small posterior inclusion probability estimatesp̂(γi = 1|X, y) ≤ 0.041. This might indicate, that the SSS algorithm was not run quite longenough. Maybe these genes were only selected in the beginning of running the SSS algorithm,before the algorithm had converged to the correct regions of the posterior model space - as thereis no burn-in period implemented in the SSS software, i.e. all models are counted right fromthe first iteration. Unfortunately, we cannot follow up on this guess, because the SSS softwaredoes not provide output about the iterations, in which the posterior probability of a model isevaluated.The posterior probability estimates provided by the Bayesian variable selection modelallow an assessment of the importance of an input variable for the explanation of the responsevariable in a given data set. This goes hand in hand with the size of the regression coefficientestimate, which gives an indication about the size of the effect that the variable has on theresponse. However, these estimates say nothing about the prognostic value of a variable - orof a model - for future predictions of the response status. Variables with a very large posteriorinclusion probability can still only have a limited value for response prognosis. So we concludethis chapter by noting that here, the Bayesian variable selection model was merely fitted to theovarian cancer data (using various sampling methods). The models have not been assessed withrespect to their prognostic value and ability to predict the response status of new observations.This could be done by splitting the data into training and validation data or in a resampling setupusing cross-validation or bootstrapping methods to accurately estimate the prediction error (orother loss function). Alternatively, one can use posterior predictive distributions. Here, onedraws hypothetical new response values from the posterior model and compares their samplingdistribution with the empirical distribution of observed response values. If the distributionsare very similar, then one can conclude that the model is useful for the prediction of newobservations, if those new observations were sampled from the same population as the originaldata.170Chapter 8Discussion and future directionsThe aim of this work was to investigate ways to implement multivariate dimension reductionand regularisation methods for statistical inference of high-dimensional “large p, small n” data,where the number of covariates p is much larger than the sample size n. The work was done inthe context of gene expression array data, collected in the context of clinical studies of cancerin order to find molecular profiles, which help to discriminate between different tumour typesor to predict the clinical outcome for individual patients.One way of dealing with “large p, small n” data is regularisation (or shrinkage), wherethe effects of all input variables are shrunken towards zero. Another approach is variable se-lection, where the number of input variables is reduced to the p∗ < n most influential ones.Variable selection can also be framed as a specific form of regularisation, which is achievedthrough a penalty term restricting the size of the L0-norm of the coefficient vector β, that is||β||0 =∑pi=1 I(βi 6= 0) (where I(.) is an indicator function). Equivalently, sparse regularisa-tion methods that shrink the effects of most variables to zero provide an automatic approach tovariable selection.This work focussed on sparse dimension reduction methods rather than regularisationmethods that keep all variables, because concise gene lists are easier to interpret biologicallyand also provide a more clear-cut starting point for further exploration. Also, for the use ofgene expression profiles as prognostic factors in future clinical applications, small profiles con-sisting of a few genes only are cheaper and easier to apply in a large-scale clinical trial thanblack-box prognostic models, which require the complete microarray data as input from eachtissue sample.171A main result of this work is the large observed instability of molecular profiles, both interms of the low similarity between resampled profiles observed in the resampling study inChapter 5 and in terms of small marginal posterior inclusion probabilities for the most frequentgenes in the Bayesian variable selection models in Chapter 7. This emphasises the impor-tance of measures of uncertainty for the selection of individual variables as well as models,i.e. molecular profiles. In this light, the fully Bayesian variable selection approach seems mostsuited to the task of identifying gene expression profiles and associating measures of uncer-tainty through posterior variable inclusion probabilities as well as posterior model selectionprobabilities. We consider it unfortunate, that some methods like ridge regression or the elas-tic net are constricted to only find one solution, even in the “large p, small n” situation. Tostill make it possible to identify more than one good solution, these methods should be usedin combination with a mechanism that introduces randomness, for example by resampling thetraining data as was done in Chapter 5.8.1 Further improvements in MCMC mixingThe results of the Bayesian variable selection applications discussed in this thesis make it clearthat there is further need for improvement of the sampling algorithms: even if care is taken todevelop an MCMC sampling algorithm, which achieves good Markov chain mixing relative toCPU time, computation time is still a problem for the regular application of Bayesian variableselection methods to high-dimensional data; especially since high-throughput technologies arebecoming ever more advanced and provide more and more measurements per sample. Someimprovement in computing times can be achieved by implementing the MCMC sampling algo-rithms in a programming language that can be compiled, for example in C++. C++ would alsobe well suited for the parallelisation of the parallel tempering algorithm via the MPI (MessagePassing Interface) communications protocol. In an efficient parallel implementation, using par-allel processors, the real computation time would remain nearly the same as for one chain,but with massively improved mixing of the Markov chain of interest. A minor increase incomputation time would results from the time needed for communication between the parallelprocessors, for example to transfer chain states between them.172As demonstrated in Chapter 7, the implementation of a simple parallel tempering algo-rithm can improve the mixing performance dramatically, even without much concern for theoptimisation of the algorithm in terms of the number of parallel chains or the range of tem-peratures applied. Nevertheless, there are more flexible algorithms which can improve Markovchain mixing even further. While parallel tempering only allows two types of moves, the ex-change move (swapping the complete states of two chains) and the mutation move (updatingthe state of one of the chains), evolutionary Monte Carlo algorithms (Liang and Wong 2000)are more general and include for example cross-over moves where only a part of the statesof two chains are proposed for swaps. The equi-energy sampler (Kou et al. 2006) is anotherapproach with the potential to improve Markov chain mixing. Instead of pre-specifying sev-eral temperatures, with which to temper the target distributions of a series of parallel Markovchains, the equi-energy sampler focusses on the density itself. The aim is to propose movesbetween regions of the (multi-modal) posterior distribution with similar values of the density,even if these regions are far apart and separated by areas of very low probability. Anotherpromising methodology is adaptive MCMC sampling. For example, one could implement theparallel tempering algorithm in a flexible way by allowing the temperature ladder to be adaptedthroughout running the Markov chain, attempting to find the best values for the temperaturesduring the run. The difficulty here is to ensure that the Markov chain remains ergodic andstationary despite the constant adaptions.In a final note, we come back to the block MCMC sampler, where the variables, for whichthe indicator γ is to be updated together, are selected based on the dependence structure amongcovariates. There are many ways how this idea can be developed further, some of which havebeen outlined in Chapter 6. Certainly, many possible approaches exist to estimate the corre-lation and partial correlation matrices and it might be useful to compare some of them withrespect to the impact of the choice of estimator on the efficiency of the block sampler. Existingbiological knowledge such as pathway information could be incorporated into the estimationprocedure, like outlined below. For example, in the shrinkage approach described in Chap-ter 6, instead of using a diagonal target matrix, the off-diagonal entries could reflect the priorknowledge about the biological correlation between pairs of genes.1738.2 Incorporating biological knowledgeThe use of proper statistical methodology alone can only provide one half of the solution tothe problem of relating high-dimensional genomic data to clinical reality. Here, biologicalknowledge is needed even more than in classical statistics (where n > p) to help interpret theresults. In the example application in Chapter 7, a very small set of genes was singled outby statistical analysis, where the data provide high confidence for a statistical relation withthe response variable. However, because of the multi-collinear data situation, this does notnecessarily imply that there is a biological relation between the expression levels of these genesand the histological type of the ovarian tumour. Hence, the gene lists provided by statistics canonly serve as a starting point for further exploration of the underlying biology. Partly throughthis process of exploratory statistical analysis and data mining of high-throughput data andsubsequent exploration and interpretation of the findings by expert biologists, more and moredetailed knowledge about the molecular biology of diseases like cancer is accumulating inpublicly available databases.The incorporation of this existing knowledge into the statistical methodology can providefixed points of reference in this haystack of data, in which we are searching for the needles.Bayesian methodology lends itself to this task, through the possibility to include existing in-formation in prior distributions. In the same spirit, it is possible to incorporate informationinto the penalty terms of penalised likelihood methods, as has been attempted in a recent pa-per by Li and Li (2008). An example for existing biological knowledge, which can be usedto guide statistical analysis, are molecular pathways. Information on molecular pathways isavailable for example from the Kyoto Encyclopedia of Genes and Genomes (KEGG) database(Ogata et al. 1999) or from the Gene Ontology classification of genes (The Gene OntologyConsortium 2000). In the KEGG database, known pathways of genes and proteins in the cellare assembled and annotated. This information can be incorporated in statistical models, forexample, by forcing genes, which are known to be involved in the same pathway, to be alwaysselected together for updates in MCMC algorithms to sample from Bayesian variable selectionmodels.174In summary, gene expression microarray data are rather harmless compared to other,newer, high-throughput technologies, where there are millions of simultaneous measurements.Examples are whole-genome SNP chips covering millions of SNPs, or exon arrays providingdata for all known exons in the genome rather than just one expression measurement for eachgene. So the problem of how to deal with such high-dimensional data will not go away, andthe use of regularisation and dimension reduction methodology will remain essential for thestatistical analysis of such data.175BibliographyJ. Albert and S. Chib. Bayesian analysis of binary and polychotomous response data. Journalof the American Statistical Association, 88:669–679, 1993.D.F. Andrews and C.L. Mallows. Scale mixtures of normal distributions. Journal of the RoyalStatistical Society, Series B, 36:99–102, 1974.A. Antoniou, P.D.P. Pharoah, S. Narod, H.A. Risch, J.E. Eyfjord, J.L. Hopper, N. Loman,H. Olsson, O. Johannsson, Å. Borg, B. Pasini, P. Radice, S. Manoukian, D.M. Eccles,N. Tang, E. Olah, H. Anton-Culver, E. Warner, J. Lubinski, J. Gronwald, B. Gorski, J. Tulin-ius, S. Thorlacius, H. Eerola, H. Nevanlinna, K. Syrjäkoski, O.-P. Kallioniemi, D. Thompson,C. Evans, J. Peto, F. Lalloo, D.G. Evans, and D.F. Easton. Average risks of breast and ovariancancer associated with BRCA1 or BRCA2 mutations detected in case series unselected forfamily history: A combined analysis of 22 studies. American Journal of Human Genetics,72(5):1117–1130, 2003.K. Bae and B.K. Mallick. Gene selection using a two-level hierarchical Bayesian model. Bioin-formatics, 20:3423–3430, 2004.Y. Benjamini and Y Hochberg. Controlling the false discovery rate: a practical and powerfulapproach to multiple testing. Journal of the Royal Statistical Society, Series B, 57:289–300,1995.A. Björkström and R. Sundberg. Continuum regression is not always continuous. Journal ofthe Royal Statistical Society, Series B, 58:703–710, 1996.A. Björkström and R. Sundberg. A generalized view on continuum regression. ScandinavianJournal of Statistics, 26:17–30, 1999.176M. Blangiardo and S. Richardson. Statistical tools for synthesizing lists of differentially ex-pressed features in related experiments. Genome Biology, 8:R54, 2007.L. Bottolo and S. Richardson. Fully Bayesian variable selection using g-priors, 2007. TechnicalReport, Imperial College London, submitted.L. Bottolo and S. Richardson. Evolutionary stochastic search, 2008. Technical Report, ImperialCollege London, submitted.H.M. Bøvelstad, S. Nygård, H.L. Størvold, M. Aldrin, Ø. Borgan, A. Frigessi, and O.C.Lingjærde. Predicting survival from microarray data - a comparative study. Bioinformat-ics, 23(16):2080–2087, 2007.L. Breiman. Random forests. Machine learning, 45:5–32, 2001.L. Breiman. Better subset regression using the nonnegative Garrote. Technometrics, 37:373–384, 1995.L. Breiman. Arcing classifiers (with discussion). The Annals of Statistics, 26:801–849, 1998.P.J. Brown. Measurement, Regression, and Calibration. Clarendon Press, Oxford, 1993.P.J. Brown, M. Vannucci, and T. Fearn. Multivariate Bayesian variable selection and prediction.Journal of the Royal Statistical Society, Series B, 60(3):627–641, 1998a.P.J. Brown, M. Vannucci, and T. Fearn. Bayesian wavelength selection in multicomponentanalysis. Journal of Chemometrics, 12:173–182, 1998b.P.J. Brown, M. Vannucci, and T. Fearn. Bayes model averaging with selection of regressors.Journal of the Royal Statistical Society, Series B, 64(3):519–536, 2002.P. Bühlmann and T. Hothorn. Boosting algorithms: regularization, prediction and model fitting(with discussion). Statistical Science, 22:477–522, 2007.L. Bullinger, K. Döhner, E. Bair, S. Fröhling, R.F. Schlenk, R. Tibshirani, H. Döhner, and J.R.Pollack. Use of gene-expression profiling to identify prognostic subclasses in adult acutemyeloid leukemia. New England Journal of Medicine, 350(16):1605–1616, 2004.177N.A. Butler and M.C. Denham. The peculiar shrinkage properties of partial least squares re-gression. Journal of the Royal Statistical Society, Series B, 62(3):585–593, 2000.F. Calvo. All-exchanges parallel tempering. The Journal of Chemical Physics, 123:124106,2005.J.M. Chambers. Regression updating. Journal of the American Statistical Association, 66:744–748, 1971.H. Chipman, E.I. George, and R.E. McCulloch. The practical implementation of Bayesianmodel selection (with discussion). IMS Lecture Notes - Monograph Series, 38:67–134, 2001.W.S. Cleveland. Robust locally weighted regression and smoothing scatterplots. Journal of theAmerican Statistical Association, 74:829–836, 1979.M. Clyde. Bayesian model averaging and model search strategies. In J.M. Bernardo, J.O.Berger, A.P. Dawid, and A.F.M. Smith, editors, Bayesian Statistics 6, pages 157–185. OxfordUniversity Press, Oxford, 1999.C.A. Davis, F. Gerick, V. Hintermair, C.C. Friedel, K. Fundel, R. Küffner, and R. Zimmer. Re-liable gene signatures for microarray classification: assessment of stability and performance.Bioinformatics, 22(19):2356–2363, 2006.L. Devroye. Non-uniform random variate generation. Springer, New York, 1986.R. Díaz-Uriarte and S. Alvarez de Andrés. Gene selection and classification of microarray datausing random forest. BMC Bioinformatics, 7:3, 2006.L.R. Dice. Measures of the amount of ecological association between species. Ecology, 26:297–302, 1945.T.J. DiCiccio, R.E. Kass, A. Raftery, and L. Wasserman. Computing Bayes factors by combin-ing simulation and asymptotic approximations. Journal of the American Statistical Associa-tion, 92:903–915, 1997.A. Dobra, C. Hans, B. Jones, J.R. Nevins, and M. West. Sparse graphical models for exploringgene expression data. Journal of Multivariate Analysis, 90:196–212, 2004.178S. Dudoit, J. Fridlyand, and T.P. Speed. Comparison of discrimination methods for the classi-fication of tumours using gene expression data. Journal of the American Statistical Associa-tion, 97:77–87, 2002.A. Dupuy and R. Simon. Critical review of published microarray studies for cancer outcomeand guidelines on statistical analysis and reporting. Journal of the National Cancer Institute,99(2):147–157, 2007.B. Efron. Biased versus unbiased estimation. Advances in Mathematics, 16:259–277, 1975.B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression (with discussion).The Annals of Statistics, 32:407–499, 2004.L. Ein-Dor, I. Kela, G. Getz, D. Givol, and E. Domany. Outcome signature genes in breastcancer: is there a unique set? Bioinformatics, 21:171–178, 2005.L. Ein-Dor, O. Zuk, and E. Domany. Thousands of samples are needed to generate a robustgene list for predicting outcome in cancer. Proceedings of the National Academy of Sciencesof the United States of America, 103:5923–5928, 2006.J. Fan and R.Z. Li. Variable selection via nonconcave penalized likelihood and its oracle prop-erties. Journal of the American Statistical Association, 96:1348–1360, 2001.M.A.T. Figueiredo. Adaptive sparseness for supervised learning. IEEE Transactions on PatternAnalysis and Machine Intelligence, 25:1150–1159, 2003.M.A.T Figueiredo and A.K. Jain. Bayesian learning of sparse classifiers. Proceedings IEEEcomputer Society Conference in Computer Vision and Pattern Recognition, 1:35–41, 2001.I.E. Frank and J.H. Friedman. A statistical view of some chemometrics regression tools. Tech-nometrics, 35(2):109–135, 1993.Y. Freund. Boosting a weak learning algorithm by majority. Information and Computation,121:256–285, 1995.Y. Freund and R. Schapire. Experiments with a new boosting algorithm. In Proceedings of theThirteenth International Conference on Machine Learning. Morgan Kaufmann PublishersInc., San Francisco, 1996.179J. Friedman and B.E. Popescu. Gradient directed regularization for linear regression and clas-sification. Technical Report, Stanford University,http://www-stat.stanford.edu/∼jhf/ftp/path.pdf, 2004.J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view ofboosting. The Annals of Statistics, 28:337–407, 2000.J. Friedman, T. Hastie, H. Höfling, and R. Tibshirani. Pathwise coordinate optimization. TheAnnals of Applied Statistics, 1:302–332, 2007.W. Fu. Penalized regression: the bridge versus the lasso. Journal of Computational and Graph-ical Statistics, 7:397–416, 1998.A.E. Gelfand and A.F.M. Smith. Sampling based approaches to calculating marginal densities.Journal of the American Statistical Association, 85:398–409, 1990.A. Gelman and D.B. Rubin. Inference from iterative simulation using multiple sequences.Statistical Science, 7:457–511, 1992.A. Gelman, J.B. Carlin, H.S. Stern, and D.B. Rubin. Bayesian Data Analysis. Chapman andHall, London, 1995.A. Gelman, G.O. Roberts, and W.R. Gilks. Efficient Metropolis jumping rules. In J.M.Bernardo, J.O. Berger, A.P. Dawid, and A.F.M. Smith, editors, Bayesian Statistics 5, pages599–607. Oxford University Press, Oxford, 1996.S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions and the Bayesian restora-tion of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741,1984.A. Genkin, D.D. Lewis, and D. Madigan. Large-scale Bayesian logistic regression for textcategorization. Technometrics, 49:291–304, 2007.E.G. George and R.E. McCulloch. Variable selection via Gibbs sampling. Journal of theAmerican Statistical Association, 88:881–889, 1993.E.I. George and R.E. McCulloch. Approaches for Bayesian variable selection. Statistica Sinica,7:339–373, 1997.180J. Geweke. Bayesian inference in econometric models using Monte Carlo integration. Econo-metrica, 57:1317–1339, 1989.C.J. Geyer. Markov Chain Monte Carlo Maximum Likelihood. In E.M. Keramidas, editor,Computing Science and Statistics: Proceedings of the 23rd Symposium on the Interface.Interface Foundation, Fairfax Station, 1991.C.J. Geyer and E.A. Thompson. Annealing Markov chain Monte Carlo with applications toancestral inference. Journal of the American Statistical Association, 90:909–920, 1995.W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. Markov Chain Monte Carlo in Practice.Chapman and Hall, London, 1996.P.E. Gill, W. Murray, and M. Wright. Practical Optimization. Academic Press, London, 1981.J. Goeman. An efficient algorithm for L1-penalized estimation. Submitted, 2008. URLhttp://www.msbi.nl/dnn/Default.aspx?tabid=202.T.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M.L.Loh, J.R. Downing, M.A. Caliguri, C.D. Bloomfield, and E.S. Lander. Molecular classifica-tion of cancer: Class discovery and class prediction by gene expression monitoring. Science,286:531–537, 1999.G. Goswami and J.S. Liu. On learning strategies for evolutionary Monte Carlo. Statistics andComputing, 17:23–38, 2007.C. Goutis. Partial least squares yields shrinkage estimators. Annals of Statistics, 24:816–824,1996.J.E. Griffin and P.J. Brown. Alternative prior distributions for variable selection withvery many more variables than observations. CRiSM Working Paper No. 05-10, Centre for Research in Statistical Methodology, University of Warwick, URLhttp://www2.warwick.ac.uk/fac/sci/statistics/crism/research/2005/paper05-10, 2005.J.E. Griffin and P.J. Brown. Bayesian adaptive lassos with non-convex penalization. CRiSMWorking Paper No. 07-02, Centre for Research in Statistical Methodology, Univer-181sity of Warwick, URL http://www2.warwick.ac.uk/fac/sci/statistics/crism/research/2007/paper07-2, 2007.D. Grimwade and T. Haferlach. Gene-expression profiling in Acute Myeloid Leukemia. NewEngland Journal of Medicine, 350(16):1676–1678, 2004.I.D. Guerreiro Da Silva, Y.F. Hu, I.H. Russo, X. Ao, A.M. Salicioni, X. Yang, and Russo J.S100P calcium-binding protein overexpression is associated with immortalization of humanbreast epithelial cells in vitro and early stages of breast cancer development in vivo. Inter-national Journal of Oncology, 16(2):231–240, 2000.C. Hans, A. Dobra, and M. West. Shotgun stochastic search for “large p” regression. Journalof the American Statistical Association, 102:507–516, 2007.T. Hastie, R. Tibshirani, M. Eisen, A. Alizadeh, R. Levy, L. Staudt, W. Chan, D. Botstein, andP. Brown. ’Gene shaving’ as a method for identifying distinct sets of genes with similarexpression patterns. Genome Biology, 1:3.1–3.21, 2000.T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning: Data mining,inference, and prediction. Springer, New York, 2001.W.K. Hastings. Monte Carlo sampling methods using Markov chains and their applications.Biometrika, 57:97–109, 1970.J.E. Hazel. Binary coefficients and clustering in stratigraphy. Geological Society of AmericaBulletin, 81(11):3237–3252, 1970.I. Hedenfalk, D. Duggan, Y. Chen, M. Radmacher, M. Bittner, R. Simon, P. Meltzer, B. Guster-son, M. Esteller, OP. Kallioniemi, B. Wilfond, A. Borg, and J. Trent. Gene-expression pro-files in hereditary breast cancer. New England Journal of Medicine, 344:539–48, 2001.D.M. Higdon. Auxiliary variable methods for Markov chain Monte Carlo with application.Journal of the American Statistical Association, 93:585–595, 1998.A.E. Hoerl and R.W. Kennard. Ridge regression: Biased estimation for nonorthogonal prob-lems. Technometrics, 12:55–67, 1970.182C.C. Holmes and L. Held. Bayesian auxiliary variable models for binary and multinomialregression. Bayesian Analysis, 1:145–168, 2006.K. Hukushima and K. Nemoto. Exchange Monte Carlo method and application to spin glasssimulations. Journal of the Physical Society of Japan, 65:1604–1608, 1996.Y. Iba. Extended Ensemble Monte Carlo. International Journal of Modern Physics C, 12:623–656, 2001.R.A. Irizarry, B. Hobbs, F. Collin, Y.D. Beazer-Barclay, K.J. Antonellis, U. Scherf, and T.P.Speed. Exploration, normalization, and summaries of high density oligonucleotide arrayprobe level data. Biostatistics, 4:249–264, 2003.E. Ising. Beitrag zur Theorie des Ferromagnetismus. Zeitschrift für Physik, 31:253–253, 1925.P. Jaccard. Étude comparative de la distribution florale dans une portion des Alpes et des Jura.Bulletin de la Société Vaudoise des Sciences Naturelles, 37:547–579, 1901.S. Janson and J. Vegelius. Measures of ecological association. Oecologia, 49:371–376, 1981.A. Jasra, D. Stephens, and C.C. Holmes. Population-based reversible jump Markov chainMonte Carlo. Biometrika, 94:787–807, 2007.R.E. Kass and A.E. Raftery. Bayes factors and model uncertainty. Journal of the AmericanStatistical Association, 90:773–795, 1995.R.E. Kass, B.P. Carlin, A. Gelman, and R.M. Neal. Markov Chain Monte Carlo in practice: Aroundtable discussion. The American Statistician, 52:93–100, 1998.S. Kirkpatrick, C.D. Gelatt, and M.P. Vecchi. Optimization by simulated annealing. Science,220:671–680, 1983.K. Knight and W. Fu. Asymptotics for lasso-type estimators. Annals of Statistics, 28:1356–1378, 2000.H. Knüpfer, R. Schmidt, D. Stanitz, M. Brauckhoff, M. Schönfelder, and R. Preiss. CYP2Cand IL-6 expression in breast cancer. The Breast, 13(1):28–34, 2004.183D.A. Kofke. On the acceptance probability of replica-exchange Monte Carlo trials. The Journalof Chemical Physics, 117:6911–6914, 2002.R. Kohn, M. Smith, and D. Chan. Nonparametric regression using linear combinations of basisfunctions. Statistics and Computing, 11:313–322, 2001.A. Kone and D.A. Kofke. Selection of temperature intervals for parallel-tempering simulations.The Journal of Chemical Physics, 122:206101, 2005.S.C. Kou, Q. Zhou, and W.H. Wong. Equi-energy sampler with applications in statistical infer-ence and statistical mechanics. Annals of Statistics, 34:1581–1619, 2006.C. Lai, M.J.T. Reinders, L.J. van’t Veer, and L.F.A. Wessels. A comparison of univariate andmultivariate gene selection techniques for classification of cancer datasets. BMC Bioinfor-matics, 7:235, 2006.O. Ledoit and M. Wolf. Improved estimation of the covariance matrix of stock returns with anapplication to portfolio selection. Journal of Empirical Finance, 10:603–621, 2003.K.E. Lee, N. Sha, E.R. Dougherty, M. Vannucci, and B.K. Mallick. Gene selection: a Bayesianvariable selection approach. Bioinformatics, 19(1):90–97, 2003.C. Li and H. Li. Network-constrained regularization and variable selection for analysis ofgenomic data. Bioinformatics, 24:1175–1182, 2008.F. Liang and W.H. Wong. Evolutionary Monte Carlo: application to cp model sampling andchange point problem. Statistica Sinica, 10:317–342, 2000.D.V. Lindley and A.F.M. Smith. Bayes estimates for the linear model. Journal of the RoyalStatistical Society, Series B, 34:1–41, 1972.E.T. Liu and K.R. Karuturi. Microarrays and clinical investigations. New England Journal ofMedicine, 350(16):1595–1597, 2004.J.S. Liu. Monte Carlo strategies in scientific computing. Springer, New York, 2001.K.H. Lu, A.P. Patterson, L. Wang, R.T. Marquez, E.N. Atkinson, K.A. Baggerly, L.R. Ramoth,D.G. Rosen, J. Liu, I. Hellstrom, D. Smith, L. Hartmann, D. Fishman, A. Berchuck,184R. Schmandt, R. Whitaker, D.M. Gershenson, G.B. Mills, and Jr. R.C. Bast. Selection ofpotential markers for epithelial ovarian cancer with gene expression arrays and recursivedescent partition analysis. Clinical Cancer Research, 10:3291–3300, 2004.S. Ma, X. Song, and J. Huang. Regularized binormal ROC method in disease classificationusing microarray data. BMC Bioinformatics, 7:253, 2006.D. Madigan and J. York. Bayesian graphical models for discrete data. International StatisticalReview, 63:215–232, 1995.B.K. Mallick, D. Ghosh, and M. Ghosh. Bayesian classification of tumours by using geneexpression data. Journal of the Royal Statistical Society, Series B, 67:219–234, 2005.E. Marinari and G. Parisi. Simulated tempering: A new Monte Carlo scheme. EurophysicsLetters, 19:451–458, 1992.W.F. Massy. Principal components regression in exploratory statistical research. Journal of theAmerican Statistical Association, 60:234–256, 1965.P. McCullach and J.A. Nelder. Generalized Linear Models. Chapman and Hall, London, 1989.N. Meinshausen and P. Bühlmann. High dimensional graphs and variable selection with thelasso. Annals of Statistics, 34:1436–1462, 2006.N. Metropolis, A.W. Rosenbluth, M.N. Rosenbluth, A.H. Teller, and E. Teller. Equations ofstate calculations by fast computing machines. Journal of Chemical Physics, 21:1087–1091,1953.S. Michiels, S. Koscielny, and C. Hill. Prediction of cancer outcome with microarrays: amultiple random validation strategy. The Lancet, 365:488–492, 2005.T.J. Mitchell and J.J. Beauchamp. Bayesian variable selection in linear regression. Journal ofthe American Statistical Association, 83:1023–1032, 1988.F. Modugno, C. Knoll, A. Kanbour-Shakir, and M. Romkes. A potential role for the estrogen-metabolizing cytochrome P450 enzymes in human breast carcinogenesis. Breast CancerResearch Treatment, 82(3):191–197, 2003.185R. Jr. Morgan, R.D. Alvarez, D.K. Armstrong, L. Copeland, J. Fiorica, D.A. Fishman, J. Fowler,D. Gershenson, B.E. Greer, C. Johnston, A. Kessinger, S. Lele, G. Locker, U. Matulonis, R.F.Ozols, R. Sabbatini, and N. Teng. NCCN practice guidelines for ovarian cancer. NationalComprehensive Cancer Network, Complete Library of NCCN Oncology Practice Guidelines,CD-ROM Version, 2000.R.M. Neal. Probabilistic inference using Markov chain Monte-Carlo Methods, 1993. URLhttp://www.cs.toronto.edu/∼radford/ftp/review.pdf. Technical Re-port, CRG-TR-93-1, Department of Computer Science, University of Toronto.D.J. Nott and P.J. Green. Bayesian variable selection and the Swendsen-Wang algorithm. Jour-nal of Computational and Graphical Statistics, 13:1–17, 2004.A. Ochiai. Zoogeographical studies on the soleoid fishes found in Japan and its neighbouringregions. Bulletin of the Japanese Society of Scientific Fisheries, 22:526–530, 1957.Office for National Statistics. National Statistics Series DH2 no.32, Mortality statis-tics: cause, 2006. URL http://www.statistics.gov.uk/downloads/theme_health/Dh2_32/DH2_No32_2005.pdf, ISBN 1-85774-641-4.H. Ogata, S. Goto, K. Sato, W. Fujibuchi, H. Bono, and M. Kanehisa. KEGG: Kyoto en-cyclopedia of genes and genomes. Nucleic Acids Research, 27(1):29–34, 1999. URLhttp://www.genome.jp/kegg.K. Ohuchida, K. Mizumoto, T. Egami, H. Yamaguchi, K. Fujii, H. Konomi, E. Nagai, M. Yam-aguchi, K. Tsuneyoshi, and M. Tanaka. S100P is an early developmental marker of pancre-atic carcinogenesis. Clinical Cancer Research, 12(18):5411–5416, 2006.A. Paju and U.-H. Stenman. Biochemistry and clinical role of trypsinogens and pancreaticsecretory trypsin inhibitor. Critical Reviews in Clinical Laboratory Sciences, 43(2):103–142, 2006.J. Pittman, E. Huang, H. Dressman, C.-F. Horng, S.H. Cheng, M.-H. Tsou, C.-M. Chen, A. Bild,E.S. Iversen, A.T. Huang, J.R. Nevins, and M. West. Integrated modeling of clinical and geneexpression information for personalized prediction of disease outcomes. Proceedings of theNational Academy of Sciences USA, 101(22):8431–8436, 2004.186M. Plummer, N. Best, K. Cowles, and K. Vines. CODA: Convergence di-agnosis and output analysis for MCMC. R News, 6:7–11, 2006. URLhttp://CRAN.R-project.org/doc/Rnews/Rnews_2006-1.pdf.C. Predescu, M. Predescu, and C.V. Ciobanu. On the efficiency of exchange in parallel tem-pering Monte Carlo simulations. Journal of Physical Chemistry, Series B, 109:4189–4196,2005.R Development Core Team. R: A Language and Environment for Statistical Com-puting. R Foundation for Statistical Computing, Vienna, Austria, 2006. URLhttp://www.R-project.org. ISBN 3-900051-07-0.A.E. Raftery, D. Madigan, and J.A. Hoeting. Bayesian model averaging for linear regressionmodels. Journal of the American Statistical Association, 92:179–191, 1997.N. Rathore, M. Chopra, and J.J. de Pablo. Optimal allocation of replicas in parallel temperingsimulations. The Journal of Chemical Physics, 122:024111, 2005.P. Roepman, P. Kemmeren, L.F.A. Wessels, P.J. Slootweg, and F.C.P. Holstege. Multiple robustsignatures for detecting lymph node metastasis in head and neck cancer. Cancer Research,66(4):2361–2366, 2006.J. Schäfer and K. Strimmer. A shrinkage approach to large-scale covariance estimation andimplications for functional genomics. Statistical Applications in Genetics and MolecularBiology, 4(1):32, 2005.R. Schapire. The strength of weak learnability. Machine Learning, 5:197–227, 1990.J.R. Schott. Matrix analysis for Statistics. Wiley, New York, 1997.D.R. Schwartz, S.L.R. Kardia, K.A. Shedden, R. Kuick, G. Michailidis, J.M.G. Taylor, D.E.Misek, R. Wu, Y. Zhai, D.M. Darrah, H. Reed, L.H. Ellenson, T.J. Giordano, E.R. Fearon,S.M. Hanash, and K.R. Cho. Gene expression in ovarian cancer reflects both morphology andbiological behavior, distinguishing clear cell from other poor-prognosis ovarian carcinomas.Cancer Research, 62:4722–4729, 2002.J.J. Sepkoski Jr. Quantified coefficients of association and measurement of similarity. Mathe-matical Geology, 6(2):135–152, 1974.187N. Sha, M. Vannucci, M.G. Tadesse, P.J. Brown, I. Dragoni, N. Davies, T.C. Roberts, A. Con-testabile, N. Salmon, C. Buckley, and F. Falciani. Bayesian variable selection in multinomialprobit models to identify molecular signatures of disease stage. Biometrics, 60:812–819,2004.S.K. Shevade and S.S. Keerthi. A simple and efficient algorithm for gene selection using sparselogistic regression. Bioinformatics, 19:2246–2253, 2003.R. Simon. Development and evaluation of therapeutically relevant predictive classifiers us-ing gene expression profiling. Journal of the National Cancer Institute, 98(17):1169–1171,2006.R. Simon, M.D. Radmacher, K. Dobbin, and L.M. McShane. Pitfalls in the use of DNA mi-croarray data for diagnostic and prognostic classification. Journal of the National CancerInstitute, 95(1):14–18, 2003.G.G. Simpson. Notes on the measurement of faunal resemblance. American Journal of Science,258A:300–311, 1960.D. Singh, P.G. Febbo, K. Ross, D.G. Jackson, J. Manola, C. Ladd, P. Tamayo, A.A. Renshaw,A.V. D’Amico, J.P. Richie, E.S. Lander, M. Loda, P.W. Kantoff, T.R. Golub, and W.R. Sell-ers. Gene expression correlates of clinical prostate cancer behavior. Cancer Cell, 1:203–209,2002.M. Smith and R. Kohn. Nonparametric regression using Bayesian variable selection. Journalof Econometrics, 75:317–343, 1996.R.R. Sokal and P.H.A. Sneath. Numerical Taxonomy: The Principles and Practice of NumericalClassification. W. H. Freeman, San Francisco, 1973.R.L. Somorjai, B. Dolenko, and R. Baumgartner. Class prediction and discovery using gene mi-croarray and proteomics mass spectroscopy data: curses, caveats, cautions. Bioinformatics,19(12):1484–1491, 2003.R. Spang, H. Zuzan, M. West, J.R. Nevins, C. Blanchette, and J.R. Marks. Prediction anduncertainty in the analysis of gene expression profiles. In Silico Biology, 2:0033, 2001. URLhttp://www.bioinfo.de/isb/2002/02/0033.188C. Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal dis-tribution. In Proceedings of the 3rd Berkeley Symposium on Mathematical Statistics andProbability, volume 1, pages 197–206. Berkeley: University of California Press, 1956.M. Stone and R.J. Brooks. Continuum regression: Cross-validated sequentially constructedprediction embracing ordinary least squares, partial least squares and principal componentsregression (with discussion). Journal of the Royal Statistical Society, Series B, 52:237–269,1990.J.D. Storey. A direct approach to false discovery rates. Journal of the Royal Statistical Society,Series B, 64:479–498, 2002.T. Strachan and A.P. Read. Human Molecular Genetics. Garland Science, New York, 3rdedition, 2004.R. Sundberg. Continuum regression and ridge regression. Journal of the Royal StatisticalSociety, Series B, 55:653–659, 1993.R.H. Swendsen and J.S. Wang. Nonuniversal critical dynamics in Monte Carlo simulations.Physical Review Letters, 58:86–88, 1987.M.G. Tadesse, N. Sha, and M. Vannucci. Bayesian variable selection in clustering high-dimensional data. Journal of the American Statistical Association, 100:602–617, 2005.The Gene Ontology Consortium. Gene ontology: tool for the unification of biology. NatureGenetics, 25:25–29, 2000.The MathWorks. Matlab 7.3 (R2006b). The MathWorks, Inc., Natick, Massachusetts, USA,2006. URL http://www.mathworks.com.R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal StatisticalSociety, Series B, 58:267–288, 1996.L. Tierney. Markov chains for exploring posterior distributions (with discussion). Annals ofStatistics, 22:1674–1762, 1994.M.E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of MachineLearning Research, 1:211–244, 2001.189U. Turpeinen, E. Koivunen, and U.-H. Stenman. Reaction of a tumour-associated trypsin in-hibitor with serine proteinases associated with coagulation and tumour invasion. Biochem-istry Journal, 254:911–914, 1988.P.J.M. Valk, R.G.W. Verhaak, M.A. Beijen, C.A.J. Erpelinck, S.B. van Waalwijk van Doorn-Khosrovani, Boer J.M., H. Berna Beverloo, M.J. Moorhouse, P.J. van der Spek, B. Löwen-berg, and R. Delwel. Prognostically useful gene-expression profiles in acute myeloidleukemia. New England Journal of Medicine, 350(16):617–628, 2004.S. van de Geer. High-dimensional generalized linear models and the lasso. Annals of Statistics,36:614–645, 2008.M.J. van de Vijver, Y.D. He, L. van’t Veer, H. Dai, A.A.M. Hart, D.W. Voskuil, G.J. Schreiber,J.L. Peterse, C. Roberts, M.J. Marton, M. Parrish, D. Atsam, A. Witteven, A. Glas, L. De-lahaye, van der Valder T., H. Bartelink, S. Rodenhuis, E.T. Rutgers, S.H. Friend, andR. Bernards. A gene-expression signature as a predictor of survival in breast cancer. TheNew England Journal of Medicine, 347:1999–2009, 2002.L.J. van’t Veer, H. Dai, M.J. van de Vijver, Y.D. He, A.A.M. Hart, M. Mao, H.L. Peterse,K. van der Kooy, M.J. Marton, A.T. Witteveen, G.J. Schreiber, R.M. Kerkhoven, C. Roberts,P.S. Linsley, R. Bernards, and S.H. Friend. Gene expression profiling predicts clinical out-come of breast cancer. Nature, 415:530–536, 2002.V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.M. West. Bayesian factor regression models in the “large p, small n” paradigm. In J.M.Bernardo, M.J. Bayarri, J.O. Berger, A.P. Dawid, D. Heckerman, A.F.M. Smith, and M. West,editors, Bayesian Statistics 7, pages 733–742. Oxford University Press, Oxford, 2003.M. West, C. Blanchette, H. Dressman, E. Huang, S. Ishida, R. Spang, H. Zuzan, J.A. Olson Jr.,J.R. Marks, and J.R. Nevins. Predicting the clinical status of human breast cancer by usinggene expression profiles. Proceedings of the National Academy of Sciences USA, 98:11462–11467, 2001.J. Whittaker. Graphical Models in Applied Multivariate Statistics. Wiley, Chichester, 1990.190E. Wit and J. McClure. Statistics for Microarrays: Design, Analysis and Inference. John Wileyand Sons, Chichester, 2004.H. Wold. Soft modeling by latent variables: The nonlinear iterative partial least squares (NI-PALS) approach. In J. Gani, editor, Perspectives in Probability and Statistics, papers inHonor of M.S. Bartlett, pages 520–540. Academic Press, London, 1975.A. Zellner. On assessing prior distributions and Bayesian regression analysis with g prior dis-tributions. In P.K. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques- Essays in Honor of Bruno de Finetti, pages 233–243. 1986.C.-H. Zhang and J. Huang. The sparsity and bias of the lasso selection in high-dimensionallinear regression. Annals of Statistics. To appear, 2008.T. Zhang and F. Oles. Text categorization based on regularized linear classifiers. InformationRetrieval, 4:5–31, 2001.P. Zhao and B. Yu. On model selection consistency of lasso. Journal of Machine LearningResearch, 7:2541–2567, 2006.H. Zou. The adaptive lasso and its oracle properties. Journal of the American StatisticalAssociation, 101:1418–1429, 2006.H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of theRoyal Statistical Society, Series B, 67(2):301–320, 2005.M. Zucknick, S. Richardson, and E.A. Stronach. Comparing the characteristics of gene ex-pression profiles derived by univariate and multivariate classification methods. StatisticalApplications in Genetics and Molecular Biology, 7(1):1–32, 2008.191Appendix ABayesian methodologyA.1 Bayesian modellingIn Bayesian data analysis, a full probability model is set up, that is a joint probability distribu-tion for all observable and unobservable quantities. This is done in order to gain informationabout the unobservable quantities - the population parameters of interest - denoted by θ, bymeans of the observed quantities, i.e. the data, denoted by Z = (X, y). These notes onBayesian modelling are based on Gelman et al. (1995), where more detailed information canbe found. The simplest and most prominent probability model is a linear regression model,where one assumes that the variability in the response variable y can be explained by a linearcombination of the explanatory variables X = (xj)nj=1, one is hence interested in conditionalinference of y given X . The goal of an analysis is to infer the weights in the linear combination,which is called the regression coefficient vector θ = β = (βi)pi=1. A vector ε representing ran-dom error is assumed to be distributed according to a Gaussian distribution with mean vector 0and known identical variances σ2 for all components εj:y = Xβ + ε, with ε ∼ N(0, σ2In). (A.1)A crucial assumption for Bayesian modelling is that the observable quantities Z are ex-changeable, meaning that the joint probability distribution p(Z) is invariant to permutations ofthe indices j = 1, ..., n. A sufficient, but not necessary, condition for exchangeability is thatthe model data Z are independently and identically distributed (iid) given the unknown modelparameters θ. This is reflected in the linear regression model (A.1) where it is assumed that192the errors εj are independently and identically distributed, which implies the same feature fory given X .The joint probability model for the parameters θ and the data Z makes inference about θpossible by applying Bayes’ rule. One can partition the joint probability density in the follow-ing way:p(θ, Z) = p(θ)p(Z|θ). (A.2)The density p(θ) is called the prior density of the parameter vector θ; it can be subjective, i.e.convey prior knowledge about the parameter of interest, or it can be an objective prior, trying toallow for all possible posterior outcomes. The density p(Z|θ) is known as the sampling densityor likelihood of the data given θ; it expresses how likely the observed data are given a valueof θ. In classical frequentist statistics, one often performs inference on the parameter θ solelyin terms of the likelihood: the maximum-likelihood estimate of θ is the parameter value underwhich the data were most likely to attain the observed quantities.Bayesian inference focusses on the posterior density for the parameter vector given theobserved data instead Following from equation (A.2), the posterior density can be derived asp(θ|Z) = p(θ, Z)p(Z)=p(θ)p(Z|θ)p(Z). (A.3)It is often infeasible to infer p(Z); in such situations only the unnormalised posterior density isknown, i.e. the right-hand side ofp(θ|Z) ∝ p(θ)p(Z|θ). (A.4)Note that in a regression analysis one is purely interested in conditional modelling of the re-sponse variable y given the explanatory variables X . In this case, equation (A.4) takes theformp(θ|X, y) ∝ p(θ)p(y|X, θ). (A.5)In a Bayesian linear regression model it is convenient to assume a normal prior distributionfor the regression coefficients β, i.e.β ∼ N(b, v), (A.6)with mean vector b and prior covariance matrix v. This is a conjugate prior distribution for β inthe linear regression model (A.1). Lindley and Smith (1972) show that the posterior distribution193is also a normal distribution with mean vector B and posterior covariance matrix Vβ|Z, λ ∼ N(B, V ) (A.7)B = V (v−1b + XT y)V = (v−1 + XT X)−1.Conjugacy is an important concept in Bayesian modelling, because conjugate models arecomputationally convenient and improve the interpretability of the results. A prior distributionp(θ) is called conjugate for a sampling distribution p(Z|θ), if the corresponding posterior dis-tribution will come from the same class of distributions as the prior. For example, if a priordistribution is Gaussian, and the sampling distribution is also Gaussian, then the prior is conju-gate, because the posterior distribution is going to be Gaussian as well.In a linear regression model, one of two generic forms for the prior covariance matrix vof the regression coefficient vector β is commonly used, mostly for computational reasons butalso because of their interpretability. One form is the independence prior covariance v = c2Ip,assuming a priori that the regression coefficients are independent. The other commonly usedprior covariance matrix is the data-dependent g prior v = c2(XT X)−1 (Zellner 1986), whichuses the empirical covariance matrix of the explanatory data X as a priori information aboutthe regression coefficient covariance matrix.A.2 Markov chain Monte Carlo samplingA Markov chain with invariant distribution p is a sequence of random variables {Xn} withtransition kernel P (Xn, A) = P{Xn+1 ∈ A|X0, ..., Xn} which defines the probabilities ofmoving to a new state conditional on the previous state, where the Markov chain distributionp is given by p(A) =∫p(dX)P (X, A) for all P -measurable sets A (e.g. Tierney 1994). Theconditional distribution of any Xn can be derived from the initial distribution of the chain:P{Xn ∈ A|X0} = P n(X0, A), where P n is the nth iterate of the kernel P . The Markov chainconverges to the invariant distribution p:limn→∞P n(X,A) = p(A) for p-almost all X. (A.8)It is property (A.8) which makes Markov chains suitable for exploring a probability distributionp which cannot be evaluated analytically, if it can be shown to be the invariant distribution of a194Markov chain with transition kernel P . If this is the case, one can simply draw samples fromrandom variables {Xn}, which are assumed to have probability distribution p after an initialburn-in period. To ensure that the Markov samples really do come from the distribution p (inconvergence), the Markov chain has to be irreducible, i.e. it has positive probability to move toany set A for which p has positive probability. The Markov chain also needs to be aperiodic,that is there may be no portions of the state space which can only be visited at certain periodictimes. If a Markov chain fulfills these two properties and has a proper invariant distributionp, then p is the unique invariant distribution and is the equilibrium distribution of the chain(Tierney 1994).There are many approaches for constructing Markov chains with a specified invariant dis-tribution p, the two most popular techniques being the Gibbs and Metropolis-Hastings sam-plers. the Metropolis-Hastings sampler is a generalisation of the Metropolis algorithm byMetropolis et al. (1953), which was first presented by Hastings (1970). The Metropolis algo-rithm samples from the target distribution p(θ) by drawing samples from a proposal distributionq(θ) which are either accepted or rejected according to an acceptance probability α. The pro-posal distribution has to be symmetric, i.e. q(θ∗|θ) = q(θ|θ∗) for all θ and θ∗. The algorithmproceeds as follows (e.g. Gelman et al. 1995):1. For iteration t = 0: draw a starting point θ0, for which the target distribution has positiveprobability mass, i.e. p(θ0) > 0.2. For iterations t = 1, ..., T :(a) Sample a proposal point θ∗ from a symmetric proposal distribution which might beconditional on the previous iteration t− 1 q(θ∗|θt−1).(b) Calculate the acceptance probabilityα = min{1,p(θ∗)p(θt−1)}. (A.9)(c) Setθt =θ∗ with probability αθt−1 otherwise.The Metropolis-Hastings algorithm is very similar to the Metropolis sampler, except that itdoes not require the proposal distribution to be symmetric:1951. For iteration t = 0: draw a starting point θ0, for which the target distribution has positiveprobability mass, i.e. p(θ0) > 0.2. For iterations t = 1, ..., T :(a) Sample a proposal point θ∗ from a proposal distribution which might be conditionalon the previous iteration t− 1 q(θ∗|θt−1).(b) Calculate the acceptance probabilityα = min{1,p(θ∗)p(θt−1)q(θt−1|θ∗)q(θ∗|θt−1)}. (A.10)(c) Setθt =θ∗ with probability αθt−1 otherwise.The Gibbs sampler was introduced by Geman and Geman (1984) and generalised by Gelfandand Smith (1990). It can be viewed as a special case of the Metropolis-Hastings sampler, wherethe proposal distribution is constructed from the full conditional posterior distributions of thecomponents θi given θ−i:qi(θ∗|θt−1) =p(θ∗i |θt−1−i ) if θ∗−i = θt−1−i0 otherwise.(A.11)With this proposal distribution the acceptance probability will always be equal to one, andhence the proposed move is always accepted, if θ∗−i = θt−1−i (see for example Gelman et al.1995):α = min{1,p(θ∗)p(θt−1)qi(θt−1|θ∗)qi(θ∗|θt−1)}= min{1,p(θ∗)p(θt−1)p(θt−1i |θ∗−i)p(θ∗i |θt−1−i )}= min{1,p(θ∗i |θ∗−i)p(θ∗−i)p(θt−1i |θt−1−i )p(θt−1−i )p(θt−1i |θ∗−i)p(θ∗i |θt−1−i )}= 1. (A.12)Once, sampling from the target distribution p(θ) has been performed via Markov chains,the samples can then be used for inference on this distribution by Monte Carlo summation.196That is, the expectation of a function g(θ) of the parameter with respect to the distribution p(θ)can be estimated by averaging over the realisations of g for the Markov chain samples:∫g(θ)p(θ)dθ ≈ 1T − T0 + 1T∑t=T0g(θt), (A.13)where T0 is the first sample which is considered to come from the target distribution p. Thefirst T0 − 1 burn-in samples, for which the Markov chain has not yet converged, are discarded.The Monte Carlo summation provides valid inference, even though the Markov samples θt arenot independent (see for example Gelman et al. 1995, for more details). Slow mixing of theMarkov chain, resulting from very large auto-correlations among Markov samples, can presenta problem though, because it implies that the chain moves slowly in the distribution spaceand might have to be run for a very large number of iterations. Thus, while in theory anydistribution can be used as a proposal distribution for Metropolis-Hastings sampling, as long asit has positive probability for all possible realisations of the target distribution p, it is advisableto choose a proposal distribution which is as close to p as possible, in order to achieve fastconvergence - and which also has a large variance ensuring fast mixing.197Appendix BSampling from the Bayesian logisticvariable selection modelB.1 Gibbs sampling algorithmIn this section the Gibbs algorithm to sample from the logistic BVS model as proposed byHolmes and Held (2006) is presented in detail. Recall from equation (3.69) that the jointdistribution is given asp(βγ, γ, z, λ|X, y) ∝ p(y|z)p(z|λ, β, γ, X)p(βγ|γ)p(γ)p(λ),wherep(λj) ∼ 14√λjKS(0.5√λj) andp(z|λ, β, γ,X) = N(Xγβγ, λ).KS() denotes the Kolmogorov- Smirnov distribution. It is proposed to sample from this distri-bution via the full conditionals p(z, λ|β, γ, X, y) and p(βγ, γ|z, λ, X). These distributions areas followsp(z, λ|β, γ, X, y) = p(z|β, γ,X, y)p(λ|z, β, γ,X)p(zj|β, γ, X, y) =Logistic(xγjβγ, 1)I(zj > 0), yj = 1Logistic(xγjβγ, 1)I(zj ≤ 0), yj = 0(B.1)p(λj|z, β, γ, X) ∝ p(zj|λ, β, γ, X)p(λj) = N(xγjβγ, λj) 14√λjKS(0.5√λj) (B.2)198andp(βγ, γ|z, λ,X) = p(γ|z, λ, X)p(βγ|γ, z, λ,X)p(γ|z, λ,X) ∝ p(z|λ, γ, X)p(γ) = N(0, λ + XγvγX ′γ)p∏i=1πγii (1− πi)1−γi (B.3)p(βγ|γ, z, λ,X) = N(B∗γ , V ∗γ ) (B.4)B∗γ = V∗γ X′γλ−1zV ∗γ = (X′γλ−1Xγ + v−1γ )−1.From the conditional distributions (B.1), (B.3) and (B.4) we can sample directly, with variousalgorithms available for updating γ, described in Section 3.3.1. Distribution (B.2) can be sam-pled from efficiently in the following way, using a rejection algorithm introduced by Holmesand Held (2006).The acceptance probability is given as α(λj) =`(r2j ,λj)p(λj)Mg(λj)with r2j = (zj − xγjβγ)2 and`(r2j , λj) = p(zj|xγj, βγ, λj) = Nzj(xγjβγ, λj). Here, g(λj) is the rejection sampling densityg(λj) = c(|rj|)λ−1/2j exp(−0.5(r2jλj+ λj)) with c(|rj|) being a normalising constant not depen-dent on λj . This corresponds to a Generalised Inverse Gaussian distribution GIG(0.5, 1, r2j ) =|rj|/IG(1, |rj|), where IG denotes an Inverse Gaussian distribution with probability densityfunction (Devroye 1986, p.148)p(x) =√|rj|2πx3exp−|rj|(x− 1)22x(x ≥ 0).This choice of rejection distribution leads toα(λj) = exp(0.5λj)p(λj) = exp(0.5λj)ch(0.5√λj)(1− a1(0.5√λj) + a2(0.5√λj)− ...),(B.5)which is an alternate series expansion representation of KS(0.5√λj), i.e. the Kolmogorov-Smirnov density, with terms ch(.) and a1(.), a2(.), ... as in Devroye (1986) (pp.161-167) andHolmes and Held (2006), which allows for efficient sampling.199B.2 Metropolis-Hastings acceptance probability for samplingfrom p(βγ, γ|z, λ,X)In the following, we derive the acceptance probability α(γ, βγ) for sampling from the condi-tional distribution p(βγ, γ|z, λ,X) = p(γ|z, λ,X)p(βγ|γ, z, λ, X) in the logistic BVS modelby Holmes and Held (2006), using the add/delete Metropolis-Hastings sampler. In the add/deletesampler, one indicator variable is selected at random from a uniform distribution (Uniform(1, ..., p))and it is proposed to change its state: the proposal distribution q(γ∗) is given byq(γ∗i ) =γi if i 6= k1 if i = k and γk = 00 if i = k and γk = 1for i = 1, ..., p. (B.6)Note that this impliesp(γ∗)q(γ)p(γ)q(γ∗)=1−πkπkif γk = 1πk1−πk if γk = 0, (B.7)if p(γ) =∏pi=1 πγii (1− πi)1−γi is the prior distribution for γ. This results in the followingacceptance probability for updating (γ, βγ):α(γ, βγ) = min{1,p(β∗, γ∗|z, λ, X)p(β, γ|z, λ, X)q(γ, β)q(γ∗, β∗)}(B.8)= min{1,p(γ∗|z, λ,X)p(β∗|γ∗, z, λ)p(γ|z, λ, X)p(β|γ, z, λ)p(β|γ, z, λ)q(γ)p(β∗|γ∗, z, λ)q(γ∗)}= min{1,p(z|λ, γ∗, X)p(z|λ, γ,X)p(γ∗)q(γ)p(γ)q(γ∗)}= min1,C(γ∗) exp(−0.5z′(λ + Xγ∗vγ∗X ′γ∗)−1z)C(γ) exp(−0.5z′(λ + XγvγX ′γ)−1z)1− πkπk if γk = 1C(γ∗) exp(−0.5z′(λ + Xγ∗vγ∗X ′γ∗)−1z)C(γ) exp(−0.5z′(λ + XγvγX ′γ)−1z)πk1− πk if γk = 0,where C(.) is a normalising constant, which will be defined later. When we apply the Sherman-Morrison-Woodbury matrix inversion formula (e.g. Schott 1997) to the density p(z|λ,X, γ) wegetp(z|λ, γ, X) = C(γ) exp(−0.5z′(λ + XγvγX ′γ)−1z) (B.9)= C(γ) exp(−0.5z′(λ−1 − λ−1Xγ(v−1γ + X ′γλ−1Xγ)−1X ′γλ−1)z).200From here it follows, using the relation between prior covariance vγ and posterior covarianceVγ of βγ Vγ = (v−1γ + X′γλ−1Xγ)−1 (see (3.64)):p(z|λ, γ,X) = C(γ) exp(−0.5z′(λ−1 − λ−1XγVγX ′γλ−1)z) (B.10)= C(γ) exp(−0.5z′λ−1z) exp(0.5z′λ−1XγVγX ′γλ−1z)= C(γ) exp(−0.5z′λ−1z) exp(0.5B′γV −1γ Bγ),because Bγ = VγX ′γλ−1z (again see (3.64) with prior mean of βγ being bγ = 0). The normal-ising constant C(γ) in the normal distribution p(z|λ, γ, X) isC(γ) = (2π)−n/2|λ + XγvγX ′γ|−1/2 (B.11)= (2π)−n/2|λ|1/2|I + λ−1XγvγX ′γ|−1/2= (2π)−n/2|λ|1/2|I + X ′γλ−1Xγvγ|−1/2= (2π)−n/2|λ|1/2|(v−1γ + X ′γλ−1Xγ)vγ|−1/2= (2π)−n/2|λ|1/2|V −1γ vγ|−1/2 = (2π)−n/2|λ|1/2|Vγ|1/2|vγ|1/2 .Hence, following from (B.8) by plugging in (B.10) and (B.11), the acceptance probability isgiven asα(γ, βγ) = min1,C(γ∗) exp(0.5B′γ∗V−1γ∗ Bγ∗)C(γ) exp(0.5B′γV−1γ Bγ)1− πkπk if γk = 1C(γ∗) exp(0.5B′γ∗V−1γ∗ Bγ∗)C(γ) exp(0.5B′γV−1γ Bγ)πk1− πk if γk = 0(B.12)= min1,|Vγ∗ |1/2|vγ|1/2|Vγ|1/2|vγ∗|1/2exp(0.5B′γ∗V−1γ∗ Bγ∗)exp(0.5B′γV−1γ Bγ)1− πkπk if γk = 1|Vγ∗ |1/2|vγ|1/2|Vγ|1/2|vγ∗|1/2exp(0.5B′γ∗V−1γ∗ Bγ∗)exp(0.5B′γV−1γ Bγ)πk1− πk if γk = 0.Note that the acceptance probability for a Gibbs sampler, updating either the complete γ vectoror a subset of components γI = (γi)i∈I by the conditional distribution p(γI |γ−I , z, λ,X), isalways equal to one:α(γ, βγ) = min{1,p(β∗γ , γ∗|z, λ, X)p(βγ, γ|z, λ, X)q(βγ, γ)q(β∗γ , γ∗)}= min{1,p(β∗γ |γ∗, z, λ, X)p(γ∗|z, λ, X)p(βγ|γ, z, λ,X)p(γ|z, λ,X)p(βγ|γ, z, λ,X)p(γI |γ−I , z, λ, X)p(β∗γ |γ∗, z, λ,X)p(γ∗I |γ∗−I , z, λ, X)}= min{1,∏pi=1 πγ∗ii (1− πi)1−γ∗i∏pi=1 πγii (1− πi)1−γi∏i∈I πγii (1− πi)1−γi∏i∈I πγ∗ii (1− πi)1−γ∗i}= 1. (B.13)201B.3 Sampling from a tempered distributionWhen the parallel tempering algorithm is applied to the logistic BVS model in Chapter 7, thehierarchical model is as follows.yj =1 if zγj > 00 otherwisezγj = xγjβγ + εjεj ∼ N(0, Tλj)λj = (2φj)2φj ∼ Kolmogorov-Smirnov (i.i.d.)βγ ∼ N(bγ = 0, vγ = c2Ipγ )γ ∼ p(γ) =p∏i=1πγii (1− πi)1−γi .This corresponds to the joint posterior distributionpT (βγ, γ, z, λ|X, y) ∝ pT (βγ, γ, z, λ, y|X)= p(y|z)pT (z|λ, β, γ, X)p(βγ|γ)p(γ)p(λ),where the prior distributions p(γ), p(βγ), and p(λj) ∼ 14√λjKS(0.5√λj) are as previously,but the likelihood pT (z|λ, β, γ,X) = Nz(Xγβγ, Tλ) is tempered with a temperature parameterT > 1, which is a scalar factor multiplied to the diagonal covariance matrix λ. Sampling isdone via a Gibbs algorithm corresponding to the algorithm described in Appendix B.1, i.e. wesample from the full conditionals pT (z, λ|β, γ,X, y) and pT (βγ, γ|z, λ, X) with• pT (z, λ|β, γ, X, y) = pT (z|β, γ,X, y)pT (λ|z, β, γ, X) with– pT (zj|β, γ,X, y) =Logistic(xγjβγ,√T )I(zj > 0), yj = 1Logistic(xγjβγ,√T )I(zj ≤ 0), yj = 0– pT (λj|z, β, γ, X) ∝ pT (zj|λ, β, γ, X)p(λj) = N(xγjβγ, Tλj) 14√λjKS(0.5√λj)• pT (βγ, γ|z, λ, X) = pT (γ|z, λ, X)pT (βγ|γ, z, λ, X) with– pT (γ|z, λ,X) ∝ pT (z|λ, γ, X)p(γ) = N(0, Tλ + XγvγX ′γ)∏pi=1 πγii (1− πi)1−γi202– pT (βγ|γ, z, λ, X) = N(B∗γ, V ∗γ )where B∗γ = V∗γ X′γ(Tλ)−1z and V ∗γ = (X′γ(Tλ)−1Xγ+v−1γ )−1. Note that N(0, Tλ+XγvγX′γ) = N(0, T (λ−1 − λ−1XγT−1V ∗γ X ′γλ−1)−1) according to the Sherman-Morrison-Woodbury matrix inversion formula (Schott 1997).For a more detailed derivation of these formulae refer to the paragraph below. When proposingto exchange the values of θ1 = (βγ1, γ1, λ1, z1) sampled from the distribution of temperatureT1 and θ2 = (βγ2, γ2, λ2, z2) from the distribution of temperature T2, then the acceptance prob-ability α12 is given asα12 = min{1,pT1(z2|β2, γ2, λ2, X2)pT2(z1|β1, γ1, λ1, X1)pT2(z2|β2, γ2, λ2, X2)pT1(z1|β1, γ1, λ1, X1)}= min{1,Nz2(Xγ2βγ2, T1λ2)Nz1(Xγ1βγ1, T2λ1)Nz2(Xγ2βγ2, T2λ2)Nz1(Xγ1βγ1, T1λ1)}= min{1,((2π)n|T1λ2|)−1/2 exp(− 12T1 (z2 −Xγ2βγ2)′λ−12 (z2 −Xγ2βγ2))((2π)n|T2λ2|)−1/2 exp(− 12T2 (z2 −Xγ2βγ2)′λ−12 (z2 −Xγ2βγ2))×((2π)n|T2λ1|)−1/2 exp(− 12T2 (z1 −Xγ1βγ1)′λ−11 (z1 −Xγ1βγ1))((2π)n|T1λ1|)−1/2 exp(− 12T1 (z1 −Xγ1βγ1)′λ−11 (z1 −Xγ1βγ1))}= min{1, exp((1T1− 1T2)× (−12(z2 −Xγ2βγ2)′λ−12 (z2 −Xγ2βγ2) +12(z1 −Xγ1βγ1)′λ−11 (z1 −Xγ1βγ1)))}.Adopting the Gibbs sampler from the untempered to the tempered model In this para-graph we derive how the conditional distributions in the Gibbs sampler change from the un-tempered to the tempered distribution (as outlined above).• pT(zj|β, γ,X,y): Show thatpT (z|m) =∫pT (z|m,λ)p(λ)dλ =∫Nz(m,Tλ)14√λKS(0.5√λ)dλ (B.14)= Logistic(m,√T ) =1√Texp(−z −m√T)(1 + exp(−z −m√T))−2 :(B.15)It is known thatp(z) =∫p(z|λ)p(λ)dλ =∫Nz(0, λ)14√λKS(0.5√λ)dλ (B.16)= Logistic(0, 1) = exp(−z)(1 + exp(−z))−2. (B.17)203It is easy to see that the variable y =√Tz + m has the density from equation (B.15) if zhas the standard logistic density (equation B.17). Hence, the same variable transforma-tion will change the integral in equation (B.16) to the form in equation (B.14).• pT(λj|z, β, γ,X): Rejection sampling with acceptance probabilityα(λj) =`(r2j ,λj)p(λj)Mg(λj)with r2j = (zj − xγjβγ)2 and `(r2j , λj) = pT (zj|xγj, βγ, λj) =Nzj(xγjβγ, Tλj).Here, g(λj) is the rejection sampling density g(λj) = Tc(|rj|)(Tλj)−1/2 exp(−0.5( r2jTλj+Tλj)) with c(|rj|) being a normalising constant not dependent on T and λj (linear trans-formation λj = T−1X where X ∼ GIG(0.5, 1, r2j ) = |rj|/IG(1, |rj|), GIG denotingthe Generalised Inverse Gaussian and IG the Inverse Gaussian densities).This leads toα(λj) = exp(0.5Tλj)T−1p(λj)= exp(0.5Tλj)T−1ch(0.5√λj)(1− a1(0.5√λj) + a2(0.5√λj)− ...),where ch and a1, a2, ... are from an alternate series expansion for the Kolmogorov-Smirnov density KS(0.5√λj), equivalently to the untempered situation.• pT(γ|z, λ,X): With pT (z|γ, β, λ,X) = Nz(Xγβγ, Tλ) and p(βγ|γ) = Nβγ (bγ = 0, vγ =c2Ipγ ) it follows for the marginal distribution (e.g. Lindley and Smith 1972)pT (z|γ, λ,X) =∫pT (z|β, γ, λ,X)p(βγ|γ)dβγ = Nz(0, Tλ + XγvγX ′γ).• pT(βγ|γ, z, λ,X): With pT (z|β, γ, λ, X) = Nz(Xγβγ, Tλ) and p(βγ|γ) = Nβγ (bγ =0, vγ = c2Ipγ ) it follows for the posterior distribution (Lindley and Smith 1972)pT (βγ|γ, z, λ,X) = N(Bγ, Vγ)Vγ = (X′γ(Tλ)−1Xγ + v−1γ )−1Bγ = VγX′γ(Tλ)−1z.204Appendix CComparing the characteristics of geneexpression profiles derived by univariateand multivariate classification methods11This chapter has appeared as: M. Zucknick, S. Richardson, E.A. Stronach (2008). Compar-ing the Characteristics of Gene Expression Profiles Derived by Univariate and Multivariate ClassificationMethods. Statistical Applications in Genetics and Molecular Biology, 7(1): article 7. Available at:http://www.bepress.com/sagmb/vol7/iss1/art7.205AbstractOne application of gene expression arrays is to derive molecular profiles, i.e. sets of genes, which dis-criminate well between two classes of samples, for example between tumour types. Users are confrontedwith a multitude of classification methods of varying complexity that can be applied to this task. To helpdecide which method to use in a given situation, we compare important characteristics of a range ofclassification methods, including simple univariate filtering, the penalised likelihood methods lasso andridge regression and elastic net, as well as random forest with and without variable selection.In addition to classification accuracy, the biological interpretability of molecular profiles is alsoimportant. This implies both parsimony and stability, in the sense that profiles should not vary muchwhen there are slight changes in the training data. We perform a random resampling study to comparethese characteristics between the methods and across a range of tuning parameter values. We measurestability by adopting the Jaccard index to assess the similarity of resampled molecular profiles.We carry out a case study on five well-established cancer microarray data sets, for two of whichwe have the benefit of being able to validate the results in an independent data set. The study showsthat those methods which produce parsimonious profiles generally result in better prediction accuracythan methods which don’t include variable selection. For very small profile sizes, the sparse penalisedlikelihood methods tend to result in more stable profiles than univariate filtering while maintainingsimilar predictive performance.206C.1 IntroductionGene expression microarray technologies allow the study of the simultaneous mRNA expres-sion of thousands of genes and their comparison between different samples and under varyingconditions. Of special interest is the construction of gene expression profiles for classifica-tion, for example of tumours, or to predict pathological characteristics and clinical outcomes ofcomplex diseases such as cancer. A wide variety of classification methods have been appliedto this task (for an overview see Dudoit et al. 2002, Dupuy and Simon 2007) and biologists andclinicians are confronted with a multitude of methods of different complexities. Thus, it is adifficult task to decide which method is best in a given context.To help with this decision, we here compare the most important characteristics of a rangeof classification methods, from simple univariate filtering to penalised likelihood methods andstate-of-the-art machine learning algorithms. We restrict ourselves to the binary classificationproblem, but all methods can be generalised to situations where one wants to discriminatebetween more than two classes.It is an additional complicating factor that often there are several potentially contrastingaims involved. One very important goal is to construct gene expression profiles which have agood prediction accuracy, i.e. which are able to classify new samples with a small misclassifi-cation error. An additional aim is that the expression profiles can be interpreted in biologicalterms and provide insight into the data structure (e.g. Dudoit et al. 2002, Somorjai et al. 2003,Díaz-Uriarte and Alvarez de Andrés 2006). This implies parsimony, that is that profiles shouldcontain only a relatively small number of genes, which can then be followed up by litera-ture searches and functional experiments to determine their role in the biological processesinfluencing the phenotype of interest. A third desirable property, that is also related to theinterpretability of profiles, is their stability in the sense that the set of genes selected into themolecular profile and the associated predictive performance should not vary much when the setof samples used for training is altered slightly (e.g. Díaz-Uriarte and Alvarez de Andrés 2006).A basic principle of statistical analysis is to provide measures of uncertainty for all esti-mates. This also applies to molecular profiles derived from microarray gene expression data(Michiels et al. 2005, Ein-Dor et al. 2005, Simon 2006). This means that the uncertainty ofgenes associated with their inclusion in the profile should be assessed as well as the probability207for a particular profile to be selected relative to other possible solutions. Here we address thisissue by estimating the instability in molecular profiles using a resampling setup, where thedata are repeatedly randomly split into training and validation data and the molecular profilesassociated with each of the splits are compared. A perfectly stable profile would contain thesame genes for all training/validation splits. We propose to use the Jaccard similarity measureto assess how similar the profiles for all data splits are.The assessment of uncertainty is particularly important for data coming from high-throughputtechnologies such as gene expression microarrays, because for these data sources a large amountof instability is expected for two reasons. Firstly, such data are associated with large technicaland biological variation leading to low signal-to-noise ratio. And secondly, the data are high-dimensional and usually comprise many more variables (p) than samples (n), which introducesmulti-collinearity in the input data matrix leading to instability in the estimation procedure.This is known as the “large p, small n” problem.One way of solving this problem is by using a univariate filtering method to reduce thenumber of variables (e.g. Golub et al. 1999, Dudoit et al. 2002, van’t Veer et al. 2002). How-ever, expression levels are often quite highly correlated between genes, because genes areco-regulated or act in the same biological pathways. Univariate approaches do not take thecorrelation structure into account, in contrast to multivariate methods. Multivariate approachesthat are capable of handling p >> n data sets include penalised likelihood methods where apenalty term added to the log-likelihood function enforces unique parameter estimates. Here,we employ and compare the L1- and L2-penalties which correspond to the lasso (Tibshirani1996) and ridge (Hoerl and Kennard 1970) logistic regression models, as well as the elasticnet which combines both penalties (Zou and Hastie 2005). Another approach comes from themachine learning community, where ensemble methods have been used extensively. Thesemethods build powerful classifiers from many weak simple classifiers. Here, we apply randomforests as a representative from this class of methods, since they have been shown to performvery well in the context of microarray data, especially when the method is combined with anadditional variable selection step (Breiman 2001, Díaz-Uriarte and Alvarez de Andrés 2006).In the following section, the classification methods to derive molecular profiles are de-scribed, the resampling setup used to assess the stability of profiles is specified and measuresfor quantifying stability are characterised. The methods are applied to five publicly available208microarray gene expression data sets which are introduced in Section C.3; for two of theseindependent data are available for validation. The results are presented in Section C.4 and thepaper concludes with a discussion.C.2 MethodsC.2.1 Classification methodsWe compare several binary classification methods, and use the logistic regression model for allmethods except the random forest:logit(p(Y = 1|X)) = Xβ,where X ∈ Rn×p is the matrix of gene expression values, β ∈ Rp denotes the vector ofregression coefficients and Y ∈ {0, 1}n is the binary response vector.For all methods the amount of shrinkage and thus the sizes of the molecular profiles andtheir prediction performances depend on tuning parameters. For the univariate method thisis simply the number of genes p∗ chosen to be included in a profile, while for the penalisedregression methods they are the penalty parameters for the L1 and L2 norms λ1 and λ2. Severalparameters can be tuned for the random forest methods, the most important one being numberof variables to be considered for node splits in the decision trees. However, Díaz-Uriarte andAlvarez de Andrés (2006) perform an extensive sensitivity analysis and come to the conclusionthat the performance of random forests is quite insensitive to the choice of tuning parametervalues, and we follow their suggestions for the choice of parameter values for microarray dataanalyses.For most analyses the statistical computing package R (R Development Core Team 2006)was used, in particular the affy library for data pre-processing of the Affymetrix data setsand the glm library for univariate logistic regression analyses. The R library glmpath wasused for the elastic net and the randomforest and varSelRF libraries for random forestwithout and with variable selection, respectively. Ridge and lasso regression analyses werecarried out with the BBR software by Genkin et al. (2007).209Univariate filteringUnivariate filtering methods select a small number of gene variables based on univariate statis-tics assessing the potential of individual genes for class prediction. Here, we use the geneeffects estimated by logistic regression models fitted for each gene variable separately plus op-tionally any clinical covariates. The p∗ “best” genes with the largest absolute effects |β̂|/s.e.(β̂)(where β̂ is the regression coefficient estimate and s.e.(β̂) its observed standard error) are se-lected and together they build the molecular profile. Since this molecular profile does notdirectly correspond to a statistical model for prediction, a second analysis step has to be under-taken where the selected genes are used in a binary classification method.Nearest-centroid classification (NC) The simple nearest-centroid classification rule has of-ten been applied successfully to gene expression data (e.g. van’t Veer et al. 2002, Michiels et al.2005). First, centroids, i.e. mean average profiles, are constructed for each class based on thetraining data available for the selected genes in the molecular profile. New samples are thenassigned to the class whose centroid is closer to the sample based on a similarity (or distance)measure, here Pearson’s correlation r. That is, for two classes 0 and 1, a sample with geneexpression profile x = (x1, ..., xp∗) is assigned to class 1 iffr(x, x̄1) > r(x, x̄0), (C.1)where x̄k (k ∈ {0, 1}) is the mean expression vector (centroid) in the training samples of classk.Diagonal linear discriminant analysis (DLDA) Dudoit et al. (2002) compared various clas-sification rules for the univariate filtering approach. They selected between 10 and 200 variablesin several microarray data sets and found that simple classification methods generally outper-formed more complex methods in this context. In particular, diagonal linear discriminant anal-ysis was found to perform very well. DLDA is similar to the nearest-centroid method, exceptthat here the sample variances are taken into account. Sample x = (x1, ..., xp∗) is assigned toclass 1 rather than class 0 iffp∗∑i=1(xi − x̄1i)2s2i<p∗∑i=1(xi − x̄0i)2s2i, (C.2)210where s2i is the ith diagonal element of the pooled variance estimate of the diagonal covariancematrix Σ, which is assumed to be the same for both class populations.Multivariate penalised regressionRidge regression The ridge estimator β̂2 (Hoerl and Kennard 1970) is the penalised max-imum likelihood solution of a regression problem, where a penalty term is imposed on thelog-likelihood function `(β) which is proportional to a tuning parameter λ2 > 0. For logisticregression the log-likelihood is given as`(β) = log p(Y |X, β) = −n∑j=1log(1 + exp(−YjXTj β)). (C.3)The penalty term contains the sum of squared regression coefficients∑pi=1 β2i (i.e. the L2 normof β), so that the ridge regression estimator for a fixed penalty parameter λ2 is given asβ̂2 = arg maxβ(`(β)− λ2p∑i=1β2i ). (C.4)Finding the ridge regression solution is equivalent to determining the maximum a posteriori(MAP) estimate of the Bayesian regression model with independent and identical Gaussianpriors βi ∼ N(0, τ2 > 0) on each parameter βi, where the prior variance τ2 is related to thepenalty parameter λ2 by τ2 = 1/(2λ2).Lasso regression Lasso regression (Tibshirani 1996) is similar to ridge regression, with theonly difference being that here the L1 norm of the regression coefficient vector λ1||β||1 =λ1∑pi=1 |βi| is used, instead of the L2 norm, leading to the optimisation problemβ̂1 = arg maxβ(`(β)− λ1p∑i=1|βi|) (C.5)The penalised likelihood solution with the L1 norm corresponds to the MAP estimate for theBayesian regression model with independent, identical Laplace (also called double exponential)distributions with mean 0 and variance τ1 = 2/λ21 as priors on the β parameters.Elastic net (ENet) The naïve elastic net simply uses both L1 and L2 penalty terms in thepenalised log-likelihood function:β̂12 = arg maxβ(`(β)− λ1p∑i=1|βi| − λ2p∑i=1β2i ) (C.6)211However, this results in over-shrinkage when compared to the lasso (Zou and Hastie 2005),and the estimates for β from the naïve elastic net are scaled to determine the final elastic netestimates:β̂EN = (1 + λ2)β̂12. (C.7)The L1-penalty has the advantage of automated variable selection over the L2-penalty.This implies that for the lasso and the elastic net, the estimated effect of most variables will beshrunk to zero, effectively excluding them from the set of relevant covariates. Note that for thelasso method there is a practical restriction on the maximum number of variables which can beselected, which depends on the sample size n and number of variables p: min(n − 1, p) (Zouand Hastie 2005). This restriction does not apply to the elastic net.All the estimated penalised regression models can be directly used for class prediction,since the logistic regression model provides probability estimates for class membership. Asample is predicted as belonging to a class, if the estimated class probability is larger than 1/2.Random forest (RF) and varSelRFThe random forest classifier (Breiman 2001) is an example of the class of ensemble classifi-cation algorithms, which combine the outputs of many “weak” classifiers, in this case classifi-cation trees, to produce a powerful ensemble. The random forest can be successful in dealingwith the multi-collinearity of “large p, small n” applications, because it combines two ideas tohelp find as many of the multiple best solutions as possible: firstly, it uses repeated bootstraps,that is each tree is grown using a different bootstrap sample of the data, and secondly it alsoemploys random subspace selection, i.e. it only uses a random subset of all available variablesto grow each tree. Because of this, for p >> n data it is likely that most or all variables willget used in node splits for some of the trees. The final classification is the mode of the classifi-cations of all trees: the random forest chooses the class that has been decided by the majorityof trees.While random forests can deal with p >> n data, it has been found that the classificationperformance can be improved if the classifier is combined with a variable selection step so thatonly a small number of variables get used in the entire forest, see Díaz-Uriarte and Alvarez deAndrés (2006). There, the performance of random forests is compared to the varSelRF method,212which implements variable selection by iteratively fitting random forests and discarding thevariables which get used as nodes least often.C.2.2 Multiple random validation study setupWe employ a multiple random validation setup (e.g. Michiels et al. 2005), where the data arerepeatedly randomly divided into training data and validation data. We perform 50 randomsamplings, each with a ratio of 2:1 for the size of training to validation sample sizes. Theresampling scheme is outlined in Table C.1.Table C.1: Resampling study setup for comparison of the characteristics of the classification methods.For k = 1, ..., m (m = 50):• Divide data randomly, assigning 2/3 of the samples into a training set k and the remaining 1/3into a validation set k (optionally, fix class proportions).• For all classification methods, and for a set choice of tuning parameter values, fit classificationmodel using training data set k and find molecular profile k.• Apply fitted model to validation data k and assess prediction performance in terms of percentageof misclassified validation samplesC.2.3 Assessing the instability of molecular profilesWe view stability of gene expression profiles in terms of whether the same genes get selectedfor different training data sets. Naturally, this concept does not apply to those classifiers that useall the gene variables. Hence, we only assess the stability for those methods that do incorporatefeature selection.In the microarray literature most attempts to evaluate the stability of gene expression pro-files for classification have focussed on resampling setups such as bootstrapping (Díaz-Uriarteand Alvarez de Andrés 2006) or repeated splits into training and validation subsets (Michielset al. 2005). Examples include the approach taken by Díaz-Uriarte and Alvarez de Andrés(2006), Davis et al. (2006), Ma et al. (2006) and others, who argue that if gene sets are stablethen the majority of genes will be included in most sets. Consequently, they use the inclusion213frequencies to derive a single measure of stability, e.g. by averaging over the frequencies of allgenes that get included at least once.Another approach is to use the size of the intersection between gene sets. For example,Ein-Dor et al. (2005) consider all(m2)pairwise comparisons between any two of the m = 50gene sets and use the average of the sizes of the pairwise intersections as a single summarymeasure. Michiels et al. (2005) and Davis et al. (2006) use a generalisation of the joint inter-section between all m gene sets, by considering the proportion of genes that get selected into> 50% or > 75% of all m profiles. Note that these approaches do not take the relative sizes ofthe gene sets into account, so that one cannot compare the stability of a classifier that produceslarge profiles with one producing very small profiles.To reflect this, Blangiardo and Richardson (2007) propose the ratio of observed to expectedsize of intersection in a situation where gene sets are independent. However, in our resamplingsetup the gene sets are not independent since the various training subsets partly overlap, andin the absence of independence the expected size of intersection is difficult to obtain withoutcomputationally demanding data-dependent permutation studies.In addition, relying on the size of the intersection between sets as a measure of similaritybetween the sets is not satisfactory in itself, because the intersection size does not fulfill severalcriteria (outlined in the next section) that are desirable in this context.Similarity indicesThere is a wide variety of similarity measures available for the comparison of sets (see Simpson1960, Hazel 1970, Sokal and Sneath 1973, and others). Measures ρ(Z1, Z2) for the comparisonof two discrete sets Z1 and Z2 are usually based on the two-by-two table counting the presencesand absences in both sets (Table C.2).Table C.2: 2× 2 table counting presences (1) and absences (0) in gene sets Z1 and Z2.Z11 0Z2 1 a b0 c d214Generally, the gene expression profiles will be parsimonious, so that the number of presentgenes will be much smaller than the number of absent genes. Because of this we consider thepresence of a gene in two profiles to contribute more to the similarity of these profiles thanits absence and prefer measures which are independent of the value of d. In addition, themeasure should also have a number of other desirable properties (e.g. Janson and Vegelius1981, Sepkoski Jr. 1974), in particular• symmetry: ρ(Z1, Z2) = ρ(Z2, Z1),• homogeneity: ρ(Z1, Z2) does not change if the numbers a, b, and c are multiplied by thesame constant, and• boundedness: min(ρ(Z1, Z2)) = 0 and max(ρ(Z1, Z2)) = 1,and ρ(Z1, Z2) = 0 ⇔ a = 0, ρ(Z1, Z2) = 1 ⇔ b = c = 0.Note that the simple intersection size a, which seems an obvious choice and has beenproposed often (see the previous section), does not fulfill the homogeneity and boundednesscriteria. This means that one cannot compare the values of a(Z1, Z2) and a(Z3, Z4) computedfor different pairs of sets, especially if the sets are of different sizes and hence the maximumpossible value of a is different for both cases. Three popular measures that do fulfill the re-quirements and are better suited for comparisons are:• Jaccard’s index (Jaccard 1901) ρj(Z1, Z2) = aa+b+c = #(Z1∩Z2)#(Z1∪Z2)• Dice-Sorensen’s index (Dice 1945) ρd(Z1, Z2) = 2a2a+b+c• Ochiai’s index (Ochiai 1957) ρo(Z1, Z2) = a√a+b√a+c .The Jaccard index is the ratio of set intersection size to set union size, which makes it intuitiveand easy to interpret. The Dice-Sorensen and Ochiai indices can be interpreted as the harmonicand geometric means of the ratios a/(a + b) and a/(a + c), respectively. The Dice-Sorensenand Jaccard indices are very similar and in fact are increasing functions of each other withρd = 2ρj/(1 + ρj) and ρj = ρd/(2 − ρd), so that for the purpose of comparison of differentsimilarity values the two indices are equivalent. In our applications we found the harmonicmean index (Dice-Sorensen) and geometric mean index (Ochiai) to be very similar (see FigureC.5 in Appendix C.A for one data set). Hence, throughout the paper we only show the results215for the Jaccard index as a representative of the three similarity indices because it is arguablythe most popular of the three measures.The indices assess the similarity between pairs of sets, so in order to compare m > 2 setswe compute the indices of all possible m2 combinations of pairs of sets and assess theempirical distributions of these values.C.3 DataWe apply the resampling scheme and assess the predictive accuracy and stability of the molecu-lar profiles derived from the methods described in the previous section to five publicly availablegene expression data sets, which are summarised in Table C.3. In the resampling scheme, thesamples are assigned randomly to either training or validation subset without restriction. An ex-ception is the ovarian cancer data set, where only 17% of all samples belong to the less frequentclass. Here, to ensure that all subsets contain samples from both classes, the class proportionsin all training and validation subsets are fixed to be the same as in the complete data.In addition, for two of the data sets (breast and ovarian cancer) independent validationdata sets are available. The breast cancer validation data (van de Vijver et al. 2002) have beengenerated by the same centre using the same platform and protocols as in the original study byvan’t Veer et al. (2002). We only include samples in the validation set that were not part of theoriginal study. In addition, we restrict our validation samples to lymph-node negative patientsonly, since that was an inclusion criterion for the original study.In the instance of the ovarian cancer validation data the validation samples have been col-lected and processed by a different team in a study conducted independently from the originalstudy by Schwartz et al. (2002). However, both study groups have very similar clinical charac-teristics and outcome data (Lu et al. 2004) and it is reasonable to assume that both groups comefrom comparable populations.All gene expression data were generated using Affymetrix oligoarrays (although with sev-eral different chip types), except the breast cancer expression data, which were generated withAgilent two-colour arrays. All Affymetrix data are pre-processed and normalised in the sameway using RMA background-correction (Irizarry et al. 2003) and loess regression for array nor-216Table C.3: Main characteristics of gene expression microarray data sets used. Data sets in italics andbrackets are used as validation data. p n Response Chip type(binary)Ovarian cancer - 7129 104 tumour type HuGeneFLSchwartz et al. (2002) (mucinous/clear-cell vs.(Lu et al. 2004) 12625 42 endometrioid/serous) U95Av2Leukaemia 7129 72 tumour type HuGeneFL(ALL/AML) - (ALL vs. AML)Golub et al. (1999)Prostate cancer - 12625 102 tumour vs. normal U95Av2Singh et al. (2002)Breast cancer - 4770 97 metastasis-free survival Agilentvan’t Veer et al. (2002) (≤ 5 yrs vs. > 5 yrs)(van de Vijver et al. 2002) 4770 87 AgilentAcute myeloid leukaemia 22283 273 normal vs. abnor- U133A(AML/karyotype) - mal karyotypeValk et al. (2004)malisation (Cleveland 1979). An exception is the ALL/AML data set where the pre-processeddata provided by the R package golubEsets were used.All Affymetrix data are centered and scaled to zero mean and unit variance for all genevariables in the binary classification analysis. The Agilent data are normalised in the same wayas described in the original paper (van’t Veer et al. 2002). After filtering and removing thesmall proportion of genes with missing values, 4770 genes remain for analysis.Note that for the breast cancer data set, clinical data, which are known predictive factorsfor breast cancer progression, are available in addition to the gene expression data. These arepatient age, tumour grade, tumour diameter and angioinvasion. Since the interest is in devel-oping molecular profiles which can improve on predictive accuracy on top of known clinicalfactors, the clinical data are included in all classification methods and their effects are not al-lowed to be shrunken by the penalised likelihood methods nor to be removed from the activevariable set in all other methods.217C.4 ResultsEach of the five data sets is randomly split into training and validation subset m = 50 times.For each of the data sets all classification methods are fitted to each of the 50 training subsetsfor a range of tuning parameter values.The tuning parameter values are carefully chosen to cover a wide range of models andmodel sizes. For univariate filtering the number of variables to be selected is p∗ ∈ {5, 10, 50,100, 500}, since we assume that the inclusion of more than 500 gene variables will not resultin further improvement of the predictive accuracy. For the Affymetrix data sets the ridge andlasso penalty parameters were chosen so that the corresponding prior variances τ range from0.01 to 100 for lasso and from 10−5 to 1 for ridge regression (both on the log10 scale). For lassoregression, the smallest profiles with τ = 0.01 usually contain less than 5 genes (an exceptionis the AML/karyotype data set). Since the data are normalised to have unit variance, the choiceτ = 100 reflects an extremely large prior variance compared to the sample variances andinduces very little shrinkage. Recall that in ridge regression no sparsity is induced, and hencethe profile sizes cannot be used to determine the range of values for the penalty parameter.Instead, preliminary test runs were performed to ensure that the range of models with bestprediction accuracy is covered for all data sets. Since the Agilent data are pre-processed andnormalised in a different way to the Affimetrix data, slightly different penalty parameter valueshad to be chosen to cover the entire range of models (τ1 ∈ {10−3, 10−2.5, 10−2, 10−1.5, 10−1}for lasso and τ2 ∈ {10−6, 10−5, 10−4, 10−3, 10−2} for ridge). Note that for the elastic net twotuning parameters exist, which regulate the size of the L1- and L2-penalty terms, respectively.Here, we use a fixed penalty for the L2-term (λ2 = 1) and vary the size of the L1-penaltyparameter λ1 from zero, resulting in the largest possible profiles, which are of comparable sizesto the largest observed lasso profiles, to a value large enough to induce maximum sparsity, i.e.so that no genes are included in the model.C.4.1 Prediction accuracyThe fitted models can be applied to the corresponding validation subsets, where we recordthe proportion of misclassified samples as a measure of prediction accuracy, resulting in 50misclassification error values per data set and fitted model, which are summarised as boxplots218in Figure C.1. We focus on the median values as the main measure for comparisons betweenmethods. Note that for the elastic net (ENet) the misclassification rates can be quite large forvery small molecular profiles, i.e. where the largest penalties are applied, but they quicklydecrease and level off when the penalty is decreased and more genes are allowed to enter themodel. For all of the data sets except the ALL/AML data, for univariate filtering (NC andDLDA) the misclassification rates decrease first but then rise again when increasing the profilesize p∗, especially when applying diagonal linear discriminant analysis (DLDA).Note that because we want to compare characteristics of molecular profiles of differentsizes, we do not tune the classification methods to optimise their performances in terms ofminimal misclassification errors, i.e. we do not attempt to choose “optimal” tuning parametervalues among those values presented alongside each other in Figure C.1. Because of this there isno need to include an internal cross-validation or bootstrapping step within each of the m = 50training/validation splits of the data. This would be necessary for an unbiased estimation ofthe generalisation error in order to avoid over-fitting when tuning parameters of classificationmethods.The predictive accuracies that can be achieved by gene expression profiles vary widelybetween data sets. The first three data sets (ovarian cancer, ALL/AML, and prostate cancer)are easily separable, and the minimum median error rates correspond to as little as only one ortwo misclassified samples for the ovarian and prostate cancer data. Classification is not so easyfor the last two data sets (breast cancer and AML/karyotype), where the best median error ratesachieved are about 30%. This reflects the idea that different types of outcome data are moreor less related to gene expressions. For example, tissue or tumour type can be explained to alarge degree by gene expression, as we observe for the first three data sets. On the other hand,making a prognosis for cancer survival is a much more complex problem which is influencedto a large degree by environmental factors, as well as additional genetic factors other than justgene expression levels. This is in part reflected by the fact that for our breast cancer data, whereclinical covariates are available, the use of these clinical variables alone for classification be-tween patients with favourable and unfavourable survival prognosis already achieves a medianmisclassification proportion of 39%. So the additional inclusion of gene expression data aspredictors only reduces the median error rate from 39% to about 30%.In general, prediction performances of the lasso, elastic net, and univariate filtering meth-219Ovarian cancer% misclassification010203040505 10 50 100500 5 10 50 1005007129 2 7 12 17 21 2 3 3 3 4 5 5 6 6 7 8 9 10 11 12 13 14 15 16 17 19 21 22 234823 3ALL/AML% misclassification010203040505 10 50 100500 5 10 50 1005007129 2 12 18 21 24 3 4 5 6 7 8.5 10 11 12 13 14 15 16 16 17 18 19 1919.5 20 214610 4Prostate cancer% misclassification010203040505 10 50 100500 5 10 50 10050012625 518.529.534.5 38 3 4 5 6 6.5 7 8 9 10 1213.515.5 1819.521.5 23 24 25 26 2727.5 3030.5 318518.5 12Breast cancer% misclassification010203040505 10 50 100500 5 10 50 1005004770 4 13 23 29 32 2 3 5.5 9 13 18 24 28 34 41 46 51 564669.5 14AML/karyotypemedian profile size% misclassification203040505 10 50 100500 5 10 50 10050022283 3493.5118.5129138 8 11 16 22 28 36 43 49 56 62 68 74 80 85 88 92 96 99 1011031071091111131151172058676.5NC DLDA Ridge Lasso ENet RF varSelRFFigure C.1: Boxplots of predictive performances in terms of proportion of misclassified validationsamples shown for a range of tuning parameter values. The median profile sizes (averaging acrossthe 50 resampling sets) corresponding to each parameter value are indicated for all methods below thecorresponding boxplots. The orange lines represent the baseline classification rates, where all samplesare assigned to the most frequent class.220ods all seem to be comparable in the sense that for all data sets we observe similar minimumvalues of the median error rates between these methods. There is not much difference betweenthe two sparse penalised likelihood methods (lasso and elastic net); the additional L2-penaltyterm introduced in the elastic net does not seem to improve the prediction accuracy. Rememberthat the two univariate filtering methods only differ in the final classifier applied to the selectedgenes; the gene lists themselves are identical. Despite this, the prediction performances cansomewhat differ. In particular, DLDA achieves smaller median error rates than the nearestcentroid (NC) methods for the prostate cancer and AML/karyotype data.For the classification methods that do not perform automatic variable selection, i.e. ridgeregression and the random forest (RF), the prediction performances are comparable to the othermethods for some data sets (with equal or slightly larger median error rates), but in some casesthey perform substantially worse. For example, both ridge regression and random forest havehigher misclassification errors in the ovarian and breast cancer data, and in addition the randomforest has higher error rates when applied to the prostate cancer data. The varSelRF method(random forest with variable selection) tends to have smaller prediction errors than the randomforest without variable selection, except in the ALL/AML data set.C.4.2 Ranking genes by their profile inclusion frequenciesClassification methods with inherent variable selection produce parsimonious gene expressionprofiles consisting of a small number of genes. In our case these methods are univariate filter-ing, lasso, elastic net and varSelRF. Since for these methods not all genes are selected into allprofiles corresponding to the resampled training data subsets, we can rank the genes by theirfrequency of selection, giving a measure for the relative importance of a gene for class predic-tion (e.g. Michiels et al. 2005, Díaz-Uriarte and Alvarez de Andrés 2006). We will be morecertain that a gene is relevant if it is selected most of the time. As an example, the selectionfrequencies are shown for the ovarian cancer data in Figure C.2, for those genes which getselected into at least half of the profiles with any of the methods that incorporate variable selec-tion (for at least one tuning parameter value). The selection frequencies for the different tuningparameter values are displayed as T-bars on top of each other, which become darker colours ofgray with decreasing average profile sizes. For univariate filtering only the smaller profiles of221Univariate filteringfrequencyAB000584AF000573D14662D38305D87292HG2167.HT2237HG2339.HT2435J03473J05068L03840L42379M18728M28713M59499M61853M61916M68840M80482M82809U00115U11862U14391U16799U42408U46692U62317U67963U68019U79288U85193U85707U90911X03635X60708X65614X71348X83573X86163X86809X98311Y00705Z35402Z4819901020304050LassofrequencyAB000584AF000573D14662D38305D87292HG2167.HT2237HG2339.HT2435J03473J05068L03840L42379M18728M28713M59499M61853M61916M68840M80482M82809U00115U11862U14391U16799U42408U46692U62317U67963U68019U79288U85193U85707U90911X03635X60708X65614X71348X83573X86163X86809X98311Y00705Z35402Z4819901020304050Elastic netfrequencyAB000584AF000573D14662D38305D87292HG2167.HT2237HG2339.HT2435J03473J05068L03840L42379M18728M28713M59499M61853M61916M68840M80482M82809U00115U11862U14391U16799U42408U46692U62317U67963U68019U79288U85193U85707U90911X03635X60708X65614X71348X83573X86163X86809X98311Y00705Z35402Z4819901020304050varSelRFfrequencyAB000584AF000573D14662D38305D87292HG2167.HT2237HG2339.HT2435J03473J05068L03840L42379M18728M28713M59499M61853M61916M68840M80482M82809U00115U11862U14391U16799U42408U46692U62317U67963U68019U79288U85193U85707U90911X03635X60708X65614X71348X83573X86163X86809X98311Y00705Z35402Z4819901020304050Figure C.2: Inclusion frequencies for genes selected into at least half of all profiles by any of themethods for at least one tuning parameter value (ovarian cancer data). Frequencies corresponding toeach of the parameter values are illustrated by overlaid T-bars varying from light gray for the largestprofiles to black for the smallest. For example, the bar enclosed by the orange box shows gene X03635being selected in all 50 univariate profiles of size p∗ = 50 (light gray), in 47 profiles with p∗ = 10(darker gray) and in 43 with p∗ = 5 (black).222sizes p∗ ∈ {5, 10, 50} are shown to avoid plot overcrowding.Lasso regression always selects the same five genes into more than half of the m = 50resamples, for all penalty values λ1 > 0.01. The same five genes also get chosen very often intoelastic net profiles. Two of these (M82809 and U11862) are the only genes that get selectedinto more than half of the varSelRF profiles, which are generally very small with a median pro-file size of three. While we observe this good agreement for the multivariate methods, differentgenes are included most frequently by univariate filtering. Only one of the five genes alwaysfound by lasso is also part of more than half of the univariate profiles with p∗ ≤ 50 genes(X65614). This is reflective of the fact that variables get selected independently in the univari-ate filtering approach, while multivariate methods take into account the correlation structurebetween genes. Note that when the profile sizes become larger either by increasing p∗ or bydecreasing the penalty parameter λ1, genes that had been included often in smaller profiles, aregenerally included in the larger profiles as well and rarely get dropped.C.4.3 Profile stabilityIn addition to assessing the frequencies of individual genes, we can use the resampling setup toevaluate the molecular profiles as a whole in terms of their stability. We compute the Jaccardsimilarity indices for all pairs of non-empty gene sets, which are summarised in Figure C.3 bytheir means and standard deviation bars. The similarity values are plotted against the mediangene set sizes to show how the stabilities of profiles constructed by the different classifica-tion methods develop with increasing profile sizes (which are induced by changing the tuningparameter values).In general, the observed Jaccard index distributions of elastic net and lasso are similar.The mean Jaccard values are largest for the smallest profiles which contain less than aboutten genes. They then decline roughly monotonically with decreasing values of the penaltyparameter λ1 (which is equivalent to increasing profile sizes). The similarity values observedfor the molecular profiles from univariate filtering follow a different pattern. They vary lessacross profile sizes and are largest for the very large profiles containing 100 or 500 genes, withthe exception of the AML/karyotype data.Note that this increase in similarity for very large univariate profiles can at least in part223Ovarian cancermedian profile sizeJaccard00.10.30.50.70 10 20 30 40 50 100 500Ovarian cancer (randomised Y)median profile sizeJaccard00.10.30.50.70 10 20 30 40 50 100 500UnivariateLassoElastic netvarSelRFALL/AMLmedian profile sizeJaccard00.10.30.50.70 10 20 30 40 50 100 500Prostate cancermedian profile sizeJaccard00.10.30.50.70 10 20 30 40 50 100 500Breast cancermedian profile sizeJaccard00.10.30.50.70 10 20 30 40 50 100 500AML/karyotypemedian profile sizeJaccard00.10.30.50.70 20 40 60 80 100 120 140 500Figure C.3: Mean Jaccard similarity measures (± standard deviation) plotted against median profilesizes for univariate filtering, lasso regression, elastic net, and random forest with variable selection(varSelRF) for the five data sets and the ovarian cancer data with randomised response (top right).be explained by a spurious effect due to how the resampling study is designed. Because eachof the m = 50 training subsets consists of two-thirds of the complete data sets, the expectedintersection between any two training subsets is considerable: 4/9. Because of the overlap insamples, one expects a certain size of intersection between any two selected gene lists, even incases where the gene expression data have no predictive power for the response of interest atall, for example because the response data have been randomised. This overlap can hence not224be attributed to the classification method’s ability to produce stable profiles for classification.This is illustrated in the top-right plot of Figure C.3, where the classification analyseshave been repeated for the ovarian cancer data set with the response variable having been ran-domised. Here, for small molecular profiles containing up to about 50 genes, the averagesimilarity values between any two profiles are small (mean values ≤ 0.1) relative to the sim-ilarities observed for the non-randomised response data (top left in Figure C.3). But for thelargest profiles with 500 genes the mean similarity value increases to about 0.2, and so most ofthe increase in the original ovarian cancer data without randomised response, that is observedbetween the univariate profiles with p∗ = 50 and p∗ = 500, can be attributed to this effect. Thisanalysis has been repeated with the other data sets as well and the same effect could always beseen. Hence, this effect needs to be accounted for when comparing similarity between differentclassification methods and across varying profile sizes, especially when very large profiles areinvolved. However, we found earlier that the best-performing methods in terms of predictionaccuracy induce sparsity and profiles usually contain less than 50 genes on average. And forthese profiles the effect is small and very similar across methods, posing no big problem.Among the very parsimonious profiles (≤ 5 genes), lasso and elastic net molecular pro-files tend to be equally or more stable than the univariate profiles with respect to our stabilitymeasures. An exception is the AML/karyotype data set where the univariate method has muchlarger similarity values across all profile sizes than all other methods. The random forest withvariable selection is comparable to the penalised likelihood methods. For some applications,the computed similarity indices are very small for all but the smallest profiles, this appliesin particular to the breast cancer data for all methods and to the AML/karyotype data for thepenalised likelihood methods. After accounting for the effect of the resampling study designdescribed above, the remaining stability that can be attributed to the classification method itselfis even smaller.225Univariate filteringmedian profile sizemean absolute correlation00.10.30.50.70 20 40 60 80 100 120 140 500VarSelRFmedian profile sizemean absolute correlation00.10.30.50.70 20 40 60 80 100 120 140 500BreastALL/AMLProstateAML/karyoOvarianLassomedian profile sizemean absolute correlation00.10.30.50.70 20 40 60 80 100 120 140 500Elastic netmedian profile sizemean absolute correlation00.10.30.50.70 20 40 60 80 100 120 140 500Figure C.4: Distributions of means of absolute correlations within profiles (mean± standard deviation)plotted against median profile sizes.We find that the overall stability patterns across the range of tuning parameter values arequite different between data sets. In particular, the similarities between profiles from univariatefiltering vary widely. The profiles are least stable for the breast cancer data and most stablefor the prostate cancer data. These differences in similarity distributions must be due to thedifferences in data structure, for example in the correlation structure between genes which arerelated to the response and get selected into classification profiles. Hence, for each profile wecompute all pairwise correlations between all genes in the profile and record the mean of theabsolute values of these correlations as a summary measure for the strength of correlationsin that profile. The distributions of these mean absolute correlations across all m = 50 re-samples are illustrated by their mean and standard deviations in Figure C.4. As we observedfor the Jaccard index, the relative sizes of these correlations differ between data sets, with thelargest differences again being seen for univariate filtering methods. For all methods, the av-erage correlations are largest for small profile sizes. However when profile sizes increase, thewithin-profile correlations decrease much faster for the multivariate methods than for univariate226filtering.Note that the shapes of the median absolute correlations plotted against median profilesizes in Figure C.4 closely resemble the shapes of median Jaccard similarities against medianprofile sizes in Figure C.3. This suggests that the differences in similarity distributions betweendata sets might indeed be explained by differences in the correlation structures. It also meansthat whenever the averaged pairwise Jaccard similarities are large, i.e. when mostly the samegenes get selected into the profiles, the absolute correlations between these genes tend to belarge as well.One big difference between univariate filtering and multivariate classification methods isthat univariate filtering variables are selected individually without taking the correlation struc-ture between variables into account. Imagine two highly positively correlated variables. Ifthese variables are also highly related to the response, then they would likely both be includedby univariate filtering despite the fact that, given one variable is already included, the otherone does not add much to the explanatory power of the profile and might be quite unnecessary.Contrary to that, the L1-penalty term in the lasso and elastic net methods discourages the in-clusion of both variables together, because the decrease in the likelihood achieved by includingboth is likely to be outweighed by the increase in the penalty term. In a resampling study, oneof the two variables might be selected into most of the resampled lasso and elastic net profiles,but they will rarely be selected together. This affects the Jaccard index for larger profiles, asindeed we have observed earlier. On the other hand, most resampled univariate filtering pro-files will contain both variables, resulting in both a larger within-profile correlation as well as alarger Jaccard similarity measure between the univariate profiles. However, one can argue thattwo highly correlated variables in two different profiles do contribute to the similarity of thesetwo profiles, since they can replace each other without much loss of information.In a first attempt to reflect this in the similarity measurements, we extend the pairwiseJaccard index by adding a term to the numerator that summarises the contributions of genes,which are present in one set but not the other, and which have large correlations with genes ofthe other set. The approach is outlined in Appendix C.B and the results for one possible way ofextending the Jaccard index to incorporate correlation are shown in Figure C.6. We observe thatthe resulting similarity values are increased for all methods and across the range of profile sizes,reflecting the added term. However, the overall patterns look very similar to those illustrated in227Figure C.3 for the original Jaccard index, albeit shifted up and with the decrease in similaritythat was observed with increasing lasso and elastic net profile sizes being slightly less steepthan seen previously. Overall, the effect of including the absolute correlations is similar for allclassification methods and it does not affect the comparisons between methods. We come tothis conclusion for all our data sets, and the results are insensitive to the exact choice of howthe approach is implemented, e.g. choice of threshold value or of which location parameteris employed to summarise the individual gene contributions (see Appendix C.B). A radicallydifferent approach might be needed to adequately reflect highly correlated genes in a measureof similarity between resampled profiles.C.4.4 Validation on independent data setsFor the breast and ovarian cancer data sets, we have independent validation data available whichrepresent populations which are comparable to the original studies in terms of clinical and phe-notypical characteristics and disease outcome. This allows us to assess how well the predictiveabilities of the gene expression profiles translate to new data, that is whether the gene sets wefound earlier using the data sets by Schwartz et al. (2002) and van’t Veer et al. (2002) are stillpredictive for the binary response in new validation data (Lu et al. 2004, van de Vijver et al.2002). Because we are interested in how well the performances of parsimonious molecular pro-files translate, we focus on those classification methods which incorporate variable selection,i.e. univariate filtering, lasso, elastic net and varSelRF. We choose tuning parameter values thatresult in very small profiles of comparable sizes with small misclassification errors observedfor the original data. On average the profiles contain between 5 and 11 genes for the ovariancancer data and between 5 and 18 genes for the breast cancer data.We employ logistic regression models with all genes from the molecular profile of interestincluded as covariates. Note that of the four clinical covariates used in the analysis of the van’tVeer et al. (2002) breast cancer data, only three are available for the validation data (patientage, tumour grade and tumour diameter, but not angioinvasion), so only these three can beincluded here. This potentially compromises the predictive abilities of the molecular profilesand of course reflects a common problem with validation studies.The validation results are listed in Table C.4 which shows the median misclassification228Table C.4: Performance of gene sets found using data sets Schwartz et al. (2002), van’t Veer et al.(2002), when applied in logistic regression models fitted to independent data (Lu et al. 2004, van deVijver et al. 2002). One-sided permutation tests are based on 1000 random sets of the same number ofgenes (significance level 0.1).Method Tuning parameter Median Median Proportionprofile size error p-values≤ 0.1Ovarian cancerBaseline error rate - 0.3810 -Univariate p∗ = 5 5 0.2143 32/50Lasso λ1 = 4.472 7 0.1667 33/50Elastic Net λ2 = 1, λ1 = 3.986 11 0.1190 21/50varSelRF - 3 0.2143 31/50Breast cancerBaseline error rate - 0.1609 -Univariate p∗ = 5 5 0.1379 50/50Lasso λ1 = 4.472 13 0.1034 45/50Elastic Net λ2 = 1, λ1 = 7.953 18 0.0805 29/50varSelRF - 14 0.1092 43/50rates across all profiles derived from the 50 training subsets of the Schwartz et al. (2002) andvan’t Veer et al. (2002) data, respectively. In order to assess whether the predictive accuraciesachieved by the molecular profiles are better than expected of randomly produced gene sets,one-sided permutation tests are performed. This is done by randomly sampling from the datawith replacement 1000 sets of the same number of k genes as contained in the real profileand comparing the error rates observed for the real profiles with the distributions of error ratesobtained for the random gene sets. We report the proportion among the m = 50 profiles foreach method and data set that have significantly low misclassification rates, i.e. where the errorrates for the real profiles are smaller than the 10%-quantiles of the random distributions.For both data sets, the median error rates are always smaller than the baseline error, whichis the minimum misclassification error achievable if the gene expression data were not taken229into account. For ovarian cancer this is the proportion of samples that would be misclassifiedif all samples were simply assigned to the most frequent class. Since for the breast cancer dataadditional clinical data are available, the baseline error rate is the error achieved by a logisticregression model only containing the clinical covariates.In both examples, the univariate filtering approach results in the highest misclassificationerror rates. The elastic net performs best in the sense that it provides the smallest misclassifica-tion rates in both applications. However, this could be linked to the larger sizes of these profiles- the elastic net profiles have on average slightly more genes than those of the other methods.This is supported by the fact that in terms of the proportion of results which are significantlybetter than expected at random, the elastic net performs less well than the other methods. Only21 (ovarian cancer) and 29 (breast cancer) out of all 50 results are significant for the elastic net,while for the other three methods the proportions of significant results range between 31/50and 33/50 for the ovarian cancer application and are even higher for the breast cancer dataranging from 43/50 to 50/50.In summary, these results show that it is possible to generate molecular profiles for binaryclassification, such that the predictive abilities translate well to new data. It is hard to come toa conclusion on which classification method performs best on a new data set based on our twoexamples only.C.5 DiscussionIt has been pointed out (e.g. Ein-Dor et al. 2005, Michiels et al. 2005) that molecular profilesderived from gene expression microarray data can be highly unstable, i.e. which genes getselected into a profile depends much on the choice of training data. Hence, there is need forcareful validation of results (Simon et al. 2003, Dupuy and Simon 2007) and it is important toassess the uncertainty associated with molecular profiles. To do this we employed a resamplingapproach, and compared important characteristics of gene expression profiles derived usingseveral univariate and multivariate methods for binary classification. We applied these methodsto five publicly available gene expression microarray data sets. In particular, we compared theclassification methods in terms of the predictive accuracy of resulting gene expression profilesand how well the predictive ability translates to new data, as well as profile stability and how230parsimonious the profiles are.Results vary between the different data sets and depend much on the data structure, e.g.the correlation structure between those genes which are related to the response variable andthat get selected into the molecular profiles. But for all data sets, the best predicting geneexpression profiles are small (between 3 and 80 genes). In terms of predictive ability, weobserve comparable performances between those methods that incorporate variable selection,in particular univariate filtering, lasso and elastic net. In contrast, the methods that employ mostor all genes for classification, i.e. ridge regression and random forest, often performed worse,sometimes substantially. This reflects the p >> n situation of gene expression data, but alsoconforms with the idea that usually only a small number of genes are expected to influence anyparticular biological condition or disease.It is particularly interesting to note that the prediction performance of simple univariatefiltering methods is comparable to that of more complex multivariate methods, even thoughthey do not take the correlation structure of gene expression data into account. This has alsobeen observed in a recent study by Lai et al. (2006). An explanation for this is the small samplesize in most available microarray data sets, due to which the correlation structure in the datacannot be estimated accurately enough, so that multivariate methods cannot profit sufficientlyfrom the correlation structure.Note, however, that if the response data to be fitted is continuous (censored or not cen-sored) rather than binary, then the sample size needed to give multivariate methods an edgeover univariate methods can be smaller. This is because a vector of continuous data containsmore information than a binary vector of the same length, which makes a perfect model fitharder to achieve. In that situation, methods which can use more information, in particular thecorrelation structure between covariates, gain more of an advantage than in the binary clas-sification situation. In a recent study comparing several methods applied to Cox proportionalhazards models for predicting survival from microarray data (Bøvelstad et al. 2007), the authorsfound that all multivariate methods performed clearly better than univariate approaches.The stability of the molecular profiles was assessed by the distribution of pairwise similar-ity between the profiles resulting from the resampled training sets. Similarity was measured bythe Jaccard index and by an extended Jaccard index that accommodates for highly correlatedgene variables in different profiles. We found both indices to lead to similar results in terms of231comparisons between classification methods. In all data sets, for the multivariate methods lassoand elastic net, the stability depends much on the number of genes in the molecular profiles anddecreases with increasing profile sizes. The stabilities observed for univariate filtering profiles,on the other hand, are not influenced much by an increase in the number of genes included inthe profiles. For very parsimonious profiles with p∗ ≤ 5 genes, both lasso regression and elasticnet are found to be more stable than the univariate methods, except in one data set. The verysmall profiles are often those we are most interested in, because of their good interpretabil-ity and because we found that generally small profiles perform better in terms of predictionaccuracy.For two of the data sets independent data were available for validation. We applied parsi-monious gene expression profiles constructed with the original data (using those classificationmethods which incorporate variable selection) to the new data, using the genes as covariatesin logistic regression models. We found that all profiles translated reasonably well in termsof their predictive accuracy achieved on both new data sets. Note that the question whetherthe predictor accuracy of existing profiles translates to new data is different from the questionwhether the same genes would be found in a new analysis on the new data as has been pointedout e.g. by Somorjai et al. (2003), Roepman et al. (2006) and Simon (2006). This will gen-erally not be the case, in part due to the correlated nature of gene expression data because ofunderlying biological processes and also due to the “large p, small n” nature of microarray datawhich implies multicollinearity and non-uniqueness of solutions.In this context it is often noted that there is a difference in the use of gene expressionprofiles as prognostic factors to predict disease outcome etc. in new samples and their usefor detecting genes which are causal to the disease. Nonetheless, gene expression profilesare often used as a starting point for the exploration of causality and underlying biologicalprocesses. In such a situation, parsimony and stability of profiles are important properties,because concise and stable gene lists are easier to interpret and provide a more clear-cut startingpoint for further exploration than e.g. a list of several hundred genes where the distinctionbetween their individual contributions is less clear. Also, for the direct use of gene expressionprofiles as prognostic factors, small profiles consisting of a few genes only are much cheaperand easier to apply on a large scale than large gene lists. In addition, the expression of a smallnumber of genes can readily and accurately be tested by using, for example, quantitative real232time PCR rather than high-throughput microarrays, thus improving the signal-to-noise ratiowhile also being more cost effective.In summary, we find that binary classification methods which produce parsimonious geneexpression profiles generally result in profiles which have better prediction accuracy than meth-ods which do not include variable selection. Multivariate sparse penalised likelihood methodslike the lasso and elastic net might have a slight edge compared to univariate filtering in termsof prediction performance and how well the predictive ability of profiles translates to new data,although the differences are not large. Their performance is likely to improve further relative tounivariate methods when sample sizes increase in the future. For very small molecular profilescontaining only 5 genes or less, the sparse penalised likelihood methods have an additional ad-vantage, as they tend to produce profiles which are more stable than univariate filtering profileswhile maintaining similar or better predictive performance.233C.6 AppendixC.A Similarity indices: results for Schwartz et al. (2002)Ovarian cancermedian profile sizeJaccard00.20.40.60.810 10 20 30 40 50 100 500median profile sizeDice−Sorensen00.20.40.60.810 10 20 30 40 50 100 500median profile sizeOchiai00.20.40.60.810 10 20 30 40 50 100 500Figure C.5: Mean values of similarity measures (± standard deviation) plotted against median profilesizes for univariate filtering, lasso regression, elastic net, and random forest with variable selection(varSelRF) (for legend see Figure C.4).234C.B Extension of Jaccard index to incorporate correlationGene expression data often contain gene variables which are highly correlated, for examplebecause the genes are co-regulated or act together in the same biological pathway. Supposegene i with expression xi is present in profile Z1 but not in profile Z2. Assume further that thisgene i is highly correlated with gene j with expression xj , which is present in profile Z2 butnot in Z1. In this situation one can argue that genes i and j contribute to the similarity of themolecular profiles Z1 and Z2 with a weight that is proportional to their (absolute) correlationRxi,xj . In order to illustrate the possible effect this could have on the similarity index, wedefine the contribution of a gene, which is present in Z1 but not in Z2, to the similarity by its(thresholded) mean absolute correlation with all genes which are in profile Z2 but not in Z1. Inorder to reduce the influence of spurious small correlations, only absolute correlations largerthan a threshold t are taken into consideration, which we take to be data dependent and set tothe median absolute correlation between genes in Z1 and Z2:t = mediani6=k;i,k∈Z1∪Z2 (Rxi,xk). (C.8)Using the above approach of weighted contributions of gene correlations to the similarity be-tween molecular profiles, we define the extended Jaccard measure as follows:ρc(Z1, Z2) =#(Z1 ∩ Z2) + C#(Z1 ∪ Z2) (C.9)whereC =∑i∈Z11#Z2∑j∈Z2,j 6∈Z1Rxi,xjI(Rxi,xj > t)+∑j∈Z21#Z1∑i∈Z1,i6∈Z2Rxi,xjI(Rxi,xj > t), (C.10)with I(Rxi,xj > t) being the indicator functionI(Rxi,xj > t) =1, if Rxi,xj > t0, otherwise.(C.11)Of course, there are many alternative ways how the threshold could be chosen instead andthe sensitivity of the results to these choices needs to be investigated. We performed sen-sitivity analyses on the data sets presented in Table C.3. We used quantiles other than themedian as a threshold with values ranging from the 10% to the 90% quantiles, and also ap-plied the mean rather than the median. In addition, data-independent constants were used235(t ∈ {0.25, 0.5, 0.75}). In conclusion, the choice of the threshold did not influence the resultsmuch as long as t was not too close to zero. We also replaced the thresholded mean abso-lute correlations by equivalent thresholded median absolute correlations, which had very littleimpact on the results.Ovarian cancermedian profile sizeJaccard (correlation)00.20.40.60.810 10 20 30 40 50 100 5005 10 50 100 50014.144.471.410.450.141.62.546.31015.9ALL/AMLmedian profile sizeJaccard (correlation)00.20.40.60.810 10 20 30 40 50 100 5005 10 50 100 5004.471.410.4514.140.141.62.546.310Prostate cancermedian profile sizeJaccard (correlation)00.20.40.60.810 10 20 30 40 50 100 50010 50 100 500514.144.471.41 0.140.4523.25812.620Breast cancermedian profile sizeJaccard (correlation)00.20.40.60.810 10 20 30 40 50 100 5005 10 5010050014.144.471.41 0.450.145.66.37.188.93.644.551011.212.614.1AML/karyotypemedian profile sizeJaccard (correlation)00.20.40.60.810 20 40 60 80 100 120 140 5005105010050014.144.47 1.41 0.140.4523.25812.620Figure C.6: Mean correlation-extended Jaccard similarity measures (± standard deviation) as definedabove plotted against median profile sizes for univariate filtering, lasso regression, elastic net, and ran-dom forest with variable selection (varSelRF) (for legend see Figure C.4). The point labels indicate thecorresponding tuning parameter values.236",
    "id": 1587377,
    "identifiers": {
        "doi": "10.25560/4397",
        "oai": "oai:spiral.imperial.ac.uk:10044/1/4397"
    },
    "title": "Multivariate Analysis of Tumour Gene Expression Profiles Applying Regularisation and Bayesian Variable Selection Techniques",
    "language": {
        "code": "en",
        "name": "English"
    },
    "publishedDate": "2009-03-01T00:00:00+00:00",
    "publisher": "Epidemiology and Public Health, Imperial College London",
    "references": [],
    "sourceFulltextUrls": [
        "http://spiral.imperial.ac.uk/bitstream/10044/1/4397/1/Zucknick-M-2009-PhD-Thesis.pdf"
    ],
    "updatedDate": "",
    "yearPublished": "2009",
    "links": [
        {
            "type": "download",
            "url": "https://core.ac.uk/download/1587377.pdf"
        },
        {
            "type": "reader",
            "url": "https://core.ac.uk/reader/1587377"
        },
        {
            "type": "thumbnail_m",
            "url": "https://core.ac.uk/image/1587377/medium"
        },
        {
            "type": "thumbnail_l",
            "url": "https://core.ac.uk/image/1587377/large"
        },
        {
            "type": "display",
            "url": "https://core.ac.uk/outputs/1587377"
        }
    ],
    "abstract": "High-throughput microarray technology is here to stay, e.g. in oncology for tumour classification\r\nand gene expression profiling to predict cancer pathology and clinical outcome. The global\r\nobjective of this thesis is to investigate multivariate methods that are suitable for this task.\r\nAfter introducing the problem and the biological background, an overview of multivariate\r\nregularisation methods is given in Chapter 3 and the binary classification problem is outlined\r\n(Chapter 4). The focus of applications presented in Chapters 5 to 7 is on sparse binary classifiers\r\nthat are both parsimonious and interpretable. Particular emphasis is on sparse penalised\r\nlikelihood and Bayesian variable selection models, all in the context of logistic regression. The\r\nthesis concludes with a final discussion chapter.\r\nThe variable selection problem is particularly challenging here, since the number of variables\r\nis much larger than the sample size, which results in an ill-conditioned problem with\r\nmany equally good solutions. Thus, one open problem is the stability of gene expression profiles.\r\nIn a resampling study, various characteristics including stability are compared between a\r\nvariety of classifiers applied to five gene expression data sets and validated on two independent\r\ndata sets.\r\nBayesian variable selection provides an alternative to resampling for estimating the uncertainty\r\nin the selection of genes. MCMC methods are used for model space exploration, but\r\nbecause of the high dimensionality standard algorithms are computationally expensive and/or\r\nresult in poor Markov chain mixing. A novel MCMC algorithm is presented that uses the\r\ndependence structure between input variables for finding blocks of variables to be updated together.\r\nThis drastically improves mixing while keeping the computational burden acceptable.\r\nSeveral algorithms are compared in a simulation study. In an ovarian cancer application in\r\nChapter 7, the best-performing MCMC algorithms are combined with parallel tempering and\r\ncompared with an alternative method",
    "tags": [
        "Thesis or dissertation",
        "Doctoral",
        "Doctor of Philosophy (PhD)"
    ],
    "fulltextStatus": "enabled",
    "subjects": [
        "Thesis or dissertation",
        "Doctoral",
        "Doctor of Philosophy (PhD)"
    ],
    "oai": "oai:spiral.imperial.ac.uk:10044/1/4397",
    "deleted": "ALLOWED",
    "disabled": false,
    "journals": null,
    "repositories": {
        "id": "105",
        "openDoarId": 0,
        "name": "Spiral - Imperial College Digital Repository",
        "urlHomepage": null,
        "uriJournals": null,
        "physicalName": "noname",
        "roarId": 0,
        "baseId": 0,
        "pdfStatus": null,
        "nrUpdates": 0,
        "lastUpdateTime": null
    },
    "repositoryDocument": {
        "id": 1587377,
        "depositedDate": null,
        "publishedDate": "2009-03-01T00:00:00+00:00",
        "updatedDate": "2024-01-08T06:21:23+00:00",
        "acceptedDate": null,
        "createdDate": "2012-04-12T16:31:24+01:00"
    },
    "urls": [
        "http://hdl.handle.net/10044/1/4397",
        "https://doi.org/10.25560/4397"
    ],
    "lastUpdate": "2024-01-08T06:21:23+00:00",
    "setSpecs": []
}