{
    "acceptedDate": "",
    "authors": [
        {
            "name": "Davies, Vinny"
        },
        {
            "name": "Reeve, Richard"
        },
        {
            "name": "Harvey, William"
        },
        {
            "name": "Maree, Francois"
        },
        {
            "name": "Husmeier, Dirk"
        }
    ],
    "contributors": [],
    "createdDate": "2014-05-01T18:02:23+01:00",
    "dataProvider": {
        "id": 42,
        "name": "Enlighten",
        "url": "https://api.core.ac.uk/v3/data-providers/42",
        "logo": "https://api.core.ac.uk/data-providers/42/logo"
    },
    "depositedDate": "",
    "documentType": "research",
    "doi": "",
    "downloadUrl": "https://core.ac.uk/download/20114855.pdf",
    "fullText": "    Davies, Vinny, Reeve, Richard, Harvey, William, Maree, Francois, and Husmeier, Dirk (2014) Sparse Bayesian Variable Selection for the Identiﬁcation of Antigenic Variability in the Foot-and-Mouth Disease Virus. Journal of Machine Learning Research: Workshop and Conference Proceedings, 33. pp. 149-158. ISSN 1938-7228   Copyright © 2014 The Authors    http://eprints.gla.ac.uk/93188    Deposited on:  28 April 2014                         Enlighten – Research publications by members of the University of Glasgow http://eprints.gla.ac.uk Sparse Bayesian Variable Selection for the Identification of AntigenicVariability in the Foot-and-Mouth Disease VirusVinny Davies Richard Reeve William HarveySchool of Mathematics and Statistics,University of Glasgow, UKv.davies.1@research.gla.ac.ukBoyd Orr Centre for Population and Ecosystem Health, andInstitute of Biodiversity, Animal Health and Comparative Medicine,University of Glasgow, UKFrancois F. Maree Dirk HusmeierTransboundary Animal Diseases Programme,Onderstepoort Veterinary Institute, SASchool of Mathematics and Statistics,University of Glasgow, UKAbstractVaccines created from closely related virusesare vital for offering protection against newlyemerging strains. For Foot-and-Mouth dis-ease virus (FMDV), where multiple serotypesco-circulate, testing large numbers of vac-cines can be infeasible. Therefore the de-velopment of an in silico predictor of cross-protection between strains is important tohelp optimise vaccine choice. Here we de-scribe a novel sparse Bayesian variable selec-tion model using spike and slab priors whichis able to predict antigenic variability andidentify sites which are important for the neu-tralisation of the virus. We are able to iden-tify multiple residues which are known to bekey indicators of antigenic variability. Manyof these were not identified previously us-ing Frequentist mixed-effects models and stillcannot be found when an `1 penalty is used.We further explore how the Markov chainMonte Carlo (MCMC) proposal method forthe inclusion of variables can offer signif-icant reductions in computational require-ments, both for spike and slab priors in gen-eral, and our hierarchical Bayesian model inparticular.1 INTRODUCTIONWith the continual emergence of new virus strains,the need to produce effective vaccines has become evermore vital. Predicting where past exposure to a closelyrelated virus strain can offer protection is an importantAppearing in Proceedings of the 17th International Con-ference on Artificial Intelligence and Statistics (AISTATS)2014, Reykjavik, Iceland. JMLR: W&CP volume 33. Copy-right 2014 by the authors.field of research, as testing large numbers of vaccinescan be time consuming and expensive. In particu-lar for Foot-and-Mouth Disease Virus (FMDV), wherea variety of virus strains co-circulate, understandingcross-protection is vital for predicting the severity ofan outbreak and understanding how different vaccinestrains will mitigate the spread of the disease. As thetesting of new candidate vaccines is expensive, the de-velopment of an in silico predictor that can identifywhich strains are likely to give the broadest cross-protection is essential.Reeve et al. (2010) used mixed-effects models to ac-count for the variation in virus neutralisation (VN)titre, an in vitro measure of antigenic variability; theextent to which one strain confers protection on theother. They identified a specific residue at which sub-stitutions had caused a drop in antigenic variability.Their results have been backed up experimentally inGrazioli et al. (2006).To achieve this, the authors corrected for the fact thatviruses with similar evolutionary paths are likely to bemore closely related. They did this by accounting forthe shared evolutionary history of the virus strains.Through including the branches of the phylogenetictree as explanatory variables in the model, they wereable to account for the similarities within individualtopotypes, groups of genetically similar viruses thathave been isolated for long periods, within the tree.One of the main weaknesses of the models of Reeveet al. (2010) was their reliance on stepwise regressiontechniques. The method used, forward inclusion usingthe Holm-Bonferroni correction (Holm 1979), does notexplore all variable configurations and can result in anon-optimal solution. A potential extension to thisis the Least Absolute Shrinkage and Selection Oper-ator (LASSO) of Tibshirani (1996), which uses an `1penalty for simultaneous variable selection. This workhas recently been extended to mixed-effects models bySparse Bayesian Variable Selection for the Identification of Antigenic Variability in FMDVSchelldorfer et al. (2011).A drawback of the classical LASSO, and its closecousin, the Elastic Net (Zou & Hastie 2005), is theway regularisation parameters are selected. Informa-tion criteria, such as the Akaike information criterion(AIC), corrected AIC (AICc) (Hurvich & Tsai 1989)and Bayesian information criterion (BIC), are asymp-totically justified, but can have poor small-sample per-formance. Alternative methods include hold-out cross-validation, which reduces the training set size, and k-fold or leave-one-out cross validation, which are po-tentially biased (Bengio & Grandvalet 2004, Rao et al.2008). The sub-optimality of these methods comparedto a Bayesian approach has been reported before (Dal-ton & Dougherty 2012).A more serious drawback is the `1 regularisation termitself, which in a Bayesian context corresponds to aLaplace prior (Park & Casella 2008). This choice iscomputationally efficient, leading to a convex opti-misation problem for penalised maximum likelihoodor Bayesian maximum a posteriori (MAP) inference.However, `1 regularisation combines the problems ofinsufficient sparsity of selection with increased biascaused by shrinkage, as discussed in detail in Chap-ter 13 of Murphy (2012). A preferred alternative,which improves variable selection and avoids exces-sive shrinkage, is the spike and slab prior proposed inMitchell & Beauchamp (1988), which, however, leadsto a non-convex optimisation problem in a penalisedlikelihood context. In the present work, we integratethe spike and slab prior into the context of hierarchi-cal Bayesian models, whose advantages have been dis-cussed on various occasions elsewhere; see e.g. Gelmanet al. (2004). In particular, Bayesian hierarchical mod-els allow consistent inference of all parameters and hy-perparameters, and inference borrows strength by thesystematic sharing and combination of information.The model we propose is designed specifically to dealwith the various aspects of the FMDV data. The un-derstanding of antigenic variability requires a focus onselecting variables, while still taking into account theexperimental condition under which the data was gath-ered. To do this we propose a novel Bayesian hierarchi-cal model into which we integrate the prior originallyproposed in Mitchell & Beauchamp (1988). In partic-ular we allow for confounding experimental variationthrough the specification of random effects. We alsospecify an additional layer in the hierarchical modelin order to allow the mean of the coefficients to vary.This comes from biological knowledge of the problem,where we expect a high intercept and a negative im-pact on the responses from the inclusion of additionalvariables.In this paper we evaluate the advantages of our modelover the use of both the standard and `1 penalisedmixed-effects model. We use simulated data with ran-dom effects designed to mimic variation in experimen-tal conditions to objectively assess prediction and vari-able selection performance. Finally the model is usedon the real life data set of Reeve et al. (2010) in or-der to assess its capability in identifying surface ex-posed residues at which substitutions are known tocause a significant drop in antigenic variability. Thisis done by accounting for the evolutionary history of astrain through including branches of the phylogenetictree that divide antigenically distinct groups withinthe virus.2 CLASSICAL METHODSBefore we describe the novel Bayesian model, we re-view established classical methods, which are morecommonly used within the biological community.2.1 Classical Mixed-Effects ModelIn classical mixed-effects models we define the responsey = (y1, . . . , yN )> and denote the explanatory vari-ables, X, as a matrix of J + 1 columns and N rows,where the first column is an intercept. Each columnof explanatory variables, Xj , is then given an associ-ated regression coefficient, wj , to control its influenceon the response.We further set Z as the matrix of indicators withN rows and k ∈ {1, . . . , ||b||} columns. b =(b>1 , . . . ,b>G)> represents a vector of parameters re-lated to each of the groups g ∈ {1, . . . , G}, where eachbg has length ||bg|| and ||b|| =∑Gg=1 ||bg||. For moredetails on mixed-effects models see Pinheiro & Bates(2000).The model is therefore defined as:y = Xw + Zb + ε (1)where we assume that ε ∼ N (ε|0,Σε) and b ∼N (b|0,Σb), where we define Σε = σ2εI and Σb =diag(([σ2b,1]>, . . . , [σ2b,G]>)>) for notational simplicityand I is the identity matrix. Integrating over b givesus the likelihood:L(w,Σε,Σb|y,X,Z) = N (y|Xw,ZΣbZ> +Σε) (2)In classical mixed-effects models, model comparisontechniques must be used to choose which variables areincluded within the model. To get a sparse model,Reeve et al. (2010) used forward inclusion, makingan adjustment for multiple testing using the Holm-Bonferroni correction. They firstly included the vari-ables correcting for the shared evolutionary pathsusing prior biological knowledge, before adding theresidue data.Vinny Davies, Richard Reeve, William Harvey, Francois F. Maree, Dirk Husmeier2.2 LASSOA classical alternative to forward variable selection isthe LASSO of Tibshirani (1996, 2011), which allows forsimultaneous variable selection. In the simplest caseof linear regression, this gives the following parameterestimates:wˆ = argminw{(y−Xw)2 + λJ∑j=1|wj |}. (3)This is a convex optimisation problem, for which a va-riety of fast and effective algorithms exist (e.g. Hastieet al. (2009)). The effect of eq. 3 is to simultaneouslyshrink and prune parameters w, thereby promoting asparse model. The degree of sparsity depends on theregularization parameter λ, which can be optimisedwith cross-validation or information criteria, e.g. BIC.A recent extension of the standard LASSO is themixed-effects LASSO proposed by Schelldorfer et al.(2011), who estimate the regression coefficientsw, ran-dom effect variance σ2b and the variance of the noiseσ2ε as:(wˆ,σˆ2b , σˆ2ε) = argminw,σ2b>0,σ2ε>0{12 log |V|+ 12 (y−Xw)V−1(y−Xw) + λJ∑j=1|wj |}(4)where V = ZΣbZ>+σεI. In the package of Schelldor-fer et al. (2011), this is minimised using a block coordi-nate gradient descent scheme. To select the value of λwe test the use of BIC, as recommended in Schelldorferet al. (2011), and AICc (Hurvich & Tsai 1989).We point out that the mixed effects Lasso of Schelldor-fer et al. (2011) has only been developed for a singlerandom effect. To deal with multiple random effects,the Cartesian product of several random effects has tobe mapped onto a single random effect, which can leadto excessive model complexity. We also note that tothe best of our knowledge, a mixed-effects model ver-sion of the Elastic net (Zou & Hastie 2005) has not yetbeen developed.3 NOVEL BAYESIAN METHODTo perform variable selection within Bayesian statis-tics, we must firstly define the model that is used. Thisis usually done by constructing the posterior distribu-tion using Bayes’ rule:p(γ|D,θ′) ∝ p(γ)p(D,θ′|γ). (5)We then sample the parameters using Markov chainMonte Carlo (MCMC), where we are interested in γ,a vector of latent indicators of whether a variable isincluded in the regression model. Each parameter issampled subject to the data, D, and the other modelparameters, θ′.3.1 LikelihoodThe likelihood for our Bayesian variable selectionmodel is similar to the classical mixed-effects modeldescribed in Section 2.1. However instead of includingall the variables, X, and their corresponding regressioncoefficient, we now only include relevant variables, Xγ ,and regressors, wγ :p(y|wγ ,b, σ2ε) = N (y|Xγwγ + Zb,Σε) (6)The relevance of variable j is determined by γj ∈{0, 1}, where feature j is said to be relevant if γj = 1.This gives γ = (γ0, γ1, . . . , γJ)> ∈ {0, 1}J whereγ0 = 1 is fixed meaning that there is always an inter-cept in the model. We then defineXγ to be the matrixof relevant explanatory variables with ||γ|| columnsand N rows, where ||γ|| = ∑Jj=0 γj , the number ofnon-zero elements of γ. Similarly wγ is given as thecolumn vector of regressors, where the inclusion of eachparameter is again dependent on γ.3.2 PriorsFor computational convenience, conjugate priors havebeen chosen where possible. In this manner, as in clas-sical mixed-effects models, we choose each bk,g to havegroup dependent Gaussian priors:bk,g ∼ N (bk,g|µb,g, σ2b,g). (7)We define this to have a fixed mean µb,g = 0 anda common variance parameter for each random effectgroup g. Further to this, we put a conjugate Inverse-Gamma prior on each σ2b,g:σ2b,g ∼ IG(σ2b,g|αb,g, βb,g) (8)where αb,g and βb,g are fixed hyper-parameters for eachg. This gives the model the flexibility to learn each σ2b,ginstead of predefining their values.The prior for wγ is set in the manner proposed inMitchell & Beauchamp (1988) such that it reflectswhether a feature is relevant. In this way we expectthat wj = 0 if γj = 0, i.e. the feature is irrelevant, andconversely it should be non-zero if the variable is rele-vant, wj 6= 0 if γi = 1. The variables are then dividedinto related groups h ∈ {1, . . . ,H}, in this case two:the intercept and the covariates. A conjugate prior isSparse Bayesian Variable Selection for the Identification of Antigenic Variability in FMDVFigure 1: Compact representation of the complex spikeand slab model as a Directed Acyclic Graph (DAG).The grey circles refer to data and hyper-parameterswhich are fixed, while the white circles refer to param-eters that are inferred with MCMC.chosen when the feature is relevant:p(wj,h|γj,h,µw,h, σ2w,h)={δ0(wj,h) if γj = 0N (wj,h|µw,h, σ2w,h) if γj = 1. (9)where δ0 is the delta function. Here we have a spikeat the mean, µw,h, and as σ2w,h →∞ the distribution,p(wj,h|γj = 1), approaches a uniform distribution, aslab of constant height. For this reason, these modelsare often known as spike and slab models. Similar non-singular versions of this prior are also possible (George& McCulloch 1993, 1997).Through giving each group h a separate hyper-parameter σ2w,h in eq. 9, we leave the model open topenalising the groups of variables to different degreesthrough the priors:σ2w,h ∼ IG(σ2w,h|αw,h, βw,h). (10)By choosing the same fixed hyper-parameters, αw,hand βw,h for each h, we lose information coupling be-tween the different groups, although this could be re-gained with an addition layer in the hierarchical model.In addition to σ2w,h, we use the hyper-parameters µw,hto reflect the likely non-zero means of each group h:µw,h ∼ N (µw,h|µ0,h,Σ0,h) (11)where the hyper-parameters µ0,h and Σ0,h are fixed.This specification comes from the expected biologi-cal values of each regression coefficients wj,h. In theFMDV data we are likely to observe a comparativelylarge intercept with negative regression coefficients forthe variables. This is a result of amino acid changesdecreasing the similarity between virus strains andtherefore reducing the measured VN titre. Similarly,traversing a significant branch of the phylogenetic treeis likely to cause differences between the strains.As with the classical mixed-effects model in Sec-tion 2.1, we assume the errors are independent andidentically distributed. Again specifying a conjugateprior gives us an Inverse-Gamma distribution:σ2ε ∼ IG(σ2ε |αε, βε). (12)where the hyper-parameters αε and βε are fixed.A prior must also be given for γ1:J , the parameterswhich determine the relevance of the variables:p(γ1:J |pi) =J∏j=1Bern(γj |pi) (13)where pi is the probability of the individual variablebeing relevant.The value of pi can either be set as a fixed hyper-parameter as in Sabatti & James (2005), where theyargue that it should be determined by underlyingknowledge of the problem. Alternatively it can begiven a conjugate Beta prior:pi ∼ B(pi|αpi, βpi). (14)as in this case, where the likely number of relevantvariables cannot be easily specified a priori. This isa more general model, which subsumes a fixed pi as alimiting case for αpiβpi/((αpi +βpi)2(αpi +βpi + 1))→ 0.3.3 PosteriorUsing Bayes’ theorem we can construct the poste-rior distribution for the inferred parameters θ1 =(γ,wγ ,b,σb,µw,σw, σ2ε , pi) given the fixed param-eters θ2 = (αb,βb,αw,βw,µ0,Σ0, αε, βε, αpi, βpi),where we define θ = (θ1,θ2). This combines the like-lihood and priors specified, as shown in Figure 1:p(θ1|y,X,Z,θ2) ∝N (y|Xγwγ + Zb,Σε)N (b|0,Σb)IG(σ2ε |αε, βε)×N (wγ |µw,Σw)G∏g=1{IG(σ2b,g|αb,g, βb,g)}×H∏h=1{IG(σ2w,h|αw,h, βw,h)N (µw,h|µ0,h,Σ0,h)}×J∏i=1{Bern(γj |pi)}B(pi|αpi, βpi) (15)Vinny Davies, Richard Reeve, William Harvey, Francois F. Maree, Dirk Husmeierwhere Σb = diag(([σ2b,1]>, . . . , [σ2b,G]>)>) andeach σ2b,g has length ||σ2b,g||. We similarly setΣwγ = diag(([σ2w,1]>, . . . , [σ2w,H ]>)>) and µw =(µ>w,1, . . . ,µ>w,H)> where each σ2w,h and µw,h havelength ||σ2w,h|| = ||µw,h||.3.4 Posterior InferenceIn order to explore the posterior distribution of theparameters we use an MCMC algorithm. Having cho-sen conjugate priors where possible means we can runa Gibbs sampler for the majority of parameters (Rip-ley 1979, Geman & Geman 1984). The only excep-tion is γ, although it is possible to use component-wise Gibbs sampling with a small adaptation; see Sec-tion 3.5.1. The conditional distributions for those pa-rameters amenable to standard Gibbs sampling are:wγ |θ′ ∼ N (wγ |VwγX>γΣ−1ε (y− Zb)+VwγΣ−1wγµw,Vwγ ) (16)b|θ′ ∼ N (b|VbZ>Σ−1ε (y−Xγwγ),Vb) (17)σ2b,g|θ′ ∼ IG(σ2b,g| ||bg||/2 + αb,g, βb,g + 12b>g bg) (18)µw,h|θ′ ∼ N (µw,h|Σ−1µ (Σ−1w wγ,h +Σ−10 µ0,h),Σµ) (19)σ2w,h|θ′ ∼ IG(σ2w,h| ||wγ,h||/2 + αw,h, βw,h+12 (wγ,h − µγ,h)>(wγ,h − µγ,h)) (20)σ2ε |θ′ ∼ IG(σ2ε |N/2 + αε, βε + 12 (y−Xγwγ− Zb)>(y−Xγwγ − Zb)) (21)pi|θ′ ∼ B(pi|αpi + ||γ1:N ||, βpi + J − ||γ1:N ||) (22)where we sample σ2b,g, µw,h and σ2w,h for each g andh respectively. We also define Vwγ = (X>γΣ−1ε Xγ +Σ−1w )−1,Vb = (Z>Σ−1ε Z+Σ−1b )−1 andΣµ = (Σ−1w,h+Σ−10,h)−1 for notational simplicity.Sampling γ is more difficult, as it does not naturallyform a standard distribution. Methods for achievingthis are discussed in more detail in Section 3.5, how-ever in order to do this we need a conditional distri-bution:p(γ|θ′) ∝ p(γ1:J |pi)∫N (y|Xγwγ + Zb,Σε)N (wγ |µw,Σw)dwγ (23)∝ pi||γ1:N ||(1− pi)J−||γ1:N ||N (y|Xγµw + Zb,Σε + XγΣwX>γ ) (24)where there are J variables. Here we have used a col-lapsing step as in Sabatti & James (2005), integratingout wγ through the application of standard Gaussianintegrals (Bishop 2006) to reduce the computationalrequirements. The normalisation constant is not re-quired in eq. 24 as it cancels out in all of the methodsdiscussed in Section 3.5: eq. 26 and eq. 29.3.5 Sampling the Latent IndicatorsMultiple methods have been proposed for sampling thelatent variables, γ. In this paper we look at two ofthese in particular; the component-wise Gibbs sam-pling approach of George & McCulloch (1993) andthrough a Metropolis-Hastings step where we can pro-pose changes to multiple parameters simultaneouslyfor a computational improvement (Metropolis et al.1953, Hastings 1970).3.5.1 Component-wise Gibbs SamplingFollowing George & McCulloch (1993) we can use acomponent-wise Gibbs sampler to consecutively sam-ple each γj from γ in a random order. To do this wefirst define a conditional distribution for γij , the valueof the ith iteration of γj , from eq. 24:γij ∼ p(γij |pi,b, σ2ε ,µb,Σw,γi−j) (25)where γi−j = (γi1, . . . , γij−1, γi−1j+1, . . . , γi−1J ) in the caseof ordered inclusion parameters. Each distribution canthen be given a Bernoulli distribution with probability:P (γij = 1|pi,b, σ2ε ,µb,Σw,γi−j) =aa+ b(26)where we define:a = p(γij = 1|pi,b, σ2ε ,µb,Σw,γi−j) (27)b = p(γij = 0|pi,b, σ2ε ,µb,Σw,γi−j). (28)3.5.2 Metropolis-Hastings SamplingUnlike with the Gibbs sampling approach, samplingvia a Metropolis-Hastings step leads to some proposalsbeing rejected. However an advantage can be gainedthrough proposing multiple variables simultaneously.We define the acceptance rate for the Metropolis-Hastings step in this case as:α(γ∗,γi−1) := min{q(γi−1|γ∗)p(γ∗|θ′)q(γ∗|γi−1)p(γi−1|θ′) , 1}(29)where q(.) is a proposal density, which we set to be:q(γ∗|γi−1) = q(γ∗) = Bern(γ∗|pi). Proposed movesfor groups of randomly ordered inclusion parameters,γ∗, are then accepted if α(γ∗,γi−1) is greater than arandom variable u ∼ U [0, 1].Tuning the number of simultaneous proposals can givesignificant computational improvements to the algo-rithm. Changing only a few variables in γ gives ahigh acceptance rate, but takes a long time to cyclethrough the variables leading to poor mixing. Con-versely if we propose too many simultaneously we re-ject too many proposals to be efficient. In this paperwe investigate how best to tune the proposals and com-pare this against the component-wise Gibbs sampler.Sparse Bayesian Variable Selection for the Identification of Antigenic Variability in FMDV4 DATA4.1 Simulated Data20 sets of data were simulated to reflect the struc-ture of the real FMDV data such that there were twogroups of variables. For each response, 20 count vari-ables were simulated from a Poisson distribution andthen an additional set of 10 binary variables was gen-erated. These were both simulated such that therewas a basic correlation within the groups in order toreflect some of the correlations found in the real data.Additionally 10 data sets were given one group of ran-dom effects, with the remaining sets given two groups,in order to mimic random variation in the real exper-imental data.Each of the variables was then given a regression pa-rameter. Half of each group were given small nega-tive regressors drawn from w1 ∼ N (−0.2, 0.01) andthe other half w2 ∼ N (0, 0.0025). Each responseyi was then generated from the model with each ofthe perturbed regressors w˜h,i ∼ N (wh,i, 0.007), whereh ∈ {1, 2}. This was done 200 times with additiveGaussian noise from N (0, 0.04) given to each response.Half of the data was used for training and the remain-ing for testing.4.2 Real Life ExampleThe FMDV data analysed in Reeve et al. (2010) comesfrom sub-Saharan Africa, where the virus is endemic.The authors evaluated data on two different serotypes,but in this paper we only consider the South AfricanTerritories (SAT) type 1 serotype. This contains 246measures of VN titre, an in vitro measure of whetherthe sites that contribute to the neutralization of thevirus remain sufficiently similar to cross-react. TheVN titre measures have been shown to be log-normallydistributed (Reeve et al. 2010), so we take the log ofthe data as our response.The fixed effects used in our model are divided into twogroups. The first contains 137 variables which countamino acid mutations (substitutions) at each residuein the proteins that form the virus shell. The secondgroup consists of 38 variables which indicate whether aspecific branch in the phylogenetic tree was traversedbetween the protective and challenge strains, wherethe protective strain is the strain the animal has beenvaccinated with and the challenge is the strain it istested against. Additionally, the model requires theinclusion of two groups of random effects. The first isthe challenge strain, used to account for the variabilityin reactivity of the viruses. The second accounts forthe sera taken from the individual animals exposed toa strain.5 SIMULATIONSOur code has been implemented in R (R Core Team2013), using the packages lme4 (Bates et al. 2013) andlmmlasso (Schelldorfer et al. 2011) for the comparisonwith standard and LASSO mixed-effects models. Forthe mixed-effects models, as in Reeve et al. (2010), for-ward inclusion was used adjusting for multiple testingusing the Holm-Bonferroni correction.For our MCMC chains we sampled 10,000 and 50,000iterations respectively for the simulated and real data.The fixed parameters, θ2, were all set to representvague priors: αb = βb = αw = βw = 0.001, µ0 = 0,Σ0 = 100, αε = βε = 0.001, αpi = βpi = 1. The onlyexception to this is the shape parameter for the vari-ance of the intercept, which is given as αw,1 = 1.501to give a finite mean and variance for the prior distri-bution of σ2w,1. Although this is not a vague prior, wehave tested a number of other values and found thatthis specification has little effect on the results.To test the convergence of the parameters, 4 chainswere ran for each model and a potential scale reductionfactor (PSRF) (Gelman & Rubin 1992) was computedfrom the within-chain and between-chain variances us-ing the R package coda (Plummer et al. 2006). Wetake a PSRF ≤ 1.1 as a threshold for convergence andterminate the burn-in when this is satisfied for 95% ofthe variables.To analyse the best proposal method we tested thecomponent-wise Gibbs sampler and several specifi-cations of the Metropolis-Hastings sampler on theFMDV data set. For the Metropolis-Hastings sampler,we proposed the inclusion or exclusion of the variablesin groups of 4, 8, 16, 32 and 64. We analysed conver-gence by monitoring the percentage of variables witha PSRF ≤ 1.1 as in Grzegorczyk & Husmeier (2013).6 RESULTSOur results compare the accuracy of predicting signifi-cant variables, as well as out-of-sample predictive per-formance for all methods on the simulated data. Whencalculating the out-of-sample predictive performance,we defined a 0.5 marginal probability for the selectioncriteria of the novel Bayesian model, allowing a smallamount of deviation to account for the variation inchains due to the finite effective sample size. For themixed-effects models, forward inclusion was used withthe Holm-Bonferroni correction at a fixed significancethreshold of 0.05. Both AICc and BIC were used forselecting the regularisation parameter of the mixed-effects LASSO. These selection criteria were then usedto compute out-of-sample likelihood. The same pro-cesses were used to evaluate the model performance onthe FMDV data and compare the convergence speedof the different proposal methods.Vinny Davies, Richard Reeve, William Harvey, Francois F. Maree, Dirk Husmeier(a) One Random-Effect Group(b) Two Random-Effect GroupsFigure 2: ROC curves for classical mixed-effects(grey), mixed-effects LASSO (black dotted), and thenovel Bayesian (black) models when applied to thesimulated data. The simulated data was generatedwith (a) one and (b) two random effect groups; seesection 4.1.To investigate the accuracy of the selection of signifi-cant variables we produced receiver operating charac-teristic (ROC) curves for each of the methods by or-dering the inclusion of variables. This can be achievedfor the novel Bayesian method by ordering of the vari-ables using their predicted marginal posterior proba-bilities. For the standard mixed-effects models this isdone by removing the significance threshold and rank-ing the edges by order of inclusion. Finally for themixed-effects LASSO we predict models for a varietyof different penalty parameters, λ, to create the so-called LASSO path (Hastie et al. 2009). This defines aranking of the variable from which again a ROC curvecan be obtained.6.1 Simulation StudyFigure 2 shows ROC curves for the classicalmixed-effects, mixed-effects LASSO and novel sparseBayesian selection model. ROC curves show modelclassification performance under different levels of sen-sitivity and specificity. This is more general than eval-uating performance at a specific cut-off point, deter-mined using model selection criteria. ROC curves alsoprovide a convenient comparison measure in the formFigure 3: Convergence diagnostics. The lines show theproportion of parameters that have converged (PSRF≤ 1.1) when using component-wise Gibbs sampling(black) and Metropolis-Hastings sampling proposing4 (grey), 8 (black dashed), 16 (grey dashed), 32 (blackthick) and 64 (grey thick) inclusion parameters simul-taneously.of the area under curve (AUC) value.For two random effects groups (Figure 2b), the pro-posed Bayesian model, AUC = 0.93, consistently out-performs the mixed effects LASSO, AUC = 0.79, andstandard mixed effects model, AUC = 0.79. This ispresumably a consequence of the fact that the mixed-effects LASSO, as developed by Schelldorfer et al.(2011), is defined for a single random effect. To dealwith two random effects, we need to map the matrix ofrandom effect combinations into a vector of substitutesingle random effects, which may render the modelover-complex and hence susceptible to over-fitting. Fordata with a single random effect (Figure 2a), the novelBayesian method still achieves a greater AUC value,0.89, than the LASSO, 0.83, and standard mixed ef-fects model, 0.81. Note that realistic data need to bemodeled with more than one random effect, though.To our knowledge, a mixed effects LASSO for suchdata has not been developed.In addition to the comparison of AUC values, we alsolooked at the predictive performance. For the datawith 2 groups of random effects the novel Bayesianmethod got a mean out-of-sample log-likelihood of−113.8, outperforming the mixed effect LASSO withBIC, −160.8, and AICc, −163.3, and the standardmixed effect model, −127.7. Similar results were alsoachieved for the data with 1 random effect group,with the models achieving a mean out-of-sample log-likelihoods of −99.9, −104.2, −105.9 and −112.4, re-spectively.6.2 Foot-and-Mouth Disease Virus DataFigure 3 shows that proposing a larger proportion of8 or 16 binary selection hyperparameters, γ, simul-Sparse Bayesian Variable Selection for the Identification of Antigenic Variability in FMDVtaneously in a Metropolis-Hastings scheme achievesfaster convergence than component-wise Gibbs sam-pling, despite the higher rejection probability (recallthat Gibbs sampling has an acceptance probability of1). This suggests that Gibbs sampling should not al-ways be the default method of choice, and that furtherimprovements may be obtained by including posteriorcorrelations in the proposal moves (see Section 8).With respect to the evaluation of the prediction, weneed to point out that the proposed novel Bayesianmethod is the only one that could be applied in afully automatic manner. The forward-variable selec-tion technique used in Reeve et al. (2010) drew onbiological prior knowledge to design an effective vari-able selection schedule, and the optimisation algorithmfor the mixed-effects LASSO, as implemented in thesoftware of Schelldorfer et al. (2011), failed due to ill-conditioned (i.e. quasi-singular) matrices. To copewith the latter problem, we applied the mixed-effectsLASSO as follows: in the first instance, we included all‘relevant’ residues (as informed by the ‘gold-standard’;see below) and the branches of the phylogenetic treeas potential explanatory variables. We then iterativelyexcluded strongly correlated ‘non-relevant’ variablesuntil the matrix inversion no longer ran into numericalproblems. We need to point out that this strategy usesprior knowledge that would usually not be availableand is not required for the proposed Bayesian method.However for a fair comparison, we used this reducedset of 107 variable for all methods.For performance evaluation, we have concentrated onthe prediction of the relevant residues, which indicateareas of the virus protein that are targeted by the im-mune system, where mutations potentially allow thevirus to escape the host immune response. For eval-uation, we used a list of known ‘true positives’ fromGrazioli et al. (2006) - these are residues in exposed re-gions of the virus protein known to be targeted by theimmune system. We also used a list of ‘true negatives’,which are areas not found in any study of FMDV an-tibody targets (see Reeve et al. (2010) and referencestherein) - these typically lie in buried regions of thevirus protein that are inaccessible to the immune sys-tem. The predictions are shown in Figure 4. It can beseen that the novel Bayesian hierarchical model findsno ‘false positive’, while also showing an increasednumber of ‘true positives’. In combination with thefully automated inference procedure this can be seenas a method improvement.7 CONCLUSIONWe have addressed the problem of identifying residuesresponsible for changes in antigenic variability withinFigure 4: Bar plot showing ‘true positives’ (white) and‘false positive’ (black) for the mixed-effects model re-sults of Reeve et al. (2010), the mixed-effects LASSOusing AICc and BIC (Schelldorfer et al. 2011) and thenovel Bayesian variable selection model.FMDV. We have proposed a novel sparse Bayesianvariable selection scheme based on spike and slab pri-ors, which outperforms competing methods (Figure 2and 4). In the process we have identified three keyresidues that are known to be critical to understand-ing cross-protection between virus.Further to this we have investigated the sampling ofthe latent inclusion variables, γ. We have shownthat proposing multiple moves simultaneously throughMetropolis-Hastings sampling can give a significantcomputational improvement over the more widely usedcomponent-wise Gibbs sampler (Figure 3).8 FURTHER WORKFurther work on this paper comes in several forms.Firstly, there may be room for improvement by re-placing the Inverse-Gamma prior for the variances ofthe random effects (σ2b,g in Figure 1) by a half-Cauchydistribution. As discussed in Gelman (2006), this mayresult in a reduced dependence of the results on theprior as well as an improvement in the convergence ofthe MCMC simulations. Secondly, the model can beextended to include a spike and slab prior for the se-lection of random effects (i.e. the bk,g’s in Figure 1).Thirdly, improved proposal distributions that accountfor the estimated posterior correlation in the binary in-clusion hyperparameters, γ, could potentially improveconvergence, in the same way as for the continuouscase (Haario et al. 2006). A method to generate corre-lated binary variables has been proposed (Leisch et al.2012). However, to apply this method in the contextof MCMC, a proper proposal distribution has to bedeveloped, which has to enter the Metropolis-Hastingsratio. Finally we would like to extend the model tofurther serotypes and diseases. In particular combin-ing all available data for FMDV serotypes would leadto a larger, more complete data set, which would giveus the best chance of identifying all the key residuesassociated with changes in antigenic variability.Vinny Davies, Richard Reeve, William Harvey, Francois F. Maree, Dirk HusmeierReferencesBates, D., Maechler, M. & Bolker, B. (2013), lme4:Linear mixed-effects models using S4 classes.Bengio, Y. & Grandvalet, Y. (2004), ‘No unbiased es-timator of the variance of k-fold cross-validation’,The Journal of Machine Learning Research 5, 1089–1105.Bishop, C. M. (2006), Pattern Recognition and Ma-chine Learning, Springer, Singapore.Dalton, L. & Dougherty, E. (2012), ‘Exact sample con-ditioned MSE performance of the Bayesian MMSEestimator for classification error - part II: Consis-tency and performance analysis’, Signal Processing,IEEE Transactions on 60(5), 2588–2603.Gelman, A. (2006), ‘Prior distributions for varianceparameters in hierarchical models’, Bayesian Anal-ysis 1(3).Gelman, A., Carlin, J. B., Stern, H. S. & Rubin, D. B.(2004), Bayesian Data Analysis, Chapman & Hall.Gelman, A. & Rubin, D. (1992), ‘Inference from itera-tive simulation using multiple sequences’, StatisticalScience 7, 457–511.Geman, S. & Geman, D. (1984), ‘Stochastic relaxation,Gibbs distributions, and the Bayesian restorationof images’, IEEE Transactions on Pattern Analysisand Machine Intelligence 6(6), 721–741.George, E. I. & McCulloch, R. E. (1993), ‘Variableselection via Gibbs sampling’, Journal of the Amer-ican Statistical Association 88(423), 881–889.George, E. I. & McCulloch, R. E. (1997), ‘Approachesfor Bayesian variable selection’, Statistica Sinica7, 339–373.Grazioli, S., Moretti, M., Barbieri, I., Crosatti, M. &Brocchi, E. (2006), Use of monoclonal antibodiesto identify and map new antigenic determinants in-volved in neutralisation on FMD viruses type SAT 1and SAT 2, in ‘Report of the Session of the ResearchGroup of the Standing Technical Committee of theEuropean Commission for the Control of Foot-and-Mouth Disease’, pp. 287–297. Appendix 43.Grzegorczyk, M. & Husmeier, D. (2013), ‘Regular-ization of non-homogeneous dynamic Bayesian net-works with global information-coupling based onhierarchical Bayesian models’, Machine Learning91, 105–151.Haario, H., Laine, M., Mira, A. & Saksman, E. (2006),‘DRAM: Efficient adaptive MCMC’, Statistics andComputing 16(4).Hastie, T., Tibshirani, R. & Friedman, J. (2009), TheElements of Statistical Learning, Springer.Hastings, W. (1970), ‘Monte Carlo sampling meth-ods using Markov chains and their applications’,Biometrika 57(1), 97–109.Holm, S. (1979), ‘A simple sequentially rejective multi-ple test procedure’, Scandinavian Journal of Statis-tics 6, 65–70.Hurvich, C. M. & Tsai, C.-L. (1989), ‘Regressionand time series model selection in small samples’,Biometrika 76(2), 297–307.Leisch, F., Weingessel, A. & Hornik, K. (2012),bindata: Generation of Artificial Binary Data.Metropolis, N., Rosenbluth, A., Rosenbluth, M.,Teller, A. & Teller, E. (1953), ‘Equations of statecalculations by fast computing machines’, Journalof Chemical Physics 21(6), 1087–1092.Mitchell, T. & Beauchamp, J. (1988), ‘Bayesian vari-able selection in linear regression’, Journal of theAmerican Statistical Association 83(404), 1023–1032.Murphy, K. P. (2012), Machine learning: a probabilis-tic perspective, MIT Press, Cambridge, MA.Park, T. & Casella, G. (2008), ‘The Bayesian lasso’,Journal of the American Statistical Association103(482).Pinheiro, J. C. & Bates, D. (2000), Mixed-Effects Mod-els in S and S-PLUS, Springer.Plummer, M., Best, N., Cowles, K. & Vines, K. (2006),‘CODA: Convergence diagnosis and output analysisfor MCMC’, R News 6(1), 7–11.R Core Team (2013), R: A Language and Environmentfor Statistical Computing, R Foundation for Statis-tical Computing, Vienna, Austria.Rao, R. B., Fung, G. & Rosales, R. (2008), ‘On thedangers of cross-validation. an experimental evalua-tion’, SIAM Data Mining .Reeve, R., Blignaut, B., Esterhuysen, J. J., Opper-man, P., Matthews, L., Fry, E. E., de Beer, T. A. P.,Theron, J., Rieder, E., Vosloo, W., O’Neill, H. G.,Haydon, D. T. & Maree, F. F. (2010), ‘Sequence-based prediction for vaccine strain selection andidentification of antigenic variability in Foot-and-Mouth disease virus’, PLoS Comput Biol 6(12).Ripley, B. (1979), ‘Algorithm AS 137: Simulating spa-tial patterns: Dependent samples from a multivari-ate density’, Journal of the Royal Statistical Society.Series C 28(1), 109–112.Sabatti, C. & James, G. M. (2005), ‘Bayesian sparsehidden components analysis for transcription net-works’, Bioinformatics 22(6), 739–746.Schelldorfer, J., Bühlmann, P. & van de Geer,S. (2011), ‘Estimation for high-dimensional linearSparse Bayesian Variable Selection for the Identification of Antigenic Variability in FMDVmixed-effects models using `1-penalization’, Scandi-navian Journal of Statistics 38(2), 197–214.Tibshirani, R. (1996), ‘Regression shrinkage and selec-tion via the lasso’, Journal of the Royal StatisticalSociety: Series B 58, 267–288.Tibshirani, R. (2011), ‘Regression shrinkage and selec-tion via the lasso: a retrospective (with comments)’,Journal of the Royal Statistical Society: Series B73(3), 273–282.Zou, H. & Hastie, T. (2005), ‘Regularization and vari-able selection via the elastic net’, Journal of theRoyal Statistical Society: Series B 67(2), 301–320.",
    "id": 20114855,
    "identifiers": {
        "doi": null,
        "oai": "oai:eprints.gla.ac.uk:93188"
    },
    "title": "Sparse Bayesian variable selection for the identiﬁcation of antigenic variability in the Foot-and-Mouth disease virus",
    "language": {
        "code": "en",
        "name": "English"
    },
    "publishedDate": "2014-04-01T01:00:00+01:00",
    "publisher": "PMLR",
    "references": [],
    "sourceFulltextUrls": [
        "https://eprints.gla.ac.uk/93188/1/93188.pdf"
    ],
    "updatedDate": "",
    "yearPublished": "2014",
    "links": [
        {
            "type": "download",
            "url": "https://core.ac.uk/download/20114855.pdf"
        },
        {
            "type": "reader",
            "url": "https://core.ac.uk/reader/20114855"
        },
        {
            "type": "thumbnail_m",
            "url": "https://core.ac.uk/image/20114855/medium"
        },
        {
            "type": "thumbnail_l",
            "url": "https://core.ac.uk/image/20114855/large"
        },
        {
            "type": "display",
            "url": "https://core.ac.uk/outputs/20114855"
        }
    ],
    "abstract": "Vaccines created from closely related viruses are vital for oﬀering protection against newly emerging strains. For Foot-and-Mouth disease virus (FMDV), where multiple serotypes  co-circulate, testing large numbers of vaccines can be infeasible. Therefore the development of an in silico predictor of cross- \n\nprotection between strains is important to help optimise vaccine choice. Here we describe a novel sparse Bayesian variable selection model using spike and slab priors which is able to predict antigenic variability and identify sites which are important for the neutralisation of the virus. We are able to iden- \n\ntify multiple residues which are known to be key indicators of antigenic variability. Many of these were not identiﬁed previously using frequentist mixed-eﬀects models and still cannot be found when an ℓ1 penalty is used. We further explore how the Markov chain Monte Carlo (MCMC) proposal method for the inclusion of variables can oﬀer significant reductions in computational requirements, both for spike and slab priors in general, and \n\nour hierarchical Bayesian model in particular",
    "tags": [],
    "fulltextStatus": "enabled",
    "subjects": [],
    "oai": "oai:eprints.gla.ac.uk:93188",
    "deleted": "ALLOWED",
    "disabled": false,
    "journals": null,
    "repositories": {
        "id": "42",
        "openDoarId": 0,
        "name": "Enlighten",
        "urlHomepage": null,
        "uriJournals": null,
        "physicalName": "noname",
        "roarId": 0,
        "baseId": 0,
        "pdfStatus": null,
        "nrUpdates": 0,
        "lastUpdateTime": null
    },
    "repositoryDocument": {
        "id": 20114855,
        "depositedDate": null,
        "publishedDate": "2014-04-01T01:00:00+01:00",
        "updatedDate": "2024-02-12T08:51:37+00:00",
        "acceptedDate": null,
        "createdDate": "2014-05-01T18:02:23+01:00"
    },
    "urls": [
        "https://eprints.gla.ac.uk/93188/1/93188.pdf"
    ],
    "lastUpdate": "2024-02-12T08:51:37+00:00",
    "setSpecs": []
}