{
    "acceptedDate": "",
    "authors": [
        {
            "name": "Adams-Cohen, Nicholas Joseph"
        }
    ],
    "contributors": [],
    "createdDate": "2020-11-24T17:41:39+00:00",
    "dataProvider": {
        "id": 2857,
        "name": "Caltech Authors - Main",
        "url": "https://api.core.ac.uk/v3/data-providers/2857",
        "logo": "https://api.core.ac.uk/data-providers/2857/logo"
    },
    "depositedDate": "",
    "documentType": "",
    "doi": "",
    "downloadUrl": "https://core.ac.uk/download/345073997.pdf",
    "fullText": "Appendix A: Additional Descriptive Statistics\nI collected the tweets analyzed in my project over a two-month time span, from May 27, 2015 to\nAugust 24, 2015. To obtain this data, I use a series of python scripts that continuously interacted\nwith the Twitter API, using regular expressions to archive any tweet that contained one of the\nfollowing issue words: gay marriage, gay marriages, same-sex marriage, same-sex marriages,\nsame sex marriage, same sex marriages, same-sex union, same-sex unions, same sex union,\nsame sex unions, marriage equality, equal marriage. During this time, I collected a total of\n5,996,741 tweets. After filtering for location in the process described in the Gathering Twitter\nData section above, I end up with 1,028,151 total tweets. In Figure A1, I plot the number of tweets\nI collected each day. The top half of Figure A1 plots the raw frequency of daily tweets, and it is\nimmediately apparent that a very large number of tweets were sent on June 26, 2015, the day the\nSupreme Court announced their decision. This drops off quickly, although I collect a large number\nof tweets until early July. The bottom half of Figure A1 plots the logged frequencies in order to\nbetter visualize the entire time series.\nEach state is represented in my dataset, with the number of tweets sent from each state enu-\nmerated in Table A1. One can also get a general sense of the distribution of users by looking at the\nheat map in Figure A2, which maps the number of tweets sent per capita using state populations\nrecorded in the 2010 census.\n29\nFigure A1: Frequencies: May 27, 2015 to July 31, 2015\n0\n100,000\n200,000\n300,000\nJun 01 Jun 15 Jul 01 Jul 15 Aug 01\nDate\nF\nre\nqu\nen\nce\ny\n(a) Raw Frequency\n3.5\n4.0\n4.5\n5.0\n5.5\nJun 01 Jun 15 Jul 01 Jul 15 Aug 01\nDate\nLo\ng \nF\nre\nqu\nen\nce\ny\n(b) Log Frequency\nFigure A2: Frequency of Tweets by State per Capita\n0.002\n0.003\n0.004\nTweets per Capita\n30\nTable A1: Number of Tweets from each State\nState Number of Tweets\nCalifornia 136,340\nTexas 109,485\nNew York 98,792\nFlorida 56,932\nIllinois 42,874\nOhio 36,222\nPennsylvania 32,802\nGeorgia 31,259\nWashington D.C. 29,888\nMichigan 27,755\nWashington 25,295\nMassachusetts 23,978\nNorth Carolina 23,539\nTennessee 21,697\nNew Jersey 20,122\nArizona 19,372\nLouisiana 18,852\nVirginia 18,602\nColorado 16,936\nMissouri 15,949\nIndiana 154,66\nMaryland 14,605\nWisconsin 14,391\nAlabama 14,104\nOregon 14,008\nMinnesota 13,389\nState Number of Users\nKentucky 12,133\nNevada 11,425\nSouth Carolina 11,425\nOklahoma 9,184\nKansas 86,25\nArkansas 7,658\nUtah 7,315\nConnecticut 7,005\nIowa 6,730\nMississippi 6,239\nNebraska 6,203\nWest Virginia 5,857\nNew Mexico 4,041\nMaine 4,006\nHawaii 3,758\nIdaho 3,443\nRhode Island 3,235\nDelaware 3,189\nNew Hampshire 2,909\nAlaska 2,428\nVermont 2,001\nMontana 1,887\nSouth Dakota 1,851\nNorth Dakota 1,592\nWyoming 1,351\n31\nAppendix B: Preprocessing Text Data\nBefore running supervised training methods to estimate sentiment, I use several preprocessing\nscripts to manipulate and simplify the Twitter text data. First, I remove all textual information that\ndoes not inform the substance of the message, including punctuation, all forms of capitalization,\nand words that fail to contribute towards a sentence’s meaning (such as “the, of, or”).\nNext, I tokenize the text, a process that splits “a string into its desired constituent parts” (Potts,\n2011). My tokenizing strategy utilizes white-space to break apart a sentence into separate words.\nThis transfers the content of a tweet into a list of individual words, ignoring the original order\nthese words appear in the sentence. While the order of words in a sentence can absolutely con-\ntribute to the content of a message, treating each document as coming from a “bag-of-words” is a\ncommon (though at times contentious) assumption that is necessary to apply many machine learn-\ning methodologies (Grimmer & Stewart, 2013). In many situations, enough information can be\ngleaned from the choice of unique words to justify this assumption.\nFinally, the entire dataset is transformed into a document-frequency matrix (DFM). A DFM is\nan N × J matrix, where N is the number of documents (in this case, tweets) and J is the number\nof unique features (in this case, individual words) found across all documents. Thus, if tweet n\ncontains two instances of word j, the njth entry of the DFM is 2. With Twitter data, this represents\na very sparse matrix, as the entire set of unique words J across the entire dataset can be quite large,\nalthough an individual tweet being capped at 140-characters contains a small number of individual\nwords (while Twitter eventually increased this cap to 280-characters, this occurred after my data\ncollection period) Thus, rather than utilizing each of the J unique features in the entire dataset, I\nanalyze a subset of features based on how frequently the feature appears. This parameter can be\ntuned, but for the baseline analysis I kept a feature if it appeared at least three times throughout\nthe dataset. In order to implement the preprocessing steps described above, this project utilized\nthe quanteda R package (Benoit & Nulty, 2016). The quanteda package provides tools to\norganize and analyze string data in order to implement sentiment analysis methodologies.\n32\nAppendix C: Validating the Supervised Scoring Method\nIn order to train a supervised classifier, I create a set of hand-annotated tweets using Mechanical\nTurk. In order to label the largest number of messages in the shortest amount of time, the set of\nannotated tweets corresponds with the top-4,000 most repeated messages in the dataset. In total,\nthese 4,000 tweets represent 1,895,554 total messages, and thus consists of 31.60% of all collected\ntweets. After stripping these 4,000 messages of usernames, hyperlinks, and punctuation, there\nwere 3,934 unique messages in the validation set.\nIn order to build this hand-annotated validation set, I utilized Amazon Mechanical Turk, a\ncrowdsourcing platform that allows a researcher to pay individuals to complete small tasks. I\ncreated a set of tasks that required Mechanical Turk users to score the sentiment of ten tweets in\nmy validation set. I present a screen shot of the task in Figure A3.\nFigure A3: Sample Mechanical Turk Task\nEach task was performed by three separate Mechanical Turk users in order to get a sentiment\nscore as close to the ground truth as possible. To create a final score for each of the 3,934 unique\nmessages, I took the majority score across the three annotations. In total, the Mechanical Turkers\nlabeled 626 messages negative, 1,778 positive, and 1,333 neutral. Only 197 messages did not have\n33\na majority category. In the body of the paper, I focus on using the 626 negative and 1,778 positive\nmessages to train a binary classifier. In Appendix E, I use the neutral messages to build a three-way\nclassifier.\nIn order to train a classifier, I split 10% of the training data to use a test set. This test set of\ntweets is not used to train the final model, and thus allows me to evaluate the performance of the\nmodel on new data.\nI start by testing a number of different classifiers, including Support Vector Machines, logit-\nboost, neural networks, and random forest.1 For each model, I run a 10-cross fold validation to\ntrain the classifier before evaluating performance on the left out set. The accuracy (compared with\na no-information red in red) and kappa coefficient of the best performing models in each category\nare found in Figure A4. I find that random forest leads to the highest accuracy without sacrificing\ninter-rater reliability.\nFigure A4: Comparing Classifiers\n(a) Accuracy\n●\n●\n●\n●\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nLogitBoost Neural network Random forest SVM\nmodel\nA\ncc\nur\nac\ny\n/\n(b) Cohen’s Kappa\n●\n●\n●\n●\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nLogitBoost Neural network Random forest SVM\nmodel\nK\nap\npa\nOn choosing an overall model classification, I tune the hyper-parameters of the random forest\nmodel. Repeating 10-cross fold validation 10 times per hyper-parameter, I test which minimum\n1I train and evaluate all classifiers with the caret package (Kuhn, 2008)\n34\nnode size leads to the best cross validation accuracy.\nFor the test set, my final model accurately predicts 81.74% of the data, with 97.78% Sensitivity\nand 34.43% Specificity.\nTo better diagnose the model, I present the receiver operating characteristic (ROC) curve (with\narea under curve reported) in Figure A5, and the test-set confusion matrix in Table A2.\nFigure A5: ROC curve\nSpecificity\nS\nen\nsi\ntiv\nity\n1.0 0.8 0.6 0.4 0.2 0.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nAUC: 0.822\nTable A2: Error Matrix:\nTest Set Predictions\nPredicted Predicted Total\nNegative Positive\nTrue Negative\n21 4\n25\nTrue Positive\n40 176\n216\nTotal 61 180\nThe error matrix reveals that one issue my classifier exhibits is over-predicting the positive\nclass. While part of this issue may stem from the fact I have an unbalanced training set, visually\n35\ninspecting may of the false-positive tweets reveals that many misclassified messages are highly\nsarcastic in tone. While this is easy for a human reader to recognize, sarcasm is very difficult\nto detect in sentiment scoring algorithms.2 Overall, this reveals that my classifier is more likely\nto falsely classify a negative tweet as positive, biasing all my scores upwards. Thus, as my core\nfinding is finding a more negative reaction in states with a law change, this upward bias is likely\nattenuating my findings. Thus, this bias should not hurt the causal interpretation of my core results.\nAppendix D: Neutral Tweets\nAn issue potentially biasing my results is the presence of a third sentiment category: neutral mes-\nsages. While theoretically possible to build a third training set of neutral tweets and training a\nthree-way classifier, binary classifiers tend to lead to more accurate labels. However, in a robust-\nness check, I retrain the classifier using the neutral labels collected on Mechanical Turk. In total,\nthis training set consists of 626 negative, 1,778 positive, and 1,333 neutral tweets.\nI train this model with the same procedure described in an earlier section: leaving out 10% of\nthe data as a test set, and doing 10-fold validation across the training set to tune over the hyper-\nparameters in the random forest model. The confusion matrix for the best performing model on\nthe left out test set is found in Table A3. In total, the model has an accuracy of 61.23% against a\n47.06% no information rate, and Cohen’s kappa coefficient 0.35.\nApplying this three-way classifier to my analysis, I rerun my main model specification includ-\ning with neutral labels. I score neutral messages as 0.5, in addition to scoring negative messages 0\nand positive messages 1. I present the results of this robustness check in Table A4\nIn Table A4, I note that, across each model specification, the Treated×After coefficient re-\nmains negative and statistically significant. In fact, the model including neutral labels more ro-\nbustly demonstrates my core results, finding a small level of near statistical significance in model\nfive (a null result in the binary model). This seems to provide additional evidence that the inclusion\nof neutral tweets biases my core results upwards, allowing me to better interpret the core results in\n2See (Maynard & Greenwood, 2014) as an example of one attempt to address sarcasm detection in tweets.\n36\nTable A3: Multiclass Model Error Matrix:\nTest Set Predictions\nPredicted Predicted Predicted Total\nNegative Neutral Positive\nTrue Negative\n15 6 2\n23\nTrue Neutral\n31 89 49\n169\nTrue Positive\n18 39 125\n182\nTotal 61 134 176\nTable A4: Difference-in-Difference Results: Three-Way Classifier\nDependent variable:\nPositive Sentiment\n(1) (2) (3) (4) (5)\nAfter 0.064∗∗∗ 0.016∗∗∗ 0.055∗∗∗ 0.030∗∗∗ 0.023∗∗∗\n(0.001) (0.001) (0.002) (0.002) (0.003)\nTreated 0.001 0.002 −0.0004 0.007∗ 0.0002\n(0.002) (0.002) (0.003) (0.004) (0.006)\nTreated*After −0.019∗∗∗ −0.026∗∗∗ −0.018∗∗∗ −0.016∗∗∗ −0.010∗\n(0.002) (0.002) (0.003) (0.004) (0.006)\nGOP −0.164∗∗∗ −0.155∗∗∗\n(0.001) (0.002)\nConstant 0.649∗∗∗ 0.648∗∗∗ 0.669∗∗∗ 0.643∗∗∗ 0.646∗∗∗\n(0.001) (0.001) (0.002) (0.002) (0.003)\nDrop June 26? No Yes No No No\nRace and Gender No No Yes No Yes\nN 1,076,512 673,792 506,274 191,980 93,681\nR2 0.004 0.001 0.011 0.062 0.060\nNote: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\n37\na causal manner.\nAppendix E: Border State Analysis\nAs the parallel trend assumption in the difference-in-difference estimator posits that the untreated\ngroup is a good counter-factual to the treatment group, a potential criticism of my work is that I do\nnot restrict the group of untreated states. That is, I analyze data from all fifty states, when perhaps\nstates like California and New York do not make good counterfactuals to the states in the treatment\ngroup.\nWhile a matching methodology represents the most rigorous way to find valid counterfactuals\nfor users in my treated set, I do not have a rich enough set of independent variables to allow for\nan accurate matching procedure. However, it is possible to use the geography of the treated states\nto find a set of users that might represent a more valid counterfactual. Thus, I re-run my analysis\nwith a smaller set of untreated states, restricting the untreated group to only those states that share\na border with one or more treated states.3 By restricting the untreated states in this way, I am\nmore likely to select states with similar demographic characteristics, allowing me to further test\nand validate my results. The result of this robustness check is found in Table A5, which replicates\nthe model specifications in the body of the paper.\nIn models one and two, the baseline models, I find a negative and statistically significant\nTreated×After coefficient. Thus, even when restricting the untreated group to smaller set of states\nmore likely to share characteristics with the treated set, I continue to find evidence of a causal im-\npact. I also find this impact in model three, where I include demographic information. In models\nfour and five, where I include the partisan labels, I find a null result. This is partially due to the\nupward bias of the binary classifier described in the previous appendix; with a higher probability\nin coding neutral messages as positive, the results are biased upwards, away from my hypothesis.\nTo test the impact of neutral messages on the border states, I rerun the analysis with the three-\n3The bordering states include: Oklahoma, Kansas, New Mexico, Colorado, Wyoming, Montana, Minnesota, Iowa,\nWisconsin, Illinois, Indiana, Alabama, Florida, South Carolina, North Carolina, Virginia, West Virginia, Pennsylvania.\n38\nTable A5: Border States with Binary Classifier\nDependent variable:\nPositive Sentiment\n(1) (2) (3) (4) (5)\nAfter 0.015∗∗∗ −0.053∗∗∗ 0.023∗∗∗ −0.033∗∗∗ −0.037∗∗∗\n(0.002) (0.003) (0.003) (0.005) (0.007)\nTreated 0.007∗∗ 0.008∗∗ 0.015∗∗∗ −0.009 0.0004\n(0.003) (0.003) (0.004) (0.007) (0.010)\nTreated*After −0.013∗∗∗ −0.015∗∗∗ −0.022∗∗∗ 0.00001 0.005\n(0.003) (0.004) (0.005) (0.007) (0.011)\nGOP −0.212∗∗∗ −0.205∗∗∗\n(0.003) (0.004)\nConstant 0.800∗∗∗ 0.797∗∗∗ 0.813∗∗∗ 0.833∗∗∗ 0.807∗∗∗\n(0.002) (0.002) (0.003) (0.005) (0.007)\nDrop June 26? No Yes No No No\nRace and Gender No No Yes No Yes\nN 580,896 362,785 272,415 102,474 49,974\nR2 0.0001 0.003 0.005 0.055 0.052\nNote: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\n39\nway classifier described in Appendix D: Neutral Tweets. In this model, I code the dependent\nvariable as 0 for negative tweets, 0.5 for neutral tweets, and 1 for positive tweets. I present the\nresults in Table A6.18\nTable A6: Border States with Three-Way Classifier\nDependent variable:\nPositive Sentiment\n(1) (2) (3) (4) (5)\nAfter 0.065∗∗∗ 0.015∗∗∗ 0.066∗∗∗ 0.026∗∗∗ 0.031∗∗∗\n(0.002) (0.002) (0.003) (0.003) (0.005)\nTreated 0.014∗∗∗ 0.015∗∗∗ 0.024∗∗∗ 0.008 0.024∗∗∗\n(0.002) (0.003) (0.003) (0.005) (0.006)\nTreated*After −0.021∗∗∗ −0.025∗∗∗ −0.028∗∗∗ −0.011∗∗ −0.019∗∗∗\n(0.003) (0.003) (0.004) (0.005) (0.007)\nRepublican −0.175∗∗∗ −0.160∗∗∗\n(0.002) (0.003)\nConstant 0.637∗∗∗ 0.635∗∗∗ 0.647∗∗∗ 0.647∗∗∗ 0.621∗∗∗\n(0.002) (0.002) (0.003) (0.003) (0.005)\nDrop June 26? No Yes No No No\nRace and Gender No No Yes No Yes\nN 607,695 378,437 286,118 106,761 52,115\nR2 0.003 0.0003 0.012 0.072 0.064\nNote: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\nHere, I find that all the results are extremely similar to Table A5, but with a negative and\nsignificant Treated×After coefficient in models 4 and 5. This robustness check helps confirm the\nresults in the main section of my paper.\n40\nAppendix F: Checking for Bot Accounts\nDetecting ‘bot’ accounts is the subject of many machine learning papers, with researchers focusing\non different techniques to determine whether messages are sent by humans or automated programs\n(e.g. Wang, 2010; Jajodia et al., 2012; Ferrara, Varol, Davis, Menczer, & Flammini, 2016). Given\nthe discussions in the wake of the 2016 U.S. election regarding automated systems disseminating\n“fake news” on social media platforms, it is important to consider whether or not my dataset is\nfilled with ‘bot’ accounts biasing my results.\nTo get a sense of how many likely bot accounts are present in my dataset, I pull a sample of\n30,000 random users. To figure out how likely these 30,000 users are ‘bot’ accounts, I utilize\nthe Botometer publicly available API.4 The Botometer API interacts with the Twitter API,\npulling over one thousand features from the user’s Twitter profile to compare against a collection of\n15,000 manually verified bot accounts and 16,000 verified human accounts (Varol, Ferrara, Davis,\nMenczer, & Flammini, 2017). The classifier then runs an ensemble method using random forests,\nAdaBoost, logistic regression, and decision trees to determine the likelihood a given user is human\nor a ‘bot.’ The classifier outputs a likelihood from zero to one; the closer the bot score is to one, the\nmore likely the account is run by an automated program. I present the distribution of classification\nscores from 30,000 randomly selected users in Figure A6.\nFigure A6 demonstrates that the majority of users are likely human, with a mean bot score of\n0.29 with a standard deviation of 0.14 across the sample. Only a small number of users are likely\nbots, with only 9.2% of users with a bot score greater than 0.5 and 1.3% of users with a bot score\ngreater than 0.75. While important to note Botometer represents only one approach to detecting\nbots, this preliminary analysis shows little evidence that bots drive my results.\n4https://botometer.iuni.iu.edu\n41\nFigure A6: Histogram of Twitter Bot Likelihood\n0\n1000\n2000\n3000\n0.00 0.25 0.50 0.75 1.00\nBot Score\nC\nou\nnt\n42\nReferences\nBenoit, K., & Nulty, P. (2016). quanteda: Quantitative analysis of textual data [Computer software\nmanual]. Retrieved from http://github.com/kbenoit/quanteda (R package\nversion 0.9.1-11)\nFerrara, E., Varol, O., Davis, C., Menczer, F., & Flammini, A. (2016). The rise of social bots.\nCommunications of the ACM, 59(7), 96–104.\nGrimmer, J., & Stewart, B. M. (2013). Text as data: The promise and pitfalls of automatic content\nanalysis methods for political texts. Political Analysis, 21, 267-297.\nJajodia, S., Wang, H., Gianvecchio, S., & Chu, Z. (2012, 11). Detecting automation of twitter\naccounts: Are you a human, bot, or cyborg? IEEE Transactions on Dependable and Secure\nComputing, 9, 811-824.\nKuhn, M. (2008). Building predictive models in r using the caret pack-\nage. Journal of Statistical Software, Articles, 28(5), 1–26. Retrieved from\nhttps://www.jstatsoft.org/v028/i05 doi: 10.18637/jss.v028.i05\nMaynard, D., & Greenwood, M. (2014, 01). Who cares about sarcastic tweets? investigating the\nimpact of sarcasm on sentiment analysis. Proceedings of LREC, 4238-4243.\nPotts, C. (2011). Sentiment symposium tutorial [Computer software manual]. Retrieved from\nhttp://sentiment.christopherpotts.net/index.html\nVarol, O., Ferrara, E., Davis, C. A., Menczer, F., & Flammini, A. (2017). Online human-bot\ninteractions: Detection, estimation, and characterization. arXiv:1703.03107.\nWang, A. H. (2010). Detecting spam bots in online social networking sites: A machine learning\napproach. In Ifip annual conference on data and applications security and privacy (pp.\n335–342).\n4\n",
    "id": 345073997,
    "identifiers": {
        "doi": null,
        "oai": "oai:authors.library.caltech.edu:104418"
    },
    "title": "Policy Change and Public Opinion: Measuring Shifting Political Sentiment With Social Media Data",
    "language": {
        "code": "en",
        "name": "English"
    },
    "publishedDate": "2020-09-01T01:00:00+01:00",
    "publisher": "'Academy of Traumatology'",
    "references": [],
    "sourceFulltextUrls": [
        "https://authors.library.caltech.edu/104418/1/05_online_appendices.pdf"
    ],
    "updatedDate": "",
    "yearPublished": "2020",
    "links": [
        {
            "type": "download",
            "url": "https://core.ac.uk/download/345073997.pdf"
        },
        {
            "type": "reader",
            "url": "https://core.ac.uk/reader/345073997"
        },
        {
            "type": "thumbnail_m",
            "url": "https://core.ac.uk/image/345073997/medium"
        },
        {
            "type": "thumbnail_l",
            "url": "https://core.ac.uk/image/345073997/large"
        },
        {
            "type": "display",
            "url": "https://core.ac.uk/outputs/345073997"
        }
    ],
    "abstract": "This article uses Twitter data and machine-learning methods to analyze the causal impact of the Supreme Court’s legalization of same-sex marriage at the federal level in the United States on political sentiment and discourse toward gay rights. In relying on social media text data, this project constructs a large data set of expressed political opinions in the short time frame before and after the Obergefell v. Hodges decision. Due to the variation in state laws regarding the legality of same-sex marriage prior to the Supreme Court’s decision, I use a difference-in-difference estimator to show that, in those states where the Court’s ruling produced a policy change, there was relatively more negative movement in public opinion toward same-sex marriage and gay rights issues as compared with other states. This confirms previous studies that show Supreme Court decisions polarize public opinion in the short term, extends previous results by demonstrating opinion becomes relatively more negative in states where policy is overturned, and demonstrates how to use social media data to engage in causal analyses",
    "tags": [
        "Article",
        "PeerReviewed"
    ],
    "fulltextStatus": "enabled",
    "subjects": [
        "Article",
        "PeerReviewed"
    ],
    "oai": "oai:authors.library.caltech.edu:104418",
    "deleted": "ALLOWED",
    "disabled": false,
    "journals": null,
    "repositories": {
        "id": "2857",
        "openDoarId": 0,
        "name": "Caltech Authors - Main",
        "urlHomepage": null,
        "uriJournals": null,
        "physicalName": "noname",
        "roarId": 0,
        "baseId": 0,
        "pdfStatus": null,
        "nrUpdates": 0,
        "lastUpdateTime": null
    },
    "repositoryDocument": {
        "id": 345073997,
        "depositedDate": null,
        "publishedDate": "2020-09-01T01:00:00+01:00",
        "updatedDate": "2021-01-22T07:17:37+00:00",
        "acceptedDate": null,
        "createdDate": "2020-11-24T17:41:39+00:00"
    },
    "urls": [
        "https://authors.library.caltech.edu/104418/1/05_online_appendices.pdf",
        "https://resolver.caltech.edu/CaltechAUTHORS:20200717-070757859"
    ],
    "lastUpdate": "2021-01-22T07:17:37+00:00",
    "setSpecs": []
}