{
    "acceptedDate": "",
    "authors": [
        {
            "name": "Mcallister, David F."
        },
        {
            "name": "Vouk, Mladen A."
        }
    ],
    "contributors": [],
    "createdDate": "2016-08-03T06:54:07+01:00",
    "dataProvider": {
        "id": 151,
        "name": "NASA Technical Reports Server",
        "url": "https://api.core.ac.uk/v3/data-providers/151",
        "logo": "https://api.core.ac.uk/data-providers/151/logo"
    },
    "depositedDate": "",
    "documentType": "research",
    "doi": "",
    "downloadUrl": "https://core.ac.uk/download/pdf/42822938.pdf",
    "fullText": "Computer Science\nTechnical Report\nMuitiversion Software Reliability Through\nFault-Avoidance and Fault-Tolerance\nReport #1 (3/1/89-8/31/89) on NAG-I-983\nby\nMladen A. Vouk and David F. McAllister\n(NASA-CR-186502) MULTI-VERSION SOFTWARE\nR_LIA_ILITY THROUGH FAULT-AVOIDANCE AND\nFAULT-/_LERANCE Semiannual Technical _eport,\nI M_r. - 31 Aug. 1989 (North Carolina State\nUniv.) 42 p CSCL 09B G3/01\nNgO-Z55_\nunclas\n027307@\nNorth Carolina State University\nBox 8206\nRaleigh, NC 27695\nhttps://ntrs.nasa.gov/search.jsp?R=19900016268 2020-03-19T22:37:44+00:00Z\nf_ v\nSemi-Annual TeChniCal Report Submitted to the\nNATIONAL AERONAUTICS AND SPACE ADMINISTRATION\nLangley Research Center, Hampton, Va,\nfor research entitled\nMULTI-VERSION SOFTWARE RELIABILITY\nTHROUGH\nFAULT-AVOIDANCE AND FAULT-TOLERANCE\n(NAG-I-983)\nfrom\nMladen A. Vouk, Co-Principal Investigator, Assistant Professor\nDavid F. McAIlister, Co-Principal Investigator, Professor\nDepartment of Computer Science\nNorth Carolina State University\nRaleigh, N.C. 27695-8206\n(919) 737-2858\nReport Period\nBeginning Date: March 1, 1989.\nEnding Date: August 31, 1989.\nRaleigh, September 1, 1989\nNAS A/NAG- 1-983/Semi-Annual ReporV1.1/NCSU.CSC.(DFM,MAV)/Sep-89 2\nTable of Contents\nProject Progress Summary\nGeneral Project Description\nResults\n2.1 Fault-Avoidance: Coverage Testing\n2.2 Fault-Avoidance: Using Back-to-Back\nRegression Testing\n2.3 Fault-Tolerance: Safety Properties of\nFault-Tolerance Schemes\n2.4 Other Work in Progress\nBibliography\nAppendix I BGG: A Testing Coverage Tool\nTesting in\nSome Hybrid\n(23 pages)\n3\n4\n6\n6\n8\n12\n16\n17\n20\nNAS A/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89\nProject Progress Summary\nIn this project we have proposed to investigate a number of experimental and theoretical issues\nassociated with the practical use of multi-version software to provide run-time tolerance to software\nfaults Ix_ the _od reported here we have worked on the following: Ok-,'÷ _._ _ -_-__ _ _=_ r- !- '\n_\"- We have finished developing and evaluating a specialized tool for measuring testing coverage\nfor a variety of metrics. ,. ,,_--_ _- _ _-\no We have started using the tool \"to collect information on the relationships between software\nfaults and coverage provided by the testing process as measured by different metrics ......\n(including data flow metrics). We have found considerable correlation b-etweencoverage -:\nprovided by some higher metrics and the elimination of faults in the code.\no We have continued studying back-to-back testing as an efficient mechanism for removal of\nun-correlated faults, and common-cause faults of v_---'-_61_s-p_in_- :_ - :-\no We continued studying software reliability estimation methods based on non-random ,,¢,1 .......\nsampling, and the relationship between software reliability and co-'d_6ovei:_ige provided \"\nthrough testing. _-.i__._:_-. : _ _._'- _\no We continued investigaiing existing, and worked orr£ormul_f_oew fault-tolerance\nmodels, In particular, we have finished simulation studies :of the Acceptance __tihg and\nMulti-stage Voting algorithms, and found t-fhat te_ two schemes for _ fault-iolerance\nare superior in many respects to so_tlae commonly used schemes. Particularly encouraging are\nthe safety properties of the Acceptafice testing scheme.\nThis report describes the results obtained in the period March 1, 1989 to August 31, 1989.\nNASA/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(I)FM,MAV)/Sep-89 4\n1. General Project Description\nSoftware reliability is very important in critical software application areas. For example, space\nbased systems, avionics systems, critical nuclear power plant systems, and life-critical medical\nsystems are all expected to operate reliably even under extremely severe conditions. However,\npractice shows that critical systems are not immune to software related failures [e.g. Neu85].\nCurrently there are two basic ways of showing that code is 100% correct. One is program proving\nand the other exhaustive testing [Adr82, AnR74, Cri85, How82,87]. Neither approach is currently\npractical for use with complex software systems. Techniques for proving software correct are not\nmature enough and exhaustive testing is ruled out principally by the huge number of possible\ninputs. Although significant progress has been made in developing efficient and effective\ndevelopment and testing techniques which greatly aid in avoiding software faults through formal\nconstructive and analytical methods [e.g. Adr82, How87, Hor87], these techniques do not\nguarantee production of error-free code. Furthermore, quantitative relationships between software\nreliability and the quality of the applied development and testing techniques have received relatively\nlittle attention. In modern critical systems the problem is further aggravated by the need for\nextensive concurrent processing.\nThe only way of handling unknown and unpredictable software failures (faults) is through fault-\ntolerance. Fault-tolerance already is, or is planned to be, part of many critical software and hardware\nsystems such as nuclear power plants [Gme79, Bis86] and aerospace systems [Mar82, Wi183,\nSpe84, Mad84, Tro85, Hi185, Avi87, Vog88a]. Two methods for achieving software fault-tolerance\nare in common use today. These are the N-version programming scheme [Avi77, Che78, Avi84] and\nthe recovery block scheme [Ran75]. Both schemes are based on software component redundancy\nand the assumption that coincident failures of components are rare and when they do occur responses\nare sufficiently dissimilar so that the mechanism for deciding answer correctness is not ambiguous.\nFor best results all of these techniques require the component failures to be mutually independent, or\nat least that the positive inter-component failure correlation is low. Fault-tolerant software (FTS)\nmechanisms based on redundancy are particularly well suited for parallel processing environments\nwhere concurrent execution of redundant components may drastically improve sometimes prohibitive\ncosts associated with their serial execution.\nNASA/NAG-1-983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89\nHence,the study of both multi-version and singleversion softwarefault-avoidanceand fault-\ntoleranceissues,with anemphasison theissueof fault correlationin multiple softwareversions,is\nof utmostimportancewherecritical softwareis concerned.We haveproposedto studydifferent\ntestingapproachessuitablefor developmentof singleandmulti-versionhigh-reliability software,\nmodelsingleandmulti-versionreliability,andinvestigatedifferentfault-tolerancemechanisms.\nIn the period 1985-87NASA fundeda multi-university experimentto develop 20 functionally\nequivalentsoftwareversions,knownasRSDIMU softwareversions.Theseversionsareto beused\nto determinethereliability gainsof severalcommonfault-tolerantsoftwaresystems,includingN-\nversionprogramming, recovery-block[Ran75,Avi84], andhybrid schemes uchastheconsensus\nrecoveryblock technique[Sco87].\nIn theperiodreportedherewehaveworkedon thefollowing:\nWe havefinisheddevelopingandevaluatingaspecializedtool for measuringtestingcoverage\nfor avarietyof metrics.\nWe havestartedusingthetool to collect informationon therelationshipsbetweensoftware\nfaults and coverageprovided by the testingprocessas measuredby different metrics\n(including dataflow metrics).We havefound considerablecorrelation betweencoverage\nprovidedby somehighermetricsandtheeliminationof faultsin thecode.\nWe havecontinuedstudyingback-to-backtestingasanefficientmechanismfor removalof\nun-correlatedfaults,andcommon-causefaultsof variablespan.\nWe continued studying software reliability estimation methods basedon non-random\nsampling,and the relationshipbetweensoftwarereliability and code coverageprovided\nthroughtesting.\nWe continued investigatingexisting, and worked on formulation of new fault-tolerance\nmodels.In particular,we have finishedsimulationstudiesof the AcceptanceVoting and\nMulti-stageVoting algorithms,andfoundthatthesetwo schemesfor softwarefault-tolerance\naresuperiorin manyrespectsto somecommonlyusedschemes.Particularlyencouragingare\nthesafetypropertiesof theAcceptancetestingscheme.\nThisreportdescribestheresultsobtainedin theperiodMarch 1,1989to August31,1989.\nNASA/NAG-1 983/Semi-AnnualReport/l.I/NCSU.CSC.(DFM,MAV)/Sep-89 6\n2. Results\n2.1 Fault-Avoidance Through Coverage Testing\nBGG, Basic Graph Generation and Analysis tool, was developed to help studies of static and\ndynamic software complexity, and testing coverage metrics. It is composed of several stand-alone\nmodules, it runs in UNIX environment, and currently handles static and dynamic analysis of\ncontrol and data flow graphs (global, intra-, and inter-procedural data flow) for programs written\nin full Pascal. Extension to C is planned. The tool is described in more detail in Appendix I where\nwe describe the structure of BGG, give details concerning the implementation of different metrics,\nand discuss the options it provides for treatment of global and inter-procedural data flow.\nI00\"\n90\nv\n80\n8\n7o\n6O\n50\n20-Version Set: Program P9\nUniformRandom\nFunctional\nNon-UniformRandom\n, - , ,\" , , , , ; ! - , _ , • , _ r, i ' \" \" ' \" ' ' \" ' I\n00 101 102 103\nNumber of Test Cases\nFigure 1. Comparison of linear block coverage observed for two random testing profiles and a\nfunctional data for a program out of the 20-version set.\nBGG is currently being used to obtain coverage growth curves for acceptance, and other, test data\nused in the RSDIMU experiment. Figure 1 illustrates the coverage growth curves we have\nobserved with random and functional (designed) test cases for the program program P9 (uclaD)\nusing an early version of the system.\n.NASA/NAG- 1-983/Semi-Annual Report/1. I/NCSU.CSC.(DFM,MAV)/Sep-89 7\nIt is interesting to note that coverage growth follows an exponential growth curve, and reaches a\nplateau extremely quickly. In the example, this happens after about 100 cases. Once the coverage is\nclose to saturation for a particular testing prof'fle, its fault detection efficiency drops sharply. This is\nillustrated in Figure 2 where we plot the coverage provided by the functional testing profile shown\nin Figure 1, and the cumulative number of different faults detected using these test cases. Out of\nthe 10 faults that the code contained, 9 were detected with the functional data set used within the\nfirst 160 cases.\nIt is clear that apart from providing static information on the code complexity, and dynamic\ninformation on the quality of test data in terms of a particular metric, BGG can also be used to\ndetermine the point of diminishing returns for a given data set, and help in making the decisions on\nwhen to switch to another testing profile or strategy.\n100\n90\n\"_ 70\n60\n50\n10 0\n10\nFunctional Test Data\n(20-Version Set\" Program Pg)\nDetected Faults i\n\\\n,, , • •\n6\nTotal Number of Faults [\nin the Program was 10 I 4\nCoverage 2\n101 102 103\nNumberof Test Cases\n4)\n::)\nu\nE\nt.>\nFigure 2. Linear block coverage and fault detection efficiency observed for program P9 with\nfunctional acceptance test cases.\nSimilar measurements have been taken for all 20 version of the RSDIMU set. We are currently\nstudying the correlation between higher metrics, such as p-uses [Fra88], and the reliability of the\nversions.\nNASA/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 8\n2.2 Fault-Avoidance: Using Back-to-Back Testing for Regression\nTesting\nAn interesting variant of back-to-back testing is its application to regression testing of a single\nprogram. Regression testing is typically conducted either during the production, or in the\nmaintenance phase, after modification of software. The intention is to check back on any changes,\nand make sure that the changes have not injected, and/or stopped masking, faults, or have corrupted\nalready tested functions and parts of the code. Sometimes it is possible to conduct regression testing\nusing all of the data available for testing, but often, due to execution time and schedule constraints, it\nis necessary to limit the regression testing to a smaller subset of the test data. An obvious problem\nthat arises during regression testing is the evaluation of the responses received from the newly\nmodified software. If the only failures of concern are self-reporting failures (e.g. system crash, or an\nobvious disruption of the computer service) a relatively simple acceptance test, or consistency check,\nmay be sufficient to verify the correctness of the answers. On the other hand, if the correctness of the\nresponses is less obvious, then a more elaborate, and often very time consuming, scheme must be\nused. Comparison of the answers with an existing, progressively generated and growing, database\nof \"correct\" answers is a natural solution.\nSome of the problems associated with regression testing may be:\n. Regression testing is limited to a smaller subset of the total data set. In this situation there is\nalways some doubt that the \"important\" test case(s), which could reveal an inadvertently injected\nbug, is(are) not part of the regression set. Regression testing could be limited to a subset for\nseveral reasons. For example, only a limited execution and calendar time is available for the\nregression testing. This can possibly be alleviated through parallel execution of mutually\nexclusive but exhaustive subsets of the full test set. Another problem, which may be more\ndifficult to resolve, is the storage problem. It is quite conceivable that the amount of storage\nrequired to record the input and output data for a complete set may be inordinately high.\nHowever, it is possible that the input set can nevertheless be reproduced, within an acceptable\ntime frame, using some generation algorithm, but that the output verification remains a problem.\n. Regression testing does not employ random data. There are indications that in some\ncircumstances random data may detect more faults than more conventional structured, partitioned\nand special value testing (e.g. EhE88, Ham88). Therefore, it is desirable to supplement testing\n\"i\nNAS A/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89\nbased on a designed (fixed or growing) test set with random test data. The problem is that, unless\nfailures are self-reporting, it may be very expensive to regression test with random data because\nof storage problems, answer correctness problems and similar.\n3, Regression testing does not monitor intermediate program states. There is experimental evidence\n(e.g. ShL88) that monitoring of internal program states can considerably enhance failure\ndetection efficiency of a testing approach. However, time, storage and correctness problems can\npresent a considerable deterrent to practical use of this technique for regression testing based on\nthe data-base approach.\nThere are, of course, other possible deficiencies of regression testing that could be discussed, not the\nleast of of them being diminished flexibility of \"fixed\" data regression sets to changes in the\noperational input profiles.\nObsolete\nOld\nNew\nEx[\ndifferen.\nComparator\nOK Warning\nFigure 3. Back-to-back regression testing.\nNASA/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV')/Sep-89 10\nOne approach that can help in solving at least some of the problems is back-to-back testing. One of\nthe primary problems with development use of back-to-back testing is the need for independently\ndeveloped multiple software versions in order to exploit fault detection properties of software\ndiversity. This can be expensive, and it is possible that some of the similar faults will not be detected.\nThis problem does not exist with regression testing. Regression testing is used primarily to make\nsure that any applied changes have not corrupted the code and functions that have already been tested\nand found correct. Because generation of a new version of the code is implicit in any software\nmodification, functionally \"almost-equivalent\" 2-tuples are available at no extra cost.\nThis means that the \"new\" and \"old\" versions of the code can be run against each other to verify\ninvariance of the the functions and responses that were not supposed to be affected by the applied\nchanges. A model of back-to-back regression testing data-flow is illustrated in Figure 3. The circles\ndepict two consecutive versions of the software, the squares the sources of data (files), and the\ndiamond the answer comparator. The response comparisons can be made at almost any desired level;\noutput only, module/function level, intermediate states, even line level. The nice part is that there is\npractically no problem with the insertion of the sampling probes because the code is not only\nfunctionally almost identical, but also structurally very similar (the differences, of course, exist in the\nmodified parts of the code).\nWe assume that three \"types\" of regression data are available. An invariant (\"old\") set, which\ncontains all the test cases which are still valid and completely unchanged following the program\nmodification. A set containing \"obsolete\" test cases, cases which are no longer valid because of\nchanged requirements, variable ranges, functionality of the code, and similar. And, finally, a set of\n\"new\" or changed test cases which contains all the test cases that had to be modified, or were\ngenerated completely anew, to accommodate the changes in the functionality and structure of the\ncode. One file, \"expected differences\", contains a \"list\" of test cases (and responses) for which the\ndifferences between the \"old\" and \"new\" code versions would be expected to arise. This data needs\nto be generated, based on performed modification(s), prior to any regression testing. For example, if\nupward compatibility of versions is required because the changes are enhancements which should not\naffect previous performance (e.g. and extension of a communication protocol), then all of the \"old\"\ndata set responses for key parameters should match (except for new variables), while the \"expected\ndifferences\" will derive primarily from the \"new\" data set.\niNAS A/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 11\nThere are two general output states of the system. The system either issues a warning, or it accepts\nthe comparison (OK event). In principle, only unexpected differences or unexpected agreement\nbetween the outputs should raise an alarm. However, it is prudent to re-examine all outputs where\ndifferences arise unless the size and sign of the expected differences is included in the data base.\nUnexpected disagreements between the versions may be indicative of incompletely corrected faults,\nnewly introduced faults, or old faults that are no longer masked owing to the implemented code\nchanges. The question of tolerances, and false alarms should also be considered [Vou88a]. It is also\npossible that an expected difference in response does not materialize. This should also be the cause\nfor alarm. The cause could be, for example, that the implemented change was not successful\n(although not detrimental), or that there is a fault in the test case, etc. The states are illustrated in\nFigure 4.\nA special case is the use of randomly generated data in regression testing. These data sets (either\ngenerated dynamically, or in part stored) can be used to probe for possible omissions and \"holes\" in\nthe regular regression test set. A big advantage that the testing successive versions back-to-back\noffers is that the random input data, and the corresponding answers, do not have to be stored but can\nbe generated during the testing. Furthermore, the range and the profile of these test cases can be\nreadily changed to accommodate a different operational profile without a (possibly) costly re-\ngeneration of the regression data base.\nWARNING\nment\nInitial\n__.._ . u x.peTddifferencene\nrerence\nFigure 4.Transition states for (back-to-back) regression testing.\nNASA/NAG- 1-983/Semi-Annual Report]l. 1fNCSU.CSC.(DFM,MAV)/Sep-89 12\nAnother obvious advantage of using back-to-back regression testing is that a very large number of\nvariables and intermediate states can be monitored relatively cheaply. This should increase sensitivity\nof the testing to any anomalies introduced or revealed during the modifications. Furthermore,\nprobing of intermediate states and classification of the expected outputs according to whether a\ndifference would, or would not, be observed with respect to the earlier version can yield useful\ninformation about the expected and actual coupling of, and dependencies within, the code (c.f.\nperturbation or mutation testing).\nThe cost efficiency of back-to-back regression testing depends on the available resources, and on the\nnature of the failures. It is shown that the process is not cost-effective if mainly self-reporting\nfailures (differences) are present after the modification, and if the available resources allow for a fast\ntable look-up of the answers. However, the technique becomes particularly effective if random\ntesting is used to supplement regression data sets, a large number of intermediate states is monitored,\nor there are frequent changes in the operational profile and variable ranges between versions, and, of\ncourse, if there are storage problems but input data can be dynamically reproduced.\nWe are currently in the process of using the incremental correction versions of RSDIMU software to\nverify usefulness and efficiency of regression back-to-back testing.\n2.3 Safety Properties of Some Hybrid Fault-Tolerance Schemes\nThe performance of classical Majority voting, and of some more reliable hybrid models such as\nConsensus Recovery Block (CRB) model, deteriorates if the output space is reduced. Binary\noutput space is an extreme case where CRB acts as a simple voter, and the acceptance test is never\ninvoked. This lead us to develop a new hybrid models which with better performance in reduced\noutput space. One ways is to use a better voting strategy (e.g. Consensus Majority Voting\n[Sun85]. Another is to reduce, or completely eliminate, as many wrong answers as possible before\nvoting.\nThe model of the scheme we discuss here is called Acceptance Voting (AV). It is\nillustrated in Figure 5. N functionally equivalent software versions are independently developed,\ntogether with an acceptance test, and a voting procedure. When AV is invoked, all versions execute\nand submit their outputs for acceptance testing. All answers are acceptance tested. Only the outputs\nNASA/NAG-1-983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 13\nthatpasstheacceptancetest continue on to the voter. Each time the model is invoked it may vote\nwith a different number of outputs, depending on how many results were passed to the voter by\nthe acceptance testing. The voting may be done using any suitable voting scheme. We have\nexamined the influence of three voting schemes, the two-out-of-n voting (2N) [SCO87], the\nmajority voting (MV) and the dynamic majority voting.\nes_l_ Version _n\ncorrect r g result\n1-oc (x\nreject __ accept accept_ reject\nccess fa_l e\nFigure 5 Block diagram of the Acceptance Voting model\nTwo-out-of-N and Majority voting are well known. In the case of AV we define Dynamic Majority\nvoting in the following way. In dynamic majority voting the agreement number is\nm = Ceiling [(k1._._)]+\nwhere k is the number of results passed to the voter, and not N. It is important to mention that k\nchanges dynamically, hence m is different for each run. The difference between the dynamic\nvoting and majority voting is that even if a small number of results are passed to the voter, dynamic\nvoting will try to find the majority among them. Majority voting will fail if there are less than\nmajority of the answers passed to voter. Thus, it is better solution than a fixed agreement number\nused in a majority voting scheme.\nNASA/NAG-1-983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 14\nA systemmay bedescribedassafety-criticalif anexecutiontime failureresult in\ndeath,injury, lossof equipmentorproperty,or environmentalharm[LEV87]. All failures are not\nof equal consequences, and a relatively small number of failures are catastrophic in nature. The\naim is to eliminate all failures, if possible; if not, as many as possible. This implies that software\nreliability should be increased or some techniques such as fault-tolerant should be used. A common\ntheory, that a reliable software system is also safe is not necessarily true. This is because, a\nreliable software may fail causing a catastrophe, on the other hand a less reliable software may fail\nmore number of times, but causing non-vital failures. In fact, it may be desirable to trade a certain\namount of overall reliability, for higher safety.\nWe classify failures into two groups: safe failures and unsafe failures. When\na) System outputs a wrong result as a correct, we have an unsafe failure,\nb) System can not decide on the correctness of a result, and is unable to output an answer, but\nis \"aware\" of the fact that it will fail, and can therefore forward this knowledge to the user,\nwe have an safe failure.\nIn Figures 6 and 7 we illustrate our results through the number of observed unsafe-failures (out of\na total 100,000 simulation test cases) against the version reliability, for three different methods. We\nhave shown results for binary output space, an extreme situation which approximates the safety\nbehavior of the system in the presence of highly dependent failures, and 131=13,2=_.\nIn binary output space Consensus Recovery Block acts as a simple voter and is equivalent to N-\nversion programming with majority voting. There is always an answer, right or wrong, that may\nsatisfy required number of agreeing versions. Voter will output this answer as a correct, which\nmay result in an unsafe-failure. The number of unsafe-failures in the AV model is lower than in\nCRB model. This is because in AV acceptance testing removes most of the wrong answers, there\nby reducing the probability of them to have the required agreement.\nAcceptance test reliability (1 - 13) does not affect unsafe-failures in Consensus Recovery Block\nmodel because in binary space the test is never invoked (for odd N). Unfortunately, at the same\ntime, every CRB failure is an unsafe one. Situation improves with larger effective output space\ncardinality (decision space), but CRB model exhibits a higher number of unsafe-failures than AV\nmodel under any output space cardinality or voting strategy. In AV, as the acceptance test\ndeteri rates ([3 is increases), the number of unsafe-failures is increases for the same version\nreliability.\nNAS A/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC. (DFM,MAV)/Sep-89 15\no\nt_\n100000\n80000\n60000\n40000\n20000\nSafety Analysis (2-out-of-N Voting, Binary Space, N=3)\nConsensus Recovery Block\nAcceptance Voting 15=0.3\nAcceptance Voting 13--0.2\nAcceptance Voting [5--0.1\n0\nZ 0.0 0.2 0.4 0.6 0.8 1.0\nVersion Reliability\nFigure 6 Number of Unsafe Failures (out of 100,000 runs) under 2-out-of-N voting vs. version\nreliability assuming binary output space, for N -- 3, [3 = 0.1\nv.\"4\n©\nSafety Analysis(Dynamic Majority Voting, Binary Output Space, N=3)\n100000\n80000\nConsensus Recovery Block\nAcceptance Voting [3=0.3\n60000\n_, Acceptance Voting 15--0.2\n_._ 40000 Acceptance Voting 13----0.1\n20000\no\nZ 0.0 0.2 0.4 0.6 0.8 1.0\nVersion Reliability\nFigure 7 Number of Unsafe Failures (out of 100,000) under Dynamic Majority Voting\nvoting vs. version reliability under binary output space, for N = 3\nNASA/NAG- 1-983/Semi-Annual ReporV1.1/NCSU.CSC.(DFM,MAV)/Sep-89 16\n2.4 Other Work in Progress\n1. We are empirically validating Consensus Recover3/Block and Majority Consensu_ voting\nmechanisms.\n. We continue to investigate cost-effectiveness of multi-version development, testing, and run-\ntime fault-tolerance approaches (assuming single-stage and multi-stage voting). The strategies\nand methods are being evaluated with respect to the development of a single ultra-high reliability\ncomponent. Of special interest are the fault-avoidance properties offered by multi-version\nsoftware development, fault-elimination properties of back-to-back testing, and cost-efficient\ndetection and elimination of correlated faults.\n. We continue to investigate software reliability models in order to provide a basis for estimation\nof the reliability of the components making up a fault-tolerant software (F'FS) system. Software\ntestability modeling, based on control and data flow construct coverage, is being conducted. A\ncoverage based software reliability model will be developed and used as part of the FTS\nreliability modeling process.\n. We continue to study single stage and multistage voting and fault-tolerant software performance\nissues. Particular attention is directed towards incorporation of the failure dependencies\n(positive or negative correlation) into the methods and models used to predict (estimate)\nreliability offered by a particular fault-tolerance mechanism or strategy.\n5. We continue to investigate empirical multi-version software properties. For this we are using the\ncode developed during the summer 1987 RSDIMU certification effort\nNASA/NAG-1 983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 17\nBibliography\n[Adr82]\n[AnR74]\n[Avi77]\n[Avi84]\n[Avi85]\n[Avi87]\n[Avi88]\n[Bha81]\n[Bis86]\n[Bis88]\n[Che78]\n[Cri85]\n[Dur84]\n[Eck85]\n[Ehr85]\n[EhE88]\n[Gi177]\n[Gme79]\n[Grn80]\n[Ham88]\n[Hec79]\n[Hi185]\nW.R. Adrion, M.A. Branstad and J.C. Cherniavsky, \"Validation, verification, and\ntesting of computer software\", ACM Computing Survey, Vol 14(2), 159-192, 1982.\nR. Anderson, \"Proving programs correct\", J. Wiley, 1974\nA. Avizienis and L. Chen, \"On the Implementation of N- version Programming for\nSoftware Fault-Tolerance During Program Execution\", Proc. COMPSAC 77, 149-155,\n1977.\nA. Avizienis and P.A. Kelly, \"Fault-Tolerance by Design Diversity: Concepts and\nExperiments\", Computer, Vol. 17, pp. 67-80, 1984.\nA. Avizienis, \"The N-Version Approach to Fault-Tolerant Software,\" IEEE Transactions\non Software Engineering, Vol. SE-11 (12), pp 1491-1501, 1985.\nA. Avizienis and D.E. Ball, \" On the Achievement of a Highly Dependable and Fault-\nTolerant Air Traffic Control System,\" IEEE Computer, Vol. 20 (2), pp 84-90,1987.\nA. Avizienis, M.R. Lyu, and W. Schutz, \"In Search of Effective Diversity: A six-\nLanguage Study of Fault-Tolerant Flight Control Software,\", Proc. FTCS 18, pp 15-22,\nJune 1988.\nB. Bhargava and C. Hua, \"Cost Analysis of Recovery Block Scheme and Its\nImplementation Issues,\" Int. Journal of Computer and Information Sciences, Vol. 10 (6),\npp 359-382, 1981.\nP.G. Bishop, D.G. Esp, M. Barnes, P Humphreys, G. Dahl, and J. Lahti, \"PODS--A\nProject on Diverse Software\", IEEE Trans. Soft. Eng., Vol. SE-12(9), 929-940, 1986.\nP.G. Bishop, and F.D. Pullen, \"PODS Revisited--A Study of Software Failure\nBehavior\", Proc. FTCS 18, pp 2-8, June 1988.\nL. Chen and A. Avizienis, \" N-Version Programming: A Fault-Tolerance Approach to\nReliability of Software Operation,\" Proc. FTCS 8, Toulouse, France, pp 3-9, 1978.\nF. Cristian, \"A rigorous approach to fault-tolerant programming\", IEEE Trans. Soft.\nEng. Vol. SE-11, 23-31, 1985.\nJ.W. Duran and S.C. Ntafos, \"An evaluation of random testing\", IEEE Trans. Soft.\nEng., Vol. SE-10, 438-444, 1984\nD.E. Eckhardt, Jr. and L.D. Lee, \"A Theoretical Basis for the Analysis of Multi-version\nSoftware Subject to Coincident Errors\", IEEE Trans. Soft. Eng., Vol. SE-11(12), 1511-\n1517, 1985.\nW. Ehrenberger, \"Statistical Testing of Real Time Software\", in \"Verification and\nValidation of Real Time Software\", ed. W.J. Quirk, Springer-Verlag, 147-178, 1985.\nWilla K. Ehrlich, and Thomas J. Emerson, \"The Effect of Test Strategy on Software\nReliability Measurement,\" 11 th Minnowbrook Workshop on Software Reliability, July\n1988.\nT. Gilb, Software Metrics, Winthrop Publishers Inc., Cambridge, Massachusetts, 1977.\nL. Gmeiner and U. Voges, \"Software Diversity in Reactor Protection Systems: An\nExperiment,\" Proc. IFAC Workshop SAFECOMP '79, pp 75-79, 1979.\nA. Grnarov, J. Arlat, and A. Avizienis, \"On the Performance of Software Fault-\nTolerance Strategies,\" Proc. FTCS 10, pp 251-253, 1980.\nD. Hamlet, and R. Taylor, \"Partition Testing Does not Inspire Confidence,\" 2nd\nWorkshop on Software Testing, Verification and Analysis, Banff, IEEE Comp. Soc., pp\n206-215, July 1988.\nH. Hecht, \"Fault Tolerant Software\", IEEE Trans. Reliability, Vol. R-28, pp. 227-232,\n1979\nA.D. Hills, \"Digital Fly-By-Wire experience\", Proc. AGARD Lecture Series (143),\nOctober 1985.\nNASA/NAG-1-983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 18\n[Hor87]\n[How82]\n[How87]\n[Ke188]\n[Kni86]\n[Las83]\n[Lit87]\n[Mad84]\n[Mar821\n[McA85]\n[McA86]\n[McA87]\n[Mus87]\n[Nag82]\n[Nag84]\n[Neu85]\n[Pan81]\n[Ram81]\n[Ram82]\n[Ran75]\n[Rap85]\n[Sag86]\n[Sco84]\nC.A.R. Hoare, \"An Overview of Some Formal Methods for Program Design,\" IEEE\nComputer, September 1987, pp 85-91.\nW.E. Howden, \"Validation of scientific programs\", ACM Computing Surveys, Vol.\n14(2), 193-227, 1982.\nW.E. Howden, \"Functional Program Testing and Analysis\", McGraw-Hill Book Co.,\n1987.\nJ. Kelly, D. Eckhardt, A. Caglayan, J. Knight, D. McAllister, M. Vouk, \"A Large Scale\nSecond Generation Experiment in Multi-Version Software: Description and Early\nResults\", Proc. FTCS 18, pp 9-14, June 1988.\nJ.C. Knight and N.G. Leveson, \"An Experimental Evaluation of the assumption of\nIndependence in Multi-version Programming\", IEEE Trans. Soft. Eng., Vol. SE-12(1),\n96-109, 1986.\nJ.W. Laski and B Korel, \"A data oriented program testing strategy\", IEEE Trans. Soft.\nEng., Vol. SE-9, 347-354, 1983\nB. Littlewood, and D.R. Miller, \"A Conceptual Model of Multi-Version Software,\"\nFTCS 17, Digest of Papers, IEEE Comp. Soc. Press, pp 150-155, July 1987.\nW.A. Madden, and K.Y. Rone, \"Design, Development, Integration: Space Shuttle\nPrimary Flight Software System\", Comm. of the ACM, Vol. 27(8), 902-913, 1984.\nD.J. Martin, \"Dissimilar Software in High Integrity Applications in Flight Controls\",\nProc. AGARD - CP 330, 36.1-36.13, September 1982.\nD.F. McAllister, \"Some observations on costs and reliability in software fault-tolerant\ntechniques\", TIMS-ORSA Conference, Boston, Mass., April 1985\nD.F. McAllister, M.A. Vouk and S.W. Yeh, \"Execution Time Distributions for\nMultiversion Software Systems\", 1986,\nD.F. McAllister, C.E. Sun, and M.A. Vouk, \"Reliability of Voting in Fault-Tolerant\nSoftware Systems for Small Output Spaces\", North Carolina State University,\nDepartment of Computer Science, Technical Report, TR-87-16, 1987, submitted to IEEE\nTrans. Reliability.\nJ. Musa, A. Iannino, and K. Okumoto, \"Software Reliability: Measurement, Prediction,\nApplication,\" McGraw-Hill Book Co., 1987.\nP.M. Nagel and J.A. Skrivan, \"Software Reliability: Repetitive Run Experimentation and\nModeling\", BSC-40366, Boeing, Seattle, Wa., 1982\nP.M. Nagel, F.W. Scholz and J.A. Skrivan, \"Software Reliability: Additional\nInvestigation into Modeling with Replicated Experiments\", NASA CR172378, Boeing,\nSeattle, Wa., 1984\nP.G. Neumann, \"Some computer related disasters and other egregious horrors\", ACM\nSIGSOFT, Software Engineering Notes, Vol. 10(4), 6-9, 1985; (now a regular SEN\ncolumn called \"Risks to public in computer systems\" by the same author).\nD.J. Panzl, \"A Method for Evaluating Software Development Techniques\", The Journal\nof Systems Software, Vol. 2, 133-137, 1981.\nC.V. Ramamoorthy, Y.K.R. Mok, F.B. Bastani, G.H. Chin and K. Suzuki,\n\"Application of a Methodology for the development and validation of reliable process\ncontrol software,\" IEEE Trans. Soft. Eng., Vol. SE-7 (6), 537-555, 1981.\nC.V. Ramamoorthy and F.B. Bastani, \"Software reliability - status and perspectives\",\nIEEE Trans. Soft. Eng., Vol. SE-8, 354-371, 1982\nB. Randell, \"System structure for software fault-tolerance\", IEEE Trans. Soft. Eng.,\nVol. SE-1,220-232, 1975.\nS. Rapps and E.J. Weyuker, \"Selecting software test data using data flow information\",\nIEEE Trans. Soft. Eng., Vol. SE-I1,367-375, 1985\nF. Saglietti and W Ehrenberger, \"Software Diversity -- Some Considerations about\nBenefits and its Liv _.tions ', Proc. IFAC SAFECOMP '86, 27-34, 1986.\nR.K. Scott, J.W. _ault, D.F. McAllister and J. Wiggs, \"Investigating Version\nDependence in Fault-Tolerant Software\", AGARD 361, pp. 21.1-21.10, 1984\nNAS A/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 19\n[Sco87]\n[Shi85]\n[ShL88]\n[Spe84]\n[Str85] L.\n[Tha78]\n[Tra88]\n[Tro85]\n[Vog88a]\n[Vog88b]\n[Vou85]\n[Vou86a]\n[Vou86b]\n[Vou88_\n[Vou88b]\n[Vou88c]\n[Vou88d]\n[Wey88]\n[Wi1831\nR.K. Scott, J.W. Gault and D.F. McAllister, \"Fault-Ti_lerant Software Reliability\nModeling\", IEEE Trans. Software Eng., Vol SE-13 (5), 582-592, 1987.\nK.G. Shin and .H. Lee, \"Evaluation of error recovery blocks used for cooperating\nprocesses\", IEEE Trans. Soft. Eng., Vol. SE-10, 692-700, 1985\nT.J. Shimeall and N.G. Leveson, \"An Empirical Comparison of Software Fault-\nTolerance and Fault Elimination,\" 2nd Workshop on Software Testing, Verification and\nAnalysis, Banff, IEEE Comp. Soc., pp 180-187, July 1988.\nA. Spector, and D. Gifford, \"The Space Shuttle Primary Computer System\", Comm. of\nthe ACM, Vol. 27(8), 874-900, 1984.\nStrigini and A. Avizienis, \"Software Fault-Tolerance and Design Diversity: Past\nExperience and Future Evolution\", Proc. IFAC SAFECOMP '85, 167-172, 1986.\nR.A. Thayer, M. Lipow and E.C. Nelson, \"Software Reliability\", North-Holland,\nAmsterdam, The Netherlands, 1978\nP. Traverse, \"AIRBUS and ATR System Architecture and Specification,\" in [Vog88a],\npp 95-104, 1988.\nR. Troy and C. Baluteau, \"Assessment of Software Quality for the Airbus A310\nAutomatic Pilot\", Proc. FTCS 15, Ann Arbor, USA, (IEEE CS Press), 438-443, June\n1985.\nU. Voges (ed.), Software Diversity in Computerized Control Systems, Springer-Verlag,\nWien, Austria, 1988.\nU. Voges, \"Use of Diversity in Experimental Reactor Safety Systems,\" in [Vog88a], pp\n29-49, 1988.\nM.A. Vouk, D.F. McAllister, K.C. Tai, \"Identification of correlated failures of fault-\ntolerant software systems\", in Proc. COMPSAC 85, 437-444, 1985.\nM.A. Vouk, D.F. McAllister, and K.C. Tai, \"An Experimental Evaluation of the\nEffectiveness of Random Testing of Fault-tolerant Software ', Proc. Workshop on\nSoftware Testing, Banff, Canada, IEEE CS Press, 74-81, July 1986.\nM.A. Vouk, M.L. Helsabeck, K.C. Tai, and D.F. McAllister, \"On Testing of\nFunctionally Equivalent Components of Fault-Tolerant Software\", Proc. COMPSAC 86,\n414-419, 1986.\nM.A. Vouk, D. F. McAllister, D.E. Eckhardt, A. Caglayan, and J.P.J. Kelly, \"Analysis\nof Faults Detected in a Multiversion Software Testing Experiment,\" North Carolina State\nUniversity, Department of Computer Science, Technical Report, TR-88-10, 1988.\nM.A. Vouk, \"On Back-To-Back Testing,\" Proc. COMPASS '88, pp 84-91, June 1988.\nVouk, M.A., \"On Growing Software Reliability Using Back-To-Back Testing,\" Proc.\n1lth Minnowbrook Workshop on Software Reliability, pp, July 1988.\nVouk M.A., \"On the Cost of Back-to-Back Testing,\" Proc.6th Annual Pacific\nNorthwest Software Quality Conference, Lawrence and Craig, Inc., Portland, OR,\npp263-282, September 1988.\nE.J. Weyuker, \"An Empirical Study of the Complexity of Data-Flow Testing,\" 2nd\nWorkshop on Software Testing, Verification and Analysis, Banff, IEEE Comp. Soc.,\npp 188-195, July 1988.\nJ.F. Williams, L.J. Yount, and J.B. Flanningan, \"Advanced Autopilot Flight Director\nSystem Computer Architecture for Boeing 737-300 Aircraft,\" Proc. 5th Digital Avionics\nSystems Conference, Seattle, WA, 1983.\nNAS A/NAG- 1-983/Semi-Annual Report]l. 1/NCSU.CSC.(DFM,MAV)/Sep-89 20\nAppendix !\nBGG: A Testing Coverage Tool 1\nMladen A. Vouk and Robert. E. Coyle 2\nNorth Carolina State University\nDepartment of Computer Science, Box 8206\nRaleigh, N.C. 27695-8206\nAbstract\nBGG, Basic Graph Generation and Analysis tool, was developed to help studies of static and\ndynamic software complexity, and testing coverage metrics. It is composed of several stand-alone\nmodules, it runs in UNIX environment, and currently handles static and dynamic analysis of\ncontrol and data flow graphs (global, intra-, and inter-procedural data flow) for programs written\nin full Pascal. Extension to C is planned. We describe the structure of BGG, give details\nconcerning the implementation of different metrics, and discuss the options it provides for\ntreatment of global and inter-procedural data flow. Its capabihties are illustrated through examples.\nIResearch supported in part by NASA Grant No. NAG-I-983\n2Teletec Corporation, Raleigh, N.C.\n• NASA/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.fDFM,MAV)/Sep-89 21\nI. Introduction\nBGG: A Testing Coverage Tool\nMladen A. Vouk and Robert. E. Coyle 3\nNorth Carolina State University\nDepartment of Computer Science, Box 8206\nRaleigh, N.C. 27695-8206\nSoftware testing strategies and metrics, and their effectiveness, have been the subject of numerous\nresearch efforts (e.g. comparative studies by Nta88, Cla85, Wei85, and references therein).\nPractical testing of software usually involves a combination of several testing strategies in hope that\nthey will supplement each other. The question of which metrics should be used in practice in order\nto guide the testing and make it more efficient remains largely unanswered, although several basic\ncoverage measures seem to be generally considered as the minimum that needs to be satisfied\nduring testing.\nStructural, or \"white-box\", approaches use program control and data structures as the basis for\ngeneration of test cases. Examples include branch testing, path testing [Hen84, WooS0] and\nvarious data flow approaches [Hec77, Las83, Rap83, Fra88]. Functional, or \"black-box\",\nstrategies rely on program specifications to guide test data selection [e.g. How80,87, Dur84].\nSome of the proposed strategies combine features of both functional and structural testing as well\nas of some other methods such as error driven testing [Nta84].\nStatement and branch coverage are regarded by many as one of the minimal testing requirements; A\nprogram should be tested until every statement and branch has been executed at least once, or has\nbeen identified as unexecutable. If the test data do not provide full statement and branch coverage\nthe effectiveness of the employed testing strategy should be questioned. Of course, there are a\nnumber of other metrics which can provide a measure of testing completeness. Many of these are\nmore sophisticated and more sensitive to the program control and data flow structure than statement\nor branch coverage. They include path coverage, domain testing, required elements testing, TERn\n(n>3) coverage, etc. [How80, Hen84, Whi80, Nta84 and reference therein].\n3Teletec Corporation, Raleigh, N.C.\nNASA/NAG-1-983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)ISep-89 22\nThesimplestdata-flow measureis thecountof definition-usepairsor tuples[Her76].Thereare\nseveralvariantsof this measure.More sophisticatedmeasuresarep-uses,all-uses,anddu-paths\n[Fra88,Nta88], ordereddatacontexts[Las83], requiredpairs [Nta84,88],andsimilar. Thedata-\nflow basedmetricshavebeenunderscrutinyfor sometime now aspotentiallybettermeasuresof\nthe testing quality than control-flow basedmetrics [e.g. Las83, Rap83, Fra88, Wey88].\nHowever, one recent study [Zei88] indicatesthat most of the data-flow metrics may not be\nsufficiently completefor isolateduse,andthatin practicetheyshouldbecombinedwith control-\nflow basedmeasures.\nOvertheyearsa numberof softwaretoolsfor measuringvariouscontrol anddataflow properties\nandcoverageof softwarecodehavebeenreported[e.g.Ost76(DAVE), Fra86(ASSET),Kor88].\nUnfortunately,in practicethesetoolsareeitherdifficult to obtain,or difficult to adaptto specific\nlanguagesandresearchneeds,or both.To circumventthat, and alsogain betterinsight into the\nproblematicsof buildingtestingcoveragetools,wehavedevelopedasystemfor staticanddynamic\nanalysisof controlanddataflow in software.\nThesystem,BGG (BasicGraphGenerationandAnalysissystem),wasbuilt asaresearchtool to\nhelpunderstand,study,andevaluatethemanysoftwarecomplexityandtestingmetricsthat have\nbeenproposedasaidsin producingbetterquality softwarein an economicalway. BGG allows\ncomparisonof coveragemetrics and evaluationof complexity metrics. It can also serveasa\nsupport tool for planning of testing strategies(e.g. stoppingcriteria), as well as for active\nmonitoringof thetestingprocessanditsquality in termsof thecoverageprovidedby thetestcases\nused.SectionII of the paperprovidesanoverviewof theBGG systemstructureand functions.\nSectionIII givesdetailsconcerningtheimplementationof variousmetricsandof handlinglocal,\nglobalandinter-proceduraldataflow. SectionIV illustratesthetool capabilitiesthroughexamples.\nII. Structure and Functions\nA simplified top level diagram of BGG is shown in Figure 1. BGG is composed of several\nmodules which can be used as an integrated system, or individually given appropriate inputs, to\nperform static and dynamic analyses of control and data flow in programs written in Pascal. The\ntool currently handles full Berkeley Pascal 4 with one minor exception. The depth of the \"with\"\nstatement nesting is limited to one. The extension to greater depth is simple and will be\nimplemented in the next version of the system. BGG runs in UNIX environment. Its\n4Standard UNIX compiler, pc.\nNASA/NAG-1 983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 23\nimplementationunderVM/CMS is plannedtogetherwith its extensionto analysisof programs\nwritten in the C language. BGG itself is written in Pascal, C and UNIX C-shell script.\nTerminal\nlied\nGraph\nAnalys\nStatic\nInstrumented !Sou ce\nCompiler(\nInstrumented\nProgram\n_ Source\nCode\nFile\n}h Code\nLanguage\nTables (FMQ)\nTest Data\npipe Dynamic\nCoverage\nAnalysis\nFigure 1. Schematic diagram of the information flow in the BGG system of tools.\nBGG pre-processor provides the user interface when the tool is used as an integrated system. It\nalso performs some housekeeping chores (checks for file existence, initializes appropriate language\ntables and files, etc.), and prepares the code for processing by formatting it and stripping it of\ncomments. The language tables are generated for the system once, during the system installation,\nand then stored. The front-end parsing is handled through the FMQ generator [Mau81, Fis88].\nNASA/NAG-1-983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 24\nThis facility also allows for relatively simple customizationof the systemregardingdifferent\nprogramminglanguagesand languagefeatures.Also, eachof the BGG moduleshasa set of\nparameterswhich canbeadjustedto allow analysesof problemswhich may exceedthedefault\nvaluesfor thenumberof nodes,identifierlengths,nestingdepth,tablesizes,etc.\nPre-processedcode,variouscontrolinformationandlanguagetablesareusedasinputto theBGG-\nStatic processor.This processorconstructscontrol anddata-flow graphs,and performsstatic\nanalysisof the code.Thesegraphsare the basisfor all further analyses.Statisticson various\nmetricsandcontrol-flowanddata-flowanomalies,suchasvariablesthatareusedbutneverdefined\netc,arereported.BGG-Staticalsoinstrumentsthecodefor dynamicexecutiontracing.\nWhen requested,BGG executestheinstrumentedcodewith providedtestcasesandanalyzesits\ndynamic execution trace through BGG-Dynamic. The dynamic analysis output contains\ninformation (by proceduresandvariables)aboutthecoveragethat thetest casesprovideunder\ndifferentmetrics.\nWheninstrumentingcodeBGG insertsacall to aspecialBGGprocedureatthebeginningof each\nlinear code block. It also addsempty blocks to act as collection points for branches.The\ninstrumentationoverheadin executablestatementsis roughlyproportionalto thenumberof linear\nblockspresentin thecode.In ourexperiencethiscanaddbetween50%and80%to thenumberof\nexecutablelines of code. The run-time tracing overheadfor the instrumentedprogramsis\nproportionalto thenumberof linearblocksof codetimesthecostof thecall to theBGG tracing\nprocedure.Thelattersimplyoutputsinformationabouttheblockandtheprocedurebeingexecuted.\nTheraw run-timetracinginformationmaybestoredin temporaryfiles, andprocessedby BGG-\nDynamic later. However,often the amountof raw tracing information is so large that that it\nbecomesimpracticalto storeit. BGG-Dynamiccanthenacceptinput via apipeandprocessit on-\nthe-fly.BecauseBGG-Dynamicanalysesmaybevery memoryandCPUintensive,particularlyin\nthecaseof data-flowmetrics,interactivetestingmaybeaslowprocess.Partof theproblemliesin\nthefact thatBGGis still a researchtoolandwasnotoptimized.Weexpectthatthenextversionof\nBGG will be muchfasterandmoreconservativein its useof memory.It will permit splicing of\ninformation from severalshort independentruns,sothatprogressivecoveragecanbecomputed\nwithoutregressionrunsonalreadyexecuteddata.\nk\nNASA/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.('DFM,MAV)/Sep-89 25\nCurrently BGG computes the following static measures: counts of local and global symbols, lines\nof code (with and without comments), total lines in executable control graph nodes, linear blocks\nof code, control graph edges and graph nodes, branches, decision points, paths (the maximum\nnumber of iterations through loops can be set by the user), cyclomatic number, definition and use\ncounts for each variable, definition-use (du) pair counts, definition-use-redefinition (dud) chain\ncounts, count of definition-use paths, average definition-use path lengths, p-uses, c-uses, and all-\nuses. Dynamic coverage is computed for definition-use pairs, def'mition-use-redefinition chains, p-\nuses, c-uses and all-uses. Definition-use path coverage and path coverage for paths that iterate\nloops k times (where k can be set by user) will be implemented. There are several system switches\nwhich allow selective display and printing of the results of the analyses.\nIII. Graphs and Metrics\nControl and data flow graphs\nEach linear block of Pascal code is a node in a graph. A linear code sequence is a set of simple\nPascal statements (assignments, I/O, and procedure/function calls ), or it is a decision statement of\nan iterative or conditional branching construct. When a linear block is entered during execution all\nof its statements are guaranteed to be executed. Decision statements are always separated out into\nsingle \"linear blocks\". Procedure/function calls are treated as simple statements which use or define\nidentifiers and/or argument variables. A linear block node has associated with it a set describing\nvariables defined in it, and a set describing variables used in it. Also attached to each node is the\nnode execution information.\nIn each Pascal statement all identifiers for simple local and global variables, named constants\ndefined using CONST, and all built-in Pascal functions are considered. Built in functions are\ntreated as global identifiers. For the purpose of the definition-use analyses, explicit references to\nelements of an array are treated as references to the array identifier only. Similarly, references to\nvariables pointed to by pointers are currently reduced to references to the first pointer in the pointer\nchain. An extension that will differentiate between a stand-alone use of a pointer (e.g. its def'mition\nor use in a pointer expression), and use of a pointer, or a pointer chain, for de-referencing of\nanother variable, will be implemented in the next version of the tool. Input/output statement\nidentifiers (function names) are considered used, while their argument variables are used (e.g.\nwrite, writeln) or defined (e.g. read, readln). The file identifier is treated as a simple variable\n(defined for input, used for output).\nNASA/NAG-1-983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 26\nCalls to functionsor proceduresaretreatedaslocal statementswhich usetheprocedure/function5\nidentifier. In the caseof function calls this useis precededby oneor moredefinitions of the\nfunction identifier in thecalled function itself. This definition is propagatedto thepoint of call,\nwhereasingledefinitionof thefunctionidentifier is thenfollowedby its local use.From thepoint\nof view of thecalling procedure,the actualargumentvariablesareeither usedonce,or defined\nonce,or both usedand definedonce(in that order), dependingon whether the corresponding\nparameteris used(anynumberof times),defined(anynumberof times),or usedanddefined (in\nany order) in the procedurethat is called. Definitions are returned_ if the corresponding\nparameteris avar parameter.\nThe point of call ordering: used-defined, for var parameters used and defined in any order, was\nchosen as a warning mechanism for programmers that have access to analyses of their own code\nbut may not have access to the analyses, or the actual code, of the procedures they call. The idea is\nto impress on the programmers that the variable may be used in the invoked unit, and therefore\nthey should be careful about what they send to it because the definition may not mask an undefined\nargument variable, an illegal value, etc. The way we handle procedure arguments permits a limited\nform of inter-procedural data flow analysis, and offers a more realistic view of the actual flow of\ninformation through the code. It also means that the code for the called procedures must be\navailable for BGG to analyze. An alternative is not to use this option, but use the defensive\napproach of assuming that every argument variable of a var parameter is always used and then\ndefined.\nA global variable that has been _ in a called procedure, or used in the procedures called\nwithin the called procedure, is reported as used at the point of call. A global variable that has been\n9.IILV....__IB_ in the called procedure, or deeper, is reported defined at the point of call. However, a\nglobal variable that has been used and defined (in any order) in the called procedure, or in any\nprocedure called within the called procedure to any depth, is reported as defined and then used at\nthe point of call. The reason global variables are treated differently from procedure arguments is to\nhighlight global variable definition in the called procedure(s) by making it visible as a definition-\nuse pair at the point of call. Again, it is a form of warning to the programmers that the underlying\nprocedures have changed a global variable value, may have re-used this value, and in turn may\n5From here on, we use term \"procedure\" to mean procedure or function, unless a distinction has to be made between\nthe two.\nNASA/NAG-1-983/Semi-AnnualReport/1.I/NCSU.CSC.(DFM,MAV)/Sep-89 27\nhave(if thedefinition waserroneous)affectedvaluesof some,or all, theparametervaluespassed\nbacktothepoint of call.\nAll procedureparametersareassumedto bedefined(pseudo-defined)onentry.Global variables\nusedin aprocedurearealsopseudo-definedon entry. Parametersand global variablessetin a\nprocedureor function areassumedused(pseudo-used)onexit. The actualuseanddefinition of\ncompletelyglobalvariables,andlocallyglobalvariables,is fully accountedfor in eachprocedurein\nwhich theyoccurasfar astheir usesandre-definitionsareconcerned.On return to the calling\nprocedure,anyglobalvariablesthathavebeenusedor definedin thecalledprocedurearereported\nassingleusesand/ordefinitionsof that globalvariableat thepoint of call, however,pseudo-uses\nenabledwithin a procedurearenot reportedback to thepoint of call. Thetool hasoptionsthat\nallowdifferenttreatmentof globalvariables(e.g.pseudodefinitionsandusescanbeswitchedoff),\nandselectivedisplayof theanalysesof only somefunctionsandprocedures.\nIterationconstructsaretreatedaslinearblockscontainingthedecisionstatementfollowed (while,\nfor), or preceded(repeat),by thesubgraphsdescribingtheiterationbody.Conditionalbranching\nconstructs(if, caseof) consistof decisionnodesfollowed by two or morebranchsubgraphs.All\ndecisionpointsareconsideredto havep-uses(edgeassociateduses)asdefinedin [Fra88].\nMetrics\nSome of the static metrics that BGG currently computes are less common or are new and require\nfurther explanations.\nBy default, path counts are computed so that each loop is traversed once. However, definition-use-\nredefinition chain counts (see below) force on some loops one more iteration in addition to the first\ntraversal. User may change the default number of iterations through a loop through a switch (one\nvalue for all loops). Cyclomatic number is computed in the usual way [McC76\"]. Implemented data\nflow visibility of all language constructs and variables is such that full definition-use coverage\nimplies full coverage of executable nodes (and in turn full statement coverage) [e.g. Nta88]. BGG\ncomputes c-uses, p-uses, and all-uses according to def'mitions given in [Fra88].\nDefinition-use-(re)definition, d(u)d, chains are data-flow constructs defined in [Vou84]. It is one\nof the metrics we are currently evaluating for sensitivity and effectiveness in software fault\ndetection. A d(u)d is a linear chain composed of a definition followed by a number of sequential\nNASA/NAG-1-983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 28\nuses,andfinally by a re-definitionof a variable.Thebasicassumptionsbehindthis metric area)\nthe longerad(u)d chain is themorecomplexis theuseof this variable,andb) themoreonere-\ndefinesa variablethe morecomplexits data-flowpropertiesare.Thefin'stproperty is measured\nthrough d(u)d length (seebelow), the secondproperty is measuredby counting d(u)d's. An\nadditional characteristicof d(u)d chainsis that they arecycle sensitiveand,for thosevariables\nwhere they arepresent,they force at least two traversalsof loops within which a variable is\ndefined.However,full d(u)d coveragedoesnot imply full du-pair coverage.Thed(u)d metric is\nintendedasa supplementarymeasuretootherdefinition-flowmetrics.\nDefinition of adu-pathcanbefound, for example,in [Fra88,Nta88].A singledu-pairmay have\nassociatedwith it oneor moredu-pathsfrom thedefinition to that use.We augmentthecountof\ndu-pathsanddu-pair countswith measuresof du-pathlengths.Theassumptionis that, from the\nstandpointof complexity(andhenceaffinity to errors),it is notonly thecountof du-pathsthatis\nimportant,but also thelengthof eachpath.A definition which is usedseveraltimes,perhapsin\nphysically widely separatedplaces in the program,requiresmore thought and may be more\ncomplexto handlethanonethat is definedandtheusedonly once,or for thefirst time. For each\ndu-pathwecomputelengthby countingthe numberof edgescoveredin reachingthepaireduse.\nFor everyvariablewealsocomputeanaveragelengthoverall du-pairsanddu-pathsassociated\nwith it. In a similar mannerwe define d(u)d-lengthas the numberof use-nodesbetweenthe\ndefinition and redefinition points of the chain. Average d(u)d-length is the d(u)d-length\naccumulatedover all d(u)d pairs divided by the numberof d(u)d's. We used(u)d-lengthsto\naugmentd(u)d-counts.\nWe alsodistinguish betweenlinear (or acyclic) d(u)d's andloop generated,or cyclic, d(u)d's.\nCyclic d(u)d'sarethosewherethevariablere-definesitself or is re-definedin a cyclic chain.All\ncyclic constructsarepotentiallymorecomplexthanthelinearones.Comparisonisdifficult unless\ntheloop countis limited, or loopingis avoided,in whichcasecyclic structureslendthemselvesto\ncomparisonwith acycliconesthroughunfolding.If iterativeconstructsareregardedonly through\ndu-pairs,manycyclesmaynotbedetectedsinceall du-pairsmightbegeneratedby goingarounda\nlooponly once.On theotherhand,for acyclic d(u)dto begenerated,asecondpassthroughaloop\nis alwaysrequired.However,if therearenodefinitionsof avariablewithin a loop then theloop\nwould not be registeredby d(u)d constructsbelongingto that variable.Whena variableis only\nused(or not usedat all) within a loop, its value is loop invariant and loop doesnot add any\ninformationthatthevariablecan(legally)transmito otherpartsof thegraph.\nNASA/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 29\nBGG also has facility for computing concentration (or density) 6f du-paths and d(u)d-paths through\ngraph nodes. We believe that graph (code) sections that show high du-chain and d(u)d-chain node\ndensities may have a higher probability of being associated with software faults than low density\nregions.\nIV. Examples\nThe examples given in this section derive from an ongoing project where BGG is being used to\ninvestigate static and dynamic complexity properties of multi-version software, multi-version\nsoftware fault profiles, and effectiveness and efficiency of different testing strategies. We are using\ntwo sets of functionally equivalent numerical programs for these studies. One set consists of 6 Pascal\nprograms (average size about 500 lines of code) described in [Vou86], the other set consists of 20\nPascal programs (average size about 2,500 lines of code) described in [Ke188].\n158\n159\n160\n161 const\n162\n163 vat\n164\n165\n166\n167\n168 begin\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194 end;\nfunction fptrunc(x: real): real;\nlargevalue = 1.Oe18;\nremainder: real;\npower: real;\nbig-part: real;\nterm: real;\nremainder :-- abs(x);\nif remainder > largevalue then\nfptrunc := x\nelse begin\npower := 1.0;\nwhile power < remainder do\npower := power * 10.0;\nblgpart := 0.0;\nwhile remainder > ma.xint do begin\nwhile power > remainder do\npower := power / 10.0;\nterm := power * trunc(remainder ] power);\nremainder := remainder - term;\nbig-part := big'part + term;\nend;\nremainder := trunc(remainder) + bigpart;\nif x < O.O then\nfptrunc :=-remainder\nelse\nfptrunc := remainder,\nend;\nFigure 2. Code section for which analysis is shown in Figures 3 and 4\nNASA/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM, MAV)/Sep-89 30\nFigure 2 shows a section of the code from program L17.3 of the 6-program suite. Figure 3\nillustrates the output that static analysis processor \"BGG-Static\" offers in the file \"Static Analysis\"\nfor the same procedure.\nOutputs like the one shown in Figure 3 provide summary profile of each local and global symbol\nfound in the code. How many times it was defined (or pseudo-defined), used (or pseudo-used),\nhow many du-pairs it forms, how many d(u)d chains, etc. This static information can be used to\njudge the complexity of procedures, or the complexity of the use of individual variables. In turn,\nthis information may help in deciding which of the variables and procedures need additional\nattention on the part of the programmers and testers.\nFigure 4 illustrates the detailed node, parameter, and global variable information available in the file\nlabelled \"Detailed Graph Analysis\" in Figure 1. Figure 4 is annotated (bold text) to facilitate\nunderstanding. We see that all parameters (e.g. X), global variables (e.g. TRUNC), and built-in\nfunctions (e.g. ABS) are pseudo-defined on entry. The parenthesized number following a\ncapitalized identifier is its number in the symbol table. Note that there are empty nodes, inserted by\nBGG, which act as collection points for branches (e.g. Block #17). Because FPTRUNC was\ndefined in several places in the code, it is pseudo-used on exit from the function (in Block #18).\nNote also that built-in function ABS is treated as a global variable, and its parameters are used only\n(because BGG does not have insight into its code), but the situation is different in the case of\nlocally defined procedures.\nFor example, Figure 5 shows another section of the code in which procedure ADJUST calls a local\nfunction FPMOD (line 285) which, in turn (not shown), calls function FPFLOOR, which then\ncalls function FPTRUNC. The details of the static analysis of the first ADJUST node where the\ncall chain begins are shown in Figure 6. Output lines relevant to the discussion are in bold. Note\nthat FPTRUNC is global from the point of view of ADJUST and is therefore pseudo-defined on\nentry. The same is true for FPMOD and FPFLOOR. All three are reported as defined and then used\nin line 285. For two of them the use actually occurs at a deeper level, in function FPMOD for\nFPFLOOR, and in function FPFLOOR for FPTRUNC. The definitions occur in functions\nthemselves, e.g. for FPTRUNC it occurs in FPTRUNC itself. All these underlying definitions and\nuses are propagated back to ADJUST.\nNAS A/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 31\noo,--,-._\n\"cScS_e,i,...-:e,i\n._o__\noo\n¢q\noo\noo\n¢q\n¢q\nt'-i\n¢q\n°°\n_\n_ __ _\n__._ _\n',r' _ _o\n¢-i\no\n.fi\no\nt_\nNASA/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC. (DFM,MAV)/Sep-89 32\n(1)\nProcedcre FPTRUNC (193)\nParameter X (194)\nGlobal TRUNC (28) used\nGlobal MAXINT (55) used\nGlobal ABS (7) used\nGlobal FPTRUNC (193) defined\nProcedure # 8 : FPTRUNC\nglobal variable\nglobal variable\nbuilt in function\nBlock # 1\nNodetype: NOT FOR\nPredecessor list: none\nSuccessor list: 2\nBlock start at line: 168\nBlock ends at line: 169\nDefinition-use list for this block:\nABS (7) is defined in line # 168\nMAXINT (55) is deirmed in line # 168\nTRUNC (28) is defined in line # 168\nLARGEVALUE (195) is defined in line # 168\nX (I94) is defined in line # 168\nX (194) is used in line # 169\nABS (7) is used in line # 169 (1)\nREMAINDER (196) is defined in line # 169\nBlock # 2\nNodetype: NOT FOR\nPredecessor list: 1\nSuccessor list: 4 3\nBlock start at line: 170\nBlock ends at line: 170\nDefinition-use list for this block:\nLARGEVALUE (195) is used in line # 170\nREMAINDER (196) is used in line # 170\n_ Block # 3\nNodetype: NOT FOR\nPredecessor list: 2\nSuccessor list: 18\nBlock start at line: 170\nBlock ends at line: 171\nDefinition-use list for this block:\nX (194) is used in line # 171\nFPTRUNC (193) is defined in line # 171\nBlock # 17\nNodetype: NOT FOR\nPredecessor list: 15 16\nSuccessor list: 18\nBlock start at line: 193\nBlock ends at line: 193\nDefinition-use list for this block: empty\nBlock # 18\nNodetype: NOT FOR\nPredecessor list: 3 17\nSuccessor list: none\nBlock start at line: 194\nBlock ends at line: 194\nDefinition-use list for this block:\nFPTRUNC (193) is used in line # 194\nnot a for-loop\npseudo-def\npseudo-def\npseudo-def\nconstant\npseudo-def\nempty collector node\nend node\npseudo.use\nABS is a built in function and is treated as a global identifier only\nFigure 4. Elements of the detailed node analysis\nNASA/NAG-1-983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 33\n274\n275\n276\n277 vat\n278\n279 begin\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296 end;\nprocedureadjust(varp: point);\ntwopi,piover2:real;\ntwopi :-pi * 2;\npiover2:=pi / 2;\nbegin\nend;\np.long := fpmod(p.long, twopi);\np.lat := fpmod(p.lat, twopi);\nif p.lat > pi then\np.lat := p.lat - twopi;\nif p.lat > l?iover2 then\npAat := pi - p.lat\nelse if p.lat < -piover2 then\np.lat := -pi - p.lat;\nFigure 5. Code section for which static analysis is shown in Figure 6.\nOf course, variables strictly local to FTPRUNC, such as \"remainder\" (see also Figures 2, 3 and\n4), do not show at the point of call to FPMOD in ADJUST. It is obvious that global data flow can\nadd considerably to the mass of definitions, uses, and other constructs a programmer has to worry\nabout. Nevertheless, we believe that it is a good practice to make this information available so that\nthe full implication of a call to a procedure can be appreciated.\nBGG provides coverage information on the program level, and on the procedure level. Figure 7\nillustrates output from the dynamic coverage processor \"BGG-Dynamic\", delivered in the \"Dynamic\nCoverage Analysis\" output file, for function FPTRUNC and a set of 103 test cases. In the example\nsome of the output information normally delivered by BGG has been turned off, e.g. all-uses.\nFor each procedure BGG-Dynamic fin'st outputs a summary of branch coverage information: the\nblock number, statement numbers encompassed by the block, the number of times the block was\nexecuted, and the execution paths taken from the block (node). For example, node 5 in Figure 7 was\nexecuted 724 times, 6 times to node 3, and 721 times to node 7. Branches which have not been\nexecuted show up as having zero executions.\nNASA/NAG- 1-983/Semi-Anntud Report/1.1/NCSU.CSC.('DFM,MAV)/Sep-89 34\nProcedure ADJUST (214)\nParameter P.LONG (216) used\nParameter P.LONG (216) defined\nParameter P.LAT (217) used\nParameter P.LAT (217) defined\nGlobal WR/TELN (35) used\nGlobal FPFLOOR (200) used\nGlobal ABS (7) used\nGlobal MAXINT (55) used\nGlobal TRUNC (28) used\nGlobal FFTRUNC (193) used\nGlobal FPMOD (203) used\nGlobal PI (107) used\nGlobal FPMOD (203) defined\nGlobal FPFLOOR (200) defined\nGlobal FPTRUNC (193) defined\n.............................. gl w .............................\nProcedure # 13 : ADJUST\nBlock # 1\nNodetype: NOT FOR\nPredecessor list:\nSuccessor list: 2\nBlock start at line: 279\nBlock ends at line: 287\nDef'mition-use list for this block:\nPI (107) is deemed in line # 279\nFPMOD (203) is defined in line # 279\nFPTRUNC (193) is defined in line # 279\nTRUNC (28) is defined in line # 279\nMAXINT (55) is defined in line # 279\nABS (7) is defined in line # 279\nFPFLOOR (200) is defined in line # 279\nWRITELN (35) is defined in line # 279\nP.LAT (217) is defined in line # 279\nP.LONG (216) is defined in line # 279\nP (215) is defined in line # 279\nPI (107) is used in line # 280\nTWOPI (218) is defined in line # 280\nPI (107) is used in line # 281\nPIOVER2 (219) is defined in line # 281\nTWOPI (2183 is used i_ line # 285\nP.LONG (216) is used in line # 285\nFPMOD (203) Is defined In line # 285\nFPFLOOR (200) is defined in line # 285\nFPTRUNC (193) is defined in line # 285\nWRITELN (35) is used in line # 285\nFPFLOOR (200) is used in line # 285\nABS (7) is used in line # 285\nMAXINT (55) is used in line # 285\nTRUNC (28) is used in line # 285\nFPTRUNC (193) is used in line # 285\nFPMOD (203) is used in line # 285\nP.LONG (216) is defined in line # 285\nTWOPI (218) is used in line # 287\nP.LAT (217) is used in line # 287\nFPMOD (203) is defined in line # 287\nFPFLOOR (200) is def'med in line # 287\nFPTRUNC (193) is defined in line # 287\nWRITELN (35) is used in line # 287\nFPFLOOR (200) is used in line # 287\nABS (7) is used in line # 287\nMAXINT (55) is used in line # 287\nTRUNC (28) is used in line # 287\nFPTRUNC (193) is used in line # 287\nFPMOD (203) is used in line # 287\nP.LAT (217) is defined in line # 287\nFigure 6. Elements of the detailed\nstatic analysis of the procedure shown\nin Figure 5.\nprocedure in scope, global variable\npseudo-definition\npseudo-definition\nBeginning of the list of\n< ........................ visible definitions and uses\nfor line 285 from Figure 5.\ncalls FPFLOOR\ncalls FPTRUNC\nactually used in FPMOD\nactually used in FPFLOOR\nactually used in ADJUST\n< ........................ List for line 285 ends\n< ........................ Block #1 ends\niNAS A/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 35\nO\nZ\nZ\niiiiiiiiiiiiiiii I\n:_ ._.\ni i\n._-_-\n_N\n_°\n1\"_ _\n_.__ .__ _\n_J\nc_ c_ r_\no_\nt\"q\n_d\nm.,\nL\n-i\nNAS A/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC. (DFM,MAV)/Sep-89 36\n===_ 8\n°°_\ne_\nr_\n°° _.-_\no\nt\",l\nO\n\"N\n_m\nlqASA/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 37\nd_\noo\nO\noO\nV,\ne_\n8\nO O\n8 _8\n.......... .° °0 _\n[,..\n_) ,\"o4 0\nZ\n$\n0\n0\ng\nt',t\n\"N\n°m\nt-\nNASA/NAG- 1-983/Semi-Annual Report/1. I/NCSU.CSC.(DFM,MAV)/Sep-89 38\ndefinition-use pairs\n70 linear blocks\no_\" 60 \"q branches\n=> 50\nt_ _ Efficiency of Random Testing\n30\n10 0 1 01 1 0 2 1 0 3\nNumber of Test Cases\nFigure 8. Coverage observed during random testing of a program from the 6-version set.\n100\"\nA\no_ 90\n4)\nL_ 80\nO\nu\n70\no\n60\n50\n0 o\n20-Version Set: Program P9\nUniform Random\nFunctional\nM\nNon-Uniform Random\n[]\n101 10 2 10 3\nNumber of Test Cases\nFigure 9. Comparison of linear block coverage observed for two random testing profiles and a\nfunctional data for a program out of the 20-version set.\nmNAS A/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 39\nThe branch table is followed by a summary of coverage by metrics: coverage for non-empty blocks\n(blocks that have not been inserted by BGG), lines of code within executable nodes, and branch\ncoverage. This is followed by coverage for data flow metrics by symbol name. The static\ndefinition, use, du-pair, d(u)d, p-use, etc. counts for a variable are printed along with the\ninformation on its the dynamic coverage expressed as percentage of the executed static constructs.\nFor each identifier, this is followed by a detailed list and description of constructs that have not\nbeen executed (e.g. du-pairs or p-uses). Execution coverage output tables can be printed in\ndifferent formats (e.g. counts of executed constructs, rather than percentages), and with different\ncontent (e.g. all-uses).\n100\"- lo\n90\ni =°\n7o\n6O\n50\nFunctional Test Data\n(20-Version Set: Pro__,\nI\n• • • • ,| • a • i i I • u • • • • i g 0\n0 0 101 10 2 10 3\n,4 E=\nz\n>_\nNumberof Test Cases\nFigure 10. Linear block coverage and fault detection efficiency observed for program P9 of the\n20-version set with functional test cases.\nBGG can also be used to obtain coverage growth curves for a particular test data set. Figures 8 and\n9 illustrate this. They show some of the coverage growth curves we have observed with random\nand functional (designed) test cases for the program L17.3 of the 6-version set using BGG\ndescribed here, and for a program P9 from the 20 version set using an early version of the system.\nNASA/NAG-1-983/Semi-AnnualReport/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 40\nIt is interestingto notethatboth figuresshowcoveragethatfollows anexponentialgrowthcurve\nandreachesaplateauextremelyquickly. For the smallerprogram(Figure8, about600 linesof\ncode)metricsreach saturationalreadyafter about 10cases,while for the largerprogram(20-\nversionset,about2,500linesof code)this happensafterabout100cases.Thereis alsoamarked\ndifferencein theinitial slopeandtheplateaulevel obtainedwith differenttestingprofiles.\nOncethecoverageis closeto saturationfor a particular testing profile, its fault detection efficiency\ndrops sharply. This is illustrated in Figure 10 where we plot the coverage provided by the\nfunctional testing profile shown in Figure 9, and the cumulative number of different faults detected\nusing these test cases. Out of the 10 faults that the code contained, 9 were detected with the\nfunctional data set used within the f'n-st 160 cases.\nIt is clear that apart from providing static information on the code complexity, and dynamic\ninformation on the quality of test data in terms of a particular metric, BGG can also be used to\ndetermine the point of diminishing returns for a given data set, and help in making the decisions on\nwhen to switch to another testing prof'de or strategy.\nV. Summary\nWe have described a research tool that computes and analyses control and data flow in Pascal\nprograms. We plan to extend the tool into C language. We have found BGG to be very useful in\nproviding information for code complexity studies, in directing execution testing by measuring\ncoverage, and as a general unit testing tool that provides programmers with information and insight\nthat is not available through standard UNIX tools such as the pc compiler, or the pxp processor.\nWe are currently using BGG in an attempt to formulate coverage based software reliability models\nby relating code complexity, testing quality (expressed through coverage), and the number of faults\nthat have been discovered in the code.\nReferences\n[Cla85]\n[Dur84]\n[Fis88]\n[Fra86]\nL.A. Clarke, A. Podgurski, D. Richardson, and S. Zeil, \"A comparison of data flow\npath selection criteria,\" in Proc. 8th ICSE, pp 244-251, 1985.\nJ.W. Duran and S.C. Ntafos, \"An Evaluation of Random Testing,\" IEEE Trans.\nSoftware Eng., vol. SE-10, pp. 438-444, 1984.\nC.N. Fisher and R.J. LeBlanc, Crafting a compiler, The Benjamin/Cummings Co.,\n1988.\nP.G. Frankl and E.J. Weyuker, \"A data flow testing tool,\" in Proc. SoftFair II, San\nFrancisco, CA, pp 46-53, 1985.\n_o\nNAS A/NAG- 1-983/Semi-Annual Report/1.1/NCSU.CSC.(DFM,MAV)/Sep-89 41\n[Fra88]\n[Hen84]\n[Hec77]\n[Her76]\n[How80]\n[How87]\n[Ke188]\n[Kor88]\n[Las83]\n[Mau81]\n[McC76]\n[Nta84]\n[Nta88]\n[Ost87]\nFRap85]\n[Vou84]\n[Vou86]\n[Wei85]\n[Wey88]\ntWhi80]\n[Woo80]\n[Zei88]\nP.G. Frankl and E.J. Weyuker, \"An applicable family of data flow testing criteria,\"\nIEEE Trans. Soft. Eng., Vol. 14 (10), pp 1483-1498, 1988.\nM.A. Hennell, D. Hedley and I.J. Riddell, \"Assessing a Class of Software Tools\",\nProc. 7th Int. Conf. Soft. Eng., Orlando, F1, USA,pp 266-277, 1984.\nM.S. Hecht, Flow Analysis of Computer Programs, Amsterdam, The Netherlands:\nNorth-Holland, 1977.\nP.M. Herman, \"A data flow analysis approach to program testing,\" Australian\nComput. J., Vol 8(3), pp 92-96, 1976.\nW. E. Howden, \"Functional Program Testing,\" IEEE Trans. Software Eng., Vol. SE-\n6, pp.162-169,1980.\nW.E. Howden, \"Functional Program Testing and Analysis\", McGraw-Hill Book Co.,\n1987.\nJ. Kelly, D. Eckhardt, A. Caglayan, J. Knight, D. McAllister, M. Vouk, \"A Large\nScale Second Generation Experiment in Multi-Version Software: Description and Early\nResults\", Proc. FTCS 18, pp 9-14, June 1988.\nB. Koren and J. Laski, \"STAD - A system for testing and debugging: user\nperspective,\" Proc. Second Workshop on Software Testing, Verification, and\nAnalysis, Banff, Canada, Computer Society Press, pp 13 - 20, 1988.\nJ.W. Laski and B. Korel, \"A Data-Flow Oriented Program Testing Strategy\", IEEE\nTrans. Soft. Eng., Vol. SE-9, pp 347-354, 1983.\nJ. Mauney and C.N. Fischer, \"FMQ -- An LL(1) Error-Correcting-Parser Generator\",\nUser Guide, University of Wisconsin-Madison, Computer Sciences Technical Report\n#449, Nov. 1981.\nT. McCabe, \"A Complexity Measure,\" IEEE Trans. Soft. Eng., Vol. SE-2, 308-320,\n1976.\nS.C. Ntafos, \"On Required Element Testing\", IEEE Trans. Soft. Eng., Vol. SE-10,\npp 793-803, 1984.\nS.C. Ntafos, \"A Comparison of Some Structural Testing Strategies\", IEEE Trans.\nSoft. Eng., Vol. SE-14 (6), pp 868-874, 1988.\nL.J. Osterweil and L.D. Fosdick, \"DAVE - a validation, error detection and\ndocumentation system for FORTRAN programs,\" Software - Practice and Experience,\nVol. 6, 473-486, 1976.\nS. Rapps and E.J. Weyuker, \"Selecting software test data using data flow\ninformation,\" IEEE Trans. Soft. Eng., Vol. SE-11(4), pp 367-375, 1985.\nM.A. Vouk and K.C. Tai, \"Sensitivity of definition-use data-flow metrics to control\nstructures\", North Carolina State University, Department of Computer Science,\nTechnical Report: TR-85-01, 1985.\nM.A. Vouk, D.F. McAllister, and K.C. Tai, \"An Experimental Evaluation of the\nEffectiveness of Random Testing of Fault-tolerant Software\", Proc. Workshop on\nSoftware Testing, Banff, Canada, IEEE CS Press, 74-81, July 1986.\nM.D. Weiser, J.D. Gannon, and P.R. McMullin, \"Comparison of structured test\ncoverage metrics,\" IEEE Software, Vol 2(2), pp 80-85, 1985.\nE.J. Weyuker, \"An empirical study of the complexity of data flow testing\", Proc.\nSecond Workshop on Software Testing, Verification, and Analysis, Banff, Canada,\nComputer Society Press, pp 188-195, 1988.\nL.J. White and E.J. Cohen, \"A Domain Strategy for Computer Program Testing\",\nIEEE Trans. Soft. Eng., Vol. SE-6, pp 247-257, 1980.\nM. R. Woodward, D. Hedley, and M. A. Hennell, \"Experience With Path Analysis\nand Testing of Programs,\" IEEE Trans. Software Eng., vol.SE-6, pp.278-286, 1980.\nS.J. Zeil, \"Selectivity of data flow and control flow path criteria\", Proc. Second\nWorkshop on Software Testing, Verification, and Analysis, Banff, Canada, Computer\nSociety Press, pp 216-222, 1988.\n\n",
    "id": 42822938,
    "identifiers": {
        "doi": null,
        "oai": "oai:casi.ntrs.nasa.gov:19900016268"
    },
    "title": "Multi-version software reliability through fault-avoidance and fault-tolerance",
    "language": {
        "code": "en",
        "name": "English"
    },
    "publishedDate": "",
    "publisher": null,
    "references": [],
    "sourceFulltextUrls": [
        "http://hdl.handle.net/2060/19900016268"
    ],
    "updatedDate": "",
    "yearPublished": "1989",
    "links": [
        {
            "type": "download",
            "url": "https://core.ac.uk/download/pdf/42822938.pdf"
        },
        {
            "type": "reader",
            "url": "https://core.ac.uk/reader/42822938"
        },
        {
            "type": "thumbnail_m",
            "url": "https://core.ac.uk/image/42822938/medium"
        },
        {
            "type": "thumbnail_l",
            "url": "https://core.ac.uk/image/42822938/large"
        },
        {
            "type": "display",
            "url": "https://core.ac.uk/outputs/42822938"
        }
    ],
    "abstract": "A number of experimental and theoretical issues associated with the practical use of multi-version software to provide run-time tolerance to software faults were investigated. A specialized tool was developed and evaluated for measuring testing coverage for a variety of metrics. The tool was used to collect information on the relationships between software faults and coverage provided by the testing process as measured by different metrics (including data flow metrics). Considerable correlation was found between coverage provided by some higher metrics and the elimination of faults in the code. Back-to-back testing was continued as an efficient mechanism for removal of un-correlated faults, and common-cause faults of variable span. Software reliability estimation methods was also continued based on non-random sampling, and the relationship between software reliability and code coverage provided through testing. New fault tolerance models were formulated. Simulation studies of the Acceptance Voting and Multi-stage Voting algorithms were finished and it was found that these two schemes for software fault tolerance are superior in many respects to some commonly used schemes. Particularly encouraging are the safety properties of the Acceptance testing scheme",
    "tags": [
        "NASA-CR-186502",
        "NAS 1.26:186502",
        "COMPUTER PROGRAMMING AND SOFTWARE"
    ],
    "fulltextStatus": "enabled",
    "subjects": [
        "NASA-CR-186502",
        "NAS 1.26:186502"
    ],
    "oai": "oai:casi.ntrs.nasa.gov:19900016268",
    "deleted": "ALLOWED",
    "disabled": false,
    "journals": null,
    "repositories": {
        "id": "151",
        "openDoarId": 0,
        "name": "NASA Technical Reports Server",
        "urlHomepage": null,
        "uriJournals": null,
        "physicalName": "noname",
        "roarId": 0,
        "baseId": 0,
        "pdfStatus": null,
        "nrUpdates": 0,
        "lastUpdateTime": null
    },
    "repositoryDocument": {
        "id": 42822938,
        "depositedDate": null,
        "publishedDate": null,
        "updatedDate": "2020-06-15T21:30:17+01:00",
        "acceptedDate": null,
        "createdDate": "2016-08-03T06:54:07+01:00"
    },
    "urls": [
        "http://hdl.handle.net/2060/19900016268"
    ],
    "lastUpdate": "2020-06-15T21:30:17+01:00",
    "setSpecs": []
}