{
    "acceptedDate": "2013-11-09T00:00:00+00:00",
    "authors": [
        {
            "name": "Drovandi, Chris"
        },
        {
            "name": "Pettitt, Tony"
        },
        {
            "name": "Henderson, Robert"
        },
        {
            "name": "McCombe, Pamela"
        }
    ],
    "contributors": [],
    "createdDate": "2013-07-02T14:31:55+01:00",
    "dataProvider": {
        "id": 310,
        "name": "Queensland University of Technology ePrints Archive",
        "url": "https://api.core.ac.uk/v3/data-providers/310",
        "logo": "https://api.core.ac.uk/data-providers/310/logo"
    },
    "depositedDate": "2014-04-01T00:00:00+01:00",
    "documentType": "research",
    "doi": "10.1016/j.csda.2013.11.003",
    "downloadUrl": "https://core.ac.uk/download/10914192.pdf",
    "fullText": "This is the author’s version of a work that was submitted/accepted for pub-lication in the following source:Drovandi, Christopher C., Pettitt, Anthony N., Henderson, Robert, & Mc-Combe, Pamela A. (2012) Marginal reversible jump Markov chain MonteCarlo with application to motor unit number estimation. (Submitted (not yetaccepted for publication))This file was downloaded from: http://eprints.qut.edu.au/54864/c© Copyright 2012 Please consult the authorsNotice: Changes introduced as a result of publishing processes such ascopy-editing and formatting may not be reflected in this document. For adefinitive version of this work, please refer to the published source:Marginal Reversible Jump Markov Chain Monte Carlo withApplication to Motor Unit Number EstimationC. C. Drovandi, A. N. Pettitt, R. D. Henderson, P. A. McCombeMathematical SciencesQueensland University of Technology, Brisbane, Australia 4000email: c.drovandi@qut.edu.auOctober 29, 2012AbstractMotor unit number estimation (MUNE) is a method which aims to provide a quantitativeindicator of progression of diseases that lead to loss of motor units, such as motor neuronedisease. However the development of a reliable, repeatable and fast real-time MUNE methodhas proved elusive hitherto. Ridall et al. (2007) implement a reversible jump Markov chainMonte Carlo (RJMCMC) algorithm to produce a posterior distribution for the number of mo-tor units using a Bayesian hierarchical model that takes into account biological informationabout motor unit activation. However we find that the approach can be unreliable for somedatasets since it can suffer from poor cross-dimensional mixing. Here we focus on improvedinference by marginalising over latent variables to create the likelihood. In particular weexplore how this can improve the RJMCMC mixing and investigate alternative approachesthat utilise the likelihood (e.g. DIC (Spiegelhalter et al., 2002)). For this model the marginal-isation is over latent variables which, for a larger number of motor units, is an intractablesummation over all combinations of a set of latent binary variables whose joint sample spaceincreases exponentially with the number of motor units. We provide a tractable and accurateapproximation for this quantity and also investigate simulation approaches incorporated intoRJMCMC using results of Andrieu and Roberts (2009).Keywords: Amyotrophic lateral sclerosis, Marginalisation, Markov chain Monte Carlo,Model choice, Motor Neurone disease, Motor unit number estimation, Neurophysiology, Re-versible jump11 Introduction1.1 BackgroundMeasuring the progress of diseases of the motor units, such as amyotrophic lateral sclerosis(ALS) and post-polio syndrome, is a challenge for clinical neurologists. As Shefner (2009)states, any outcome measure that assesses the patient’s function, including survival, is areflection of both the underlying disease process and the body’s physiological attempts atcompensation. Motor unit number estimation (MUNE) is an attempt to estimate the numberof motor units (MUs) that remain and serial studies can show the progression of the disease.Bromberg (2007) provides a recent account of progress in the area.In addition to assessing disease progression, MUNE can be used to monitor attempts attherapy. For example one aspect of the Miami project to cure paralysis involves attemptingto enhance motor neuron survival, axon regeneration and muscle re-innervation from patientswho have experienced a spinal cord injury (Casella et al., 2010). MUNE can be utilised toinvestigate how many MUs re-innervate the muscles following cell transplantation treatment.An aim of our MUNE project is to develop a reliable and repeatable method for estimatingthe number of MUs, N , in a given muscle so that the method can be incorporated into theelectromyography system (e.g. Viking Select EMG machine, Nicolet Biomedical, Madison,WI, USA) used in hospital neurology clinics around the world. A reliable MUNE methodwould be able to handle a wide variety of clinical datasets without user intervention while arepeatable MUNE method would give comparable results under two different data collectionson the same patient under the same conditions. Typically, data collection in the clinic takes10 to 15 minutes.Ridall et al. (2006) and Ridall et al. (2007) present a Bayesian MUNE method based on dataobtained from a stimulus response curve (Henderson et al., 2006) which is the graph of thecompound muscle action potential (CMAP) obtained using surface electrodes from repeatedstimulation of a nerve at stimulus intensities ranging from threshold (where no MUs respond)to supramaximal (where all MUs respond giving the maximum response). See Figure 1(d) foran example of a real dataset collected from a patient severely affected by ALS. Motor unitsfire probabilistically over a range of stimulus values (Pecher, 1939).The Bayesian hierarchical model developed by Ridall et al. (2006) overcomes some of thesimplifying assumptions used in other statistical approaches, such as the Poisson method(Daube, 1995; Lomen-Hoerth and Slawnych, 2003) and the binomial method (Blok et al.,2005), and accommodates data from both diseased and normal patients. Ridall et al. (2006)compare N models using the Bayesian information criterion (BIC) (Raftery, 1996), but thisapproach has two difficulties, the first that the number of unknown parameters has to bespecified and, the second, that the posterior median has to be found. The first is difficult asthe model of Ridall et al. (2006) involves random effects and latent variables and the second isproblematic as the fixed N model posterior displays multimodality, as pointed out by Glasbey(2007) in the discussion of Ridall et al. (2007). Use of the deviance information criterion (DIC)(Spiegelhalter et al., 2002) also has difficulties as the MUNE model involves hidden variablesand the deviance is ambiguously defined in such cases. Such difficulties are illustrated, forexample, for the mixture model in Celeux et al. (2006).2The approach of Ridall et al. (2007) estimates the number of MUs, N , by using reversible jumpMarkov chain Monte Carlo (RJMCMC) on the joint model space of N and MU parameters.Extensive use of the algorithm with clinical data has shown that it sometimes suffers frompoor mixing, with between N jumps occurring at extremely low frequencies. The challenge indesigning a successful and reliable RJMCMC algorithm is therefore to overcome the withinfixed N model posterior multimodality and have moves between varying N models withreasonably large probabilities. If the within-model posterior MCMC paths become trappedwithin local modes with low probability this can result in misleading between varying Nmodel comparisons. Therefore we seek an approach that has thorough within fixed N modelexploration of the posterior and between varying N model posterior comparisons. Our mainfocus is on improving the latter, between varying N model mixing of the chain with the aimof increasing the reliability of the method.1.2 Approach and OutlineThe model relies upon latent binary indicator variables that specify which MUs are firing ateach observation. The RJMCMC approach of Ridall et al. (2007) involves proposals for theseindicators when proposing a model dimension change (i.e. a change in N) and reduces theprobability of acceptance. The main objective of this paper is to demonstrate that the use ofthe likelihood (with the latent variables marginalised over) and various approximations to itsubstantially improves mixing.This paper is not the first demonstration of applying marginalisation within RJMCMC toimprove mixing. For example, Vermaak et al. (2004) analytically integrates out various pa-rameters and samples from the resulting marginal space in a time series example. Andrieu andRoberts (2009) approximately integrates out a set of continuous latent variables via an impor-tance sampling approximation. However, our application is unique as we require marginalisingover a multivariate binary distribution whose dimension grows exponentially with an increasein the number of MUs. For even small N exact computation is not possible. We compareand contrast two approaches to improving the RJMCMC, one based on a deterministic ap-proximation and one based on simulation to marginalise over the binary latent variables usingan MCMC algorithm presented in Andrieu and Roberts (2009) and which has good theoret-ical properties. A by-product of the availability of the deterministic approximation to thelikelihood is that model choice criteria (as opposed to posterior model probabilities) such asDIC (Spiegelhalter et al., 2002) can be calculated. However Robert and Marin (2008) showsthat within-model based criteria such as DIC cannot be converted to cross model posteriorprobabilities and we empirically confirm this theoretical result so emphasising the need for agood RJMCMC algorithm.We also give details of the modification of the statistical model of Ridall et al. (2006) andRidall et al. (2007) as suggested by recent data analysis and consultation with neurologists.More specifically, we make some simplifications to the model so that fewer parameters requireupdating in the MCMC algorithm and revise the prior distribution to incorporate expertopinion of the co-author neurophysiologists (RDH, PAM) (Baumann et al., 2012). A usefulcontribution is that we adapt the data collection protocol so that some of the parameterscan be estimated prior to the algorithm. This is beneficial for improving identification ofother parameters. Furthermore, we refine the within-model MCMC updates of most of the3parameters so that the mixing of the algorithm is improved and requires no tuning comparedwith Ridall et al. (2007). The within-model MCMC updates are described in Appendix A.The paper is structured as follows. In Section 2 we detail data collection and present thedatasets we analyse in this paper. The statistical model and prior distributions of Ridallet al. (2006) and Ridall et al. (2007) are presented in Section 3 together with the modificationswe apply. In Section 4 we present the new reversible jump approach that marginalises overthe set of latent binary indicators. The marginalisation approximations are detailed in thissection. The results provided in Section 5 highlight the advantage of marginalisation withinRJMCMC and compare the two approximations specified in Section 4. Section 6 contains thediscussion.2 Data2.1 Data Collection ProcessThe data collection process we adopt resembles that of McComas et al. (1971) and has recentlybeen described as the stimulus response curve (Henderson et al., 2006) or as the muscle scan(Blok et al., 2007). In these electromyography scans, a stimulating electrode is placed on anerve (here we focus on the ulnar nerve) and a surface recording electrode is taped to themuscle supplied by that nerve. The nerve is given repeated electrical stimulation at variousstimulus strengths (measured in milliamps (mA)), which, depending on the intensity, resultsin involuntary muscle contraction whose magnitude is calculated as a CMAP area measuredin microvolt milliseconds (µV ms) by the recording electrode. Therefore the data consistsof many hundreds of stimulus/CMAP pairs. More details on the data we collect is given inSection 2.2.Unlike McComas et al. (1971), we collect the full stimulus-response curve by starting at lowstimuli where no MUs are firing and steadily increasing the stimulus until all MUs havebeen recruited. The frequency of the stimuli is 2Hz. This means that the interval betweenthe recordings is greater than the refractory period of the nerve and all stimuli produce aneffect that is independent of the preceding stimulus. The quality of the recorded data can bedependent on the skill of the practitioner as well as keeping the patient still, ensuring that allthe muscle movement recorded is evoked by electrical stimulation.Further details of the data collection protocol we apply can be found in Ridall et al. (2006,2007); Henderson et al. (2006).2.2 Data for AnalysisThe data consists of T pairs of stimulus and evoked CMAP response, (St, yt) for t = 1, . . . , T ,where the stimulus is controlled but the CMAP is measured with error.We use eight datasets to illustrate the comparison of the RJMCMC algorithm with andwithout the use of marginalisation. Figures 1(a)-1(c) display simulated data from a 10, 15and 20 unit model respectively. The other datasets (Figures 1(d)-1(h)) are real data collectedon patients with ALS who are at different stages of disease. These are referred to as patient4B, L, A, O and R data respectively.3 ModellingHere we describe the statistical model that takes into account known physiology of MU acti-vation. The model is adapted from Ridall et al. (2006), and we report any modifications tothe originally proposed model and prior distributions.3.1 The Statistical ModelWe describe the model for a fixed number of MUs, N . MU k ∈ {1, . . . , N} fires at timet ∈ {1, . . . , T} if the stimulus at this time, St, exceeds the MUs threshold, τk,t, which isassumed to vary with time. This is denoted by the firing indicatorsk,t|τk,t, St = 1(τk,t < St).The threshold for unit k is assumed to be normally distributed with mean mk and precisionδ2kτk,t|mk, δk ∼ N(mk,1δ2k).When MU k fires, in an all or nothing response, it produces a single MU action potential (withmean µk) that is made up of the potential of all the individual muscle fibres that comprisethe MU.We have the following model for a particular CMAP observation, yt(yt|St, st,µ, µb, σb, σ, ηt, N) ∼ N(yt;µt, Vt),where st = {s1,t, . . . , sN,t} and µ = {µ1, . . . , µN}. Here also we have thatµt = µb +N∑k=1sk,tµk,Vt =σ2b + σ21(nt > 0)ηt,nt =N∑k=1sk,t.Here µb and σb are, respectively, the mean and standard deviation of the baseline (the re-sponse that is produced when no MUs are firing). These baseline parameters are set fixed byestimating them from baseline data, see later. Here ηt takes account of the outliers in the dataand is assigned a gamma distribution, with details provided in the prior distributions sectionbelow. Our variance component is slightly different from that of Ridall et al. (2006, 2007).We find empirically that the variance does not appear to increase as the number of firing MUs510 15 20 25200040006000Stimulus (mA)CMAP (µVms)(a) Simulated data, 10 MUs10 15 20 2502000400060008000Stimulus (mA)CMAP (µVms)(b) Simulated data, 15 MUs10 15 20 25020004000600080001000012000Stimulus (mA)CMAP (µVms)(c) Simulated data, 20 MUs20 25 30 35 400100020003000Stimulus (mA)CMAP (µVms)(d) Patient B data15 20 25 30 3501000200030004000Stimulus (mA)CMAP (µVms)(e) Patient L data8 10 12 14 1602000400060008000100001200014000Stimulus (mA)CMAP (µVms)(f) Patient A data8 9 10 11 12 13 14 15 160200040006000800010000Stimulus (mA)CMAP (µVms)(g) Patient O data15 20 25 30020004000600080001000012000Stimulus (mA)CMAP (µVms)(h) Patient R dataFigure 1: Data used for analysis. (a)-(c) Simulated data 10, 15 and 20 unit model respectively.(d)-(h) Data collected from five ALS patients. The simulated data have 500 observations.Patient B, L, O, A and R data have 995, 424, 500, 420 and 354 observations, respectively.6increases and we keep it constant when MUs fire (see also Henderson et al. (2007)). Hence theuse of the indicator function in the expression for Vt where nt is the number of MUs firing atstimulus St based on the current values of the firing indicators. We assume that the CMAPdata are conditionally independent.Based on these model specifications the full probability model (including only the stochasticcomponents) is specified asp(y,µ,η, s, δ,m, σ|N) = p(y|S,µ,η, σ, s, N)p(s|δ,m, N)p(µ|N)p(δ|N)p(m|N)p(η)p(σ),where y = {y1, . . . , yT }, S = {S1, . . . , ST }, η = {η1, . . . , ηT }, s = {s1, . . . , sT }, δ ={δ1, . . . , δN} and m = {m1, . . . ,mN}. Due to conditional independencies in the model someof the joint distributions in the above full probability model can be simplifiedp(y|S,µ,η, s, σ,N) =T∏t=1p(yt|St,µ, ηt, σ, st, N),p(s|δ,m, N) =T∏t=1N∏k=1p(sk,t|δk,mk, N),p(µ|N) =N∏k=1p(µk|N), p(δ|N) =N∏k=1p(δk|N),p(η) =T∏t=1p(ηt).To robustify the inference we introduce two fixed parameters, Snone and Sall. Snone (Sall resp.)corresponds to the stimulus value such that no (all resp.) MUs are firing at lower (higher resp.)stimulus values. Therefore the firing indicators in such stimulus ranges are deterministicallyset to zero (one resp.). Snone and Sall can usually be set straightforwardly via an inspectionof the data by the clinician conducting the CMAP scan and are routinely determined duringstandard nerve conduction studies.3.2 Prior Distributions3.2.1 Median of the Excitability CurvesWe assume that themk’s are ordered such that Snone = m0 < m1 < · · · < mN < mN+1 = Sall.The spacings, bj = (mj − mj−1)/(Sall − Snone), j = 1, . . . , N + 1, are assumed to follow aDirichlet distribution a priorip(b|Snone, Sall, N) =Γ(∑N+1j=1 αj)∏N+1j=1 Γ(αj)N+1∏j=1bαj−1j ,where b = {b1, . . . , bN} and bN+1 = 1 − b1 − · · · − bN . The probability distribution of themean thresholds, m = {m1, . . . ,mN}, is obtained by applying the transformation b → m.7The resulting probability distribution for m isp(m|m0,mN , N) =1(mN+1 −m0)NΓ(∑N+1j=1 αj)∏N+1j=1 Γ(αj)N+1∏j=1(mj −mj−1mN+1 −m0)αj−1.We set αj = 1, which reduces the above probability distribution top(m|Snone, Sall, N) =N !(Sall − Snone)N,which depends on N . Note that Ridall et al. (2006) allocates a normal random effects for mwhile Ridall et al. (2007) sets αj = 2 to mimic slight repulsion between the values of mk.3.2.2 Precision of the Excitability CurvesThe threshold precisions of the MUs are allocated a gamma prior distributionδ2k ∼ Gamma(αδ , βδ).We apply an informative prior here as, according to the neurologists’ view, the excitabilitycurve should be neither too flat (which would imply that MUs are firing probabilistically overa wide range which is not observed in our extensive examination of the stimulus responsecurve) nor too steep (which would imply that there is little probabilistic firing or alternationand again is not seen in our examinations of the stimulus response curve). More specifically weset αδ = 3 and βδ = 1. The implied prior on δk is approximately gamma with an approximatemode and standard deviation of 1.5 and 0.5, respectively. In Ridall et al. (2007) the δkwere assigned random effects where αδ and βδ were hyperparameters. This fixed effects priorpermits the incorporation of the expert neurologists’ knowledge.3.2.3 Mean of the MU Action PotentialsThe minimum possible MU action potential, µmin, is a matter of scientific debate. Bromberg(2003) suggests a value of 25 µV ms but our view is that given the size of the observationvariance in our data a value of 50-100 µV ms is preferred. We consider that it is important toavoid confusing noise with small MUs. Additionally, small MUs play only a minor role in thestrength of the muscle and therefore are of less importance in following diseases that causeweakness. Here we set µmin = 100 µV ms. We set the maximum MUAP, µmax, following aninspection of the data to restrict the model to realistic values of the MUAP. Therefore weplace an uninformative data dependent prior over this range such thatµk ∼ U(µmin, µmax).This fixed effects prior is in contrast to the gamma random effects truncated at µmin model inRidall et al. (2006) and Ridall et al. (2007). We find that the MU action potentials can havea wide range of values and are not consistent with a gamma distribution. This is especiallythe case when collaterial sprouting takes effect, resulting in a mix of large and small MUs andlarge gaps in the CMAP plot; see patient L in Figure 1(e) for example. Furthermore, thisavoids the possibility of poor mixing of the gamma hyperparameters.83.2.4 Precision of the MU Action PotentialsHere we assign an uninformative (improper) prior on the precision of the MU action potentialso thatp(σ2) ∝1σ2 + σ2b1(σ2 > 0).This prior was selected for two reasons: first, it removes the arbitrariness of hyperparametersfor the gamma prior in Ridall et al. (2006) and, second, it facilitates closed form full conditionalsampling by including the (fixed) σ2b . Spiegelhalter and Smith (1982) note that the use ofimproper priors in general results in a ratio of undefined constants appearing in the Bayesfactor resulting from different constants for the two competing models. However, if the prioris common across models then the arbitrary constants will “cancel” in the ratio of posteriorprobabilities (Pericchi, 2005).3.2.5 Baseline ParametersRidall et al. (2006) and Ridall et al. (2007) allocate priors to the parameters of µb and σ2b andallow them to be updated during the MCMC step. We adapted our data collection protocolso that there is sufficient data collected at low stimulus values when no MUs are firing (beforeSnone) to obtain sufficiently precise estimates of µb and σ2b . We fix these parameters at themaximum likelihood estimates for the baseline data. We find that fixing these two parametersat these estimates allows improved identification of the first MUAP mean, µ1, and the varianceof the MUAPs, σ2.3.2.6 Marginal t-distributionTo account for potential outliers we divide the variance of each observation by ηt, where ηt isassigned a gamma distribution such thatηt ∼ Gamma(ǫ, ǫ),where we set ǫ = 2. By integrating out ηt, yt marginally has a t-distribution with 4 degrees offreedom which is reasonably robust to the outliers that are commonly observed in the stimulusresponse curve, especially in patients with ALS.4 The Likelihood and Marginal RJMCMCIn this section we present the main focus of the paper, that is, the application of marginalisa-tion to RJMCMC in the MUNE context. For even moderate N the marginalisation cannot beperformed exactly as it involves a sum over latent binary indicator variables whose dimensiongrows exponentially with increasing N . Therefore we propose three approximations. Thefirst is simply a Monte Carlo estimate of the summation. For the second and third approxi-mations we consider splitting the summation into two mutually exclusive summations. Thefirst component of the summation deterministically sets the firing indicators for MUs that are9almost always firing or not firing. This contributes the most to the overall likelihood. Thesecond summation is over the firing combinations not considered in the first summation. Thesecond likelihood approximation ignores the second component of the summation. The thirdapproximation is the same as the second but introduces a Monte Carlo approximation of thesecond component of the summation. The first and third approximations are unbiased, andpotentially applicable to the MCMC algorithms presented in Andrieu and Roberts (2009). Asnoted in the introduction, this likelihood can be used in other model choice approaches suchas DIC. We also describe briefly the moves for the other parameters of the model required inthe reversible jump and the corresponding acceptance probability.4.1 The LikelihoodThe method of RJMCMC improvement we adopt requires that the likelihood is evaluatedfor each observation and this involves summing over all possible combinations of the firingindicators, sa, for the tth observationp(yt|St,µ,m, δ, σ,N) =∑sa∈{0,1}Np(yt|St,µ, σ, sa, N)p(sa|m, δ, N), (1)We refer to (1) as the likelihood for yt and the likelihood for (yt, st) = p(yt|St,µ, σ, st, N)p(st|δ,m, N)as the complete data likelihood. Here the term for yt in (1) has ηt integrated out producinga t-density. The log of the overall likelihood is then∑Tt=1 log p(yt|St,µ,m, δ, σ,N). We notethat there are 2N possible firing combinations and the sum in (1) becomes intractable for onlymoderate N . Therefore we give below various approximations to the likelihood.4.2 Approximating the Likelihood4.2.1 A Stochastic ApproximationOne stochastic approximation to the likelihood in (1) involves Monte Carlo integration. Givencurrent posterior samples for the parameters m and δ, we can simulate M draws fromp(st|m, δ, N) producing sitfor i = 1, . . . ,M . Then an unbiased Monte Carlo approxima-tion to the marginalised likelihood is as followsp(yt|St,µ,m, δ, σ,N) ≈1MM∑i=1p(yt|St,µ, σ, sit, N).We refer to this estimator as stochastic approximation 1 (SA1).4.2.2 A Deterministic plus Stochastic ApproximationThe conditional distribution of st shown in equation (1) is given byp(st|m, δ, N) =N∏k=1psk,tk,t (1− pk,t)1−sk,t ,10where pk,t = P (τk,t < St) and st = (s1,t, . . . , sN,t). From the above equation it is evidentthat if one of the pk,t is close to zero and the corresponding sk,t is close to one (or vice-versa)the overall product will be close to zero and hence such a combination of st will contribute anegligible amount to the sum in (1). Therefore we inspect the pk,t for all k and if pk,t < pǫ orpk,t > 1− pǫ (with pǫ set small) then we only consider firing combinations that have sk,t = 0or sk,t = 1, respectively.The remaining B MUs that have pǫ ≤ pk,t ≤ 1 − pǫ are considered to be firing stochasticallyand have the most contribution towards (1). We denote this set of indicators as StF ={{0, 1}N |sk,t = 0 if pk,t < pǫ or sk,t = 1 if pk,t > 1 − pǫ}. The superscript t is used to denotethat for different observations SF can change. The original summation in equation (1) becomesp(yt|St,µ,m, δ, σ,N) =∑sa∈StFp(yt|St,µ, σ, sa, N)p(sa|m, δ, N) +∑sa∈S¯tFp(yt|St,µ, σ, sa, N)p(sa|m, δ, N),(2)where S¯tF denotes the complement of the set StF where {0, 1}N is the sample space. The secondapproximation to the likelihood (which we refer to as the deterministic approximation (DA))assumes that pǫ is set low enough that the second component of the summation in equation(2) is negligible and computes the first sum exactly. In thof the datasets is approximation weonly need to consider 2B combinations of the firing indicators rather than 2N and generally wefind B << N . The quality of the approximation improves as pǫ decreases but will thereforeincrease B resulting in extra computation. The applicability of the approximation dependson how large the proposed N is and the extent to which the excitability curves overlap. Oursuggestion is to choose pǫ as low as possible such that computation is still reasonable. Weconjecture that choosing pǫ as large as 0.001 will lead to precise inferences for most datasets.The computation with this value of pǫ was manageable for all datasets except one investigatedin this paper.The third approximation (SA2) computes the first summation exactly as in the second ap-proximation and uses a Monte Carlo approximation of the second summation by drawingfrom the prior distribution of the indicators over the set S¯tF . There is a possibility that withthis approximation we could increase the value of pǫ (decreasing B) to reduce the computa-tional effort required to compute the first summation. More discussion on SA2 and the otherapproximations is provided below.4.3 Likelihood Approximation DiscussionAndrieu and Roberts (2009) shows how the mixing performance of an RJMCMC algorithmcan be improved by marginalization which is approximate. The paper considers MCMCalgorithms which use an approximation to a marginal target distribution where auxiliaryvariables have been marginalised over. The approximation uses an unbiased Monte Carloestimate of the marginal target distribution. The interesting aspect of the first and the thirdapproximations to the likelihood (SA1 and SA2) is that they are both unbiased estimators,with SA2 typically having lower variance. The algorithms in Andrieu and Roberts (2009),the MCWM and GIMH algorithms, have the possibility of being utilised in the RJMCMCalgorithm for the MUNE problem. The GIMH algorithm presented in Andrieu and Roberts11(2009) has particularly appealing properties, since an unbiased estimate of the (marginalised)likelihood still leads to an algorithm that is exact for the marginal posterior distribution ofthe parameters as long as the parameter from the previous iteration of the MCMC (and thecorresponding unbiased likelihood estimate) is recycled for subsequent iterations of the MCMCalgorithm. Unfortunately, this is typically not the case for reversible jump algorithms, sincewithin-model moves are usually required so that the full posterior distribution is sufficientlyexplored. Thus, in our problem, the parameter is updated between reversible jump iterationsand hence the GIMH algorithm is unfortunately not applicable.The second algorithm in Andrieu and Roberts (2009) is the MCWM algorithm. An im-plementation of this algorithm in the MUNE context could be done via the first or thirdunbiased stochastic approximations presented above. An importance sampling version of thefirst approximation using a good approximation to the posterior of the firing indicators mightimprove performance but this would involve a multivariate binary distribution which, unlessthe number of MUs, N , were small, would be difficult to draw from. Scha¨fer and Chopin(2011) present a review of parametric distributions on multivariate binary spaces, howeverit would be too computationally demanding to incorporate their approach within our RJM-CMC algorithm. Andrieu and Roberts (2009) does note that the MCWM algorithm, unlikethe GIMH algorithm, is not exact but depends on a large number of simulations in the MonteCarlo unbiased estimate for its equivalence to the true marginal distribution. We provide acomparison of the deterministic and stochastic (MCWM) approximations in Section 5.5.Unfortunately we find that SA2 is generally computationally more demanding than DA, evenwhen pǫ is increased in SA2 relative to DA. The difficulty arises from sampling from thetruncated set of firing indicators S¯tF . One option to implement SA2 is to form the (prior)probability of all combinations of the firing indicators in set S¯tF , renormalise the probabilitiesand draw from this distribution. Forming the probability function over these 2N − 2B firingindicators will be expensive. An alternative approach would be to draw from S¯tF via rejectionsampling. We found this to be quite expensive also. Hence, we do not consider SA2 for theremainder of the paper. Thus we replace SA1 with just SA for the following.Doucet et al. (2012) provide some guidelines for choosing M in the unbiased estimate of thelikelihood in the context of the GIMH algorithm. In particular, it suggests to choose M suchthat the standard deviation of the estimated log-likelihood be approximately equal to oneat a parameter value that has reasonable posterior support. We investigate this guidelineempirically for the MCWM algorithm in the context of our MUNE application.4.4 Reversible Jump DetailsRidall et al. (2007) propose four cross-dimension moves. Two of these updates consist of birthmoves from one to two MUs and two to three MUs. Both updates propose an N + 1 modelfrom an N model. The other two updates consist of the corresponding death moves. Forsimplicity we consider the one to two unit (and vice-versa) moves only. Our proposals for theparameters δk and µk are slightly different to Ridall et al. (2007) (see Appendix B for fulldetails of the moves and differences with Ridall et al. (2007)).The most substantial difference in the reversible jump algorithm we apply here is that wesum the firing indicators out of the target distribution. However, in the cross-dimensional12moves we do not integrate out the extra normal random variates as is presented in Section4.1. We refer to this as the marginal RJMCMC approach. We compare this method with analgorithm that is exactly the same as marginal RJMCMC except that the cross-dimensionalmoves require proposals for the latent firing indicators as per Ridall et al. (2007). We referto this approach as standard RJMCMC. For the comparison with Ridall et al. (2007) we usethe DA (see Section 5.2 for the comparison).We also trial the simulation approach of Andrieu and Roberts (2009) that uses instead theSA detailed above. We present the comparison of the two marginalisation approaches for theMUNE problem in Section 5.5.Consider a birth move from an N model to an N + 1 model. This is performed via a splitmethod of one unit into two (see Appendix B for the details). Given current parameters(mN , δN , µN ) for the N model and proposed parameters (mN+1, δN+1, µN+1) for theN + 1 model the acceptance probability of a birth from N to N + 1 MUs using the marginalapproach is given byα1→2 = min(1,pˆ(y|mN+1, δN+1,µN+1,η, σ,N + 1)pˆ(y|mN , δN ,µN ,η, σ,N)1µmax − µmin(N + 1)Sall − SnonepmergeipsplitiJµJm),where the use of pˆ denotes the fact that we are using some approximation to the likelihood.Here the Jµ and Jm represent the Jacobian parts of the acceptance probability for the pro-posals of µ and m respectively. Note that Jδ = 1 (see Appendix B for more details on theJacobian). Here pspliti and pmergei are the probability of selecting the ith unit to split basedon the current parameters and the probability of merging based on proposed parameters,respectively (see Appendix B for more details). The prior on N is discrete uniform over awide enough range to encompass all plausible values of N . The corresponding merge movehas a Metropolis-Hastings ratio that is the reciprocal of the above Metropolis-Hastings ratio.Upon acceptance of a cross-dimensional move with the marginal approach, we need to gen-erate a new set of indicators since these are required in the within-model updates. In thedeterministic approximation, we form an approximation to the full conditional distribution ofthe latent binary indicators for each observation, which is exact when pǫ = 0. We thereforedraw the indicators for each observation from this approximate full conditional distribution.In the stochastic marginalization approximation, the proposals are generated from the priorof the indicators conditional on the current excitability parameters. For each observation,we compute the conditional probability of those indicators (including the likelihood of theobservation) and draw one of these indicators from this proportional to their probabilities.It is unclear whether or not drawing from this prior would create a distribution of indicatorsthat is close to the full conditional, although it would be more accurate with more draws fromthe prior. It should be noted that neither of these approaches areis theoretically valid froman MCMC point of view as neither are from the true full conditional. However we can beconfident of obtaining a reasonable approximation for small pǫ. In fact, for small pǫ this jointmove is likely to be better than the within-model updates for the indicators, which just up-dates the indicator for each unit within each observation one at a time. The former approach,drawing from the full conditional, should be more efficient than drawing from the prior. Thisis another benefit of the approach.For each iteration of the reversible jump algorithms, we perform one within-model update13followed by a proposed cross-dimensional move with a birth or death move equally likely tobe proposed.5 Results5.1 Simulation StudyWe simulated data from the model with N = 5, . . . , 12 as the true number of MUs. Thedataset for N = 10 MUs is shown in Figure 1(a); the other datasets are not shown. The mainmotivation for the simulation study is to discover if there is empirical evidence that the DA(using pǫ = 0.001) and/or SA produces accurate results despite the algorithms not targettingthe true posterior distribution. We found that around N = 12 is the upper limit that can behandled if the likelihood is being computed exactly. In addition, from this simulation study,we investigated: (1) if the exact algorithm reproduces the correct number of MUs, and, (2)the computational times of the algorithm when using the exact, DA and SA likelihood.We found that with the exact algorithm the modal number of MUs was the same as thetrue number of MUs except for the 11 unit dataset (where the mode was N = 10). The DAapproach led to posterior distributions for N that was in close agreement with the resultsbased on the exact calculation of the likelihood. To keep the computational times roughlycomparable with the DA approach, for SA we used M = 32 for the N = 5 dataset, M = 64for N = 6 (both consistent with the number of summations in the exact calculation) andM = 100 for all other datasets. Unfortunately, we found that SA gave poor approximationsto the posterior probabilities in all cases. The modal value was close to that of the exactalgorithm but the posterior distribution of N was substantially more diffuse than the exact(and hence DA) results. We compare the DA and SA methods on more datasets later.The mean computer times over three runs of each approach for all datasets is shown in Figure2. For the SA method we used a value of M so that the computation was always greater thanDA, which as mentioned above, provides a more accurate computation of the posterior modelprobabilities. The exponential growth in computer time for the exact method is evident fromthe figure.5.2 Marginal and Standard Reversible Jump ComparisonIn this section we provide an extensive comparison of the marginal and standard RJMCMCapproaches for both simulated data and real data. For each dataset the algorithms wererun three times with a different seed each time. For the marginal method we use the DAwith pǫ = 0.001 in the likelihood approximation (except for patient A data). The posteriordistribution of N produced by the marginal method for all datasets below are shown in Figure3.In order to compare, relatively, the precision of the estimated posterior distribution of N forthe two different algorithms for each dataset we compute the following. The overall posteriorprobability ofN = n by combining the output from the three runs is given by p¯n. The posteriorprobability of N = n for the ith run is denoted pi,n. For each run, the sum of the absolute145 6 7 8 9 10 11 12020406080100120value of N for datasetcomputation time (hrs)  exactDASAFigure 2: Mean computer times (over 3 runs) for the RJMCMC when using the exact, DAand SA methods for computing the likelihood for each of the datasets in the simulation study.differences between these probabilities over a range of N values (Nmin, Nmax) is considered,and the overall criterion averages these values over the three runs. Mathematically, this isgiven by 1/3∑3i=1∑Nmaxn=Nmin|pi,n − p¯n|.We stress that the above criterion is solely to assess the precision of the estimated probabilitiesof each value of N , since this is what interests the practitioners. In particular, practitionersare in pursuit of the mode (i.e. the final MUNE) as well as some measure of uncertainty like acredible interval, and the focus of our MUNE method is to obtain accurate estimates of thesequantities. We are less concerned with statistical convergence of distributions, as measuredby a p-value, the extremity of which is largely determined by the MCMC sample size.5.2.1 Simulated Data - 10 MUsFor this dataset the standard method does not give satisfactory results. For each of the threeruns the cross-dimensional moves were accepted well less than 0.01% of the time. There isobvious Monte Carlo error between the three runs (the criterion value is 0.507). However themarginal approach provided a much higher acceptance rate of 0.4%. The mixing in this caseis sufficient enough that there is no visual difference between the posterior distributions for Nover the three runs with a criterion value of 0.02 (one of these runs is shown in Figure 3(a)).5.2.2 Simulated Data - 15 MUsThe standard approach performs slightly better for this dataset; in the best case scenario theacceptance rate was 0.014%. There is visually detectable Monte Carlo error between the threeruns (the criterion value is 0.036). The marginal approach is far superior, with an acceptancerate of roughly 0.36%. This is enough to achieve less difference between the three runs (thecriterion value is 0.024) and hence a more accurate representation of the posterior distributionof N (one of these runs is shown in Figure 3(b)).155.2.3 Simulated Data - 20 MUsFor the 20 unit simulated dataset, the standard approach almost achieves appropriate mixing,with a cross-dimensional acceptance rate of roughly 1.75% in the three runs. The marginalmethod acceptance rate was approximately 9.15% (see Figure 3(c)). The criterion value forthe former algorithm was 0.013 and 0.009 for the latter.The posterior distributions from the two approaches are very similar. This indicates that thelikelihood is being estimated with reasonable accuracy as both algorithms sample from thesame target distribution if the likelihood was computed exactly. We found that the posteriordistributions of N matched for the remaining datasets when the standard approach resultedin sufficient mixing between models of different dimension. This provides further empiricalevidence for the correctness of the DA algorithm when pǫ is small.In all simulated data examples the marginal approach almost recovers the correct number ofMUs (for the 20 unit data, the mode was estimated to be 19 MUs). This demonstrates theutility of the algorithm.5.2.4 Patient B DataWe now investigate the performance of the methods on real data. For the patient B data thestandard approach fails completely; there was only a handful of between N acceptances in alliterations. In contrast, the marginal approach (see Figure 3(d)) resulted in an acceptance rateof roughly 0.9%. However, there is still some variability between the posterior distributionsof N between the three runs (a criterion value of 0.053 was obtained).5.2.5 Patient L DataFor the patient L data the marginal approach resulted in 4 times more between N modelmoves with an acceptance rate of roughly 0.4%. The posterior distribution of N for one of theruns of the marginal method is shown in Figure 3(e). The criterion values for the standardand marginal methods were 0.028 and 0.006 respectively.5.2.6 Patient A DataFor the patient A dataset the standard RJMCMC completely fails, with only a few acceptancesin the one million iterations (criterion value of 0.736). Conversely, the marginal method (seeFigure 3(f)) results in enough mixing with an acceptance rate of approximately 2.74% and acriterion value of 0.027. (Here we used pǫ = 0.01 as we found that using pǫ = 0.001 was fartoo computationally intensive.)5.2.7 Patient O DataHere the standard RJMCMC approach obtained an acceptance probability of roughly 2.6%;there is some visual between run Monte Carlo error (the criterion value was 0.036). Themarginal RJMCMC had an acceptance rate of around 15%, which is extremely good in the16context of this model and other RJMCMC implementations in various other applications. Thecriterion value was 0.012. The posterior distribution of N for this dataset using the marginalmethod is shown in Figure 3(g).5.2.8 Patient R DataFinally for the patient R data the standard RJMCMC update gave a low acceptance rate,this time 0.4%. By inspecting the three posterior distributions of N for the three differentruns it was evident that the posterior model probabilities have not been precisely estimated.In contrast, the acceptance rate for the marginal method was extremely high, around 13.2%(see Figure 3(h) for the posterior of N based on this method). The criterion values for thestandard and marginal approaches are 0.046 and 0.008 respectively.We have demonstrated with an extensive comparison of the methods on simulated and realdata that the marginal approach is substantially more reliable than the standard approach asit, often substantially, improves the cross-dimensional mixing. Moreover the standard methodfails completely on some of the datasets investigated here.5.3 Comparison with DICHere we compare the marginal RJMCMC method with the within-model approach based onDIC (Spiegelhalter et al., 2002) for patient O and patient R data. We consider an appropriaterange of N values for the within-model runs. Here we produce 300,000 iterations discardingthe first 100,000. We approximate the likelihood at every 50th iteration via the DA (withpǫ = 0.001), thereby obtaining 4,000 log-likelihood values for each value of N . For the DICwe compute the mean deviance based on these likelihoods and apply a penalty of 2× (3N +1)to account for the number of parameters rather than estimating the number (since we applyfixed effects, the number of parameters is given by three multiplied by the number of MUsproposed plus one for the within-unit variability parameter.). In an attempt to overcomewithin-model multimodality we run the within-model algorithms three times for each valueof N and choose the run which gives the highest DIC.We show the relative DIC values for various values of N in Tables 1 and 2 for patient O andR respectively. A value of zero in the table reflects the optimal value of N based on thiscriterion, with the DIC value given by the last column. The other quantities in the tablerepresent the value above the minimum DIC. Based on the DIC, the inference for N is around13 and 25 for patient O and R respectively. The same modal value of N = 25 was obtainedfrom RJMCMC for the patient R data. For the patient O data, RJMCMC produced a modalvalue of N = 17, which is quite different to the DIC result. A crude conversion of DIC toposterior model probabilities as per the BIC conversion in (Friedman et al., 2001, pg. 207)suggests that inferences on N are much more precise than the RJMCMC results.5.4 Sensitivity to pǫFor the results above we mostly used a value of pǫ = 0.001 (for the DA) to ensure a veryaccurate approximation of the likelihood so that the two RJMCMC approaches (standard1710 11 1200.10.20.30.40.50.60.70.80.9NP(N|y)(a) simulated, 10 MUs14 15 16 17 1800.10.20.30.40.50.60.7NP(N|y)(b) simulated, 15 MUs16 17 18 19 20 21 22 23 2400.050.10.150.20.250.30.35NP(N|y)(c) simulated, 20 MUs12 13 14 15 1600.10.20.30.40.50.60.7NP(N|y)(d) patient B10 11 12 1300.10.20.30.40.50.60.70.8NP(N|y)(e) patient L21 22 23 24 25 26 27 28 29 30 3100.050.10.150.20.250.30.35NP(N|y)(f) patient A12 14 16 18 20 22 2400.050.10.150.20.25NP(N|y)(g) patient O18 20 22 24 26 28 30 3200.050.10.150.20.25NP(N|y)(h) patient RFigure 3: Posterior distribution of N produced by the marginal RJMCMC method using thedeterministic approximation (with pǫ = 0.01 for patient A data and pǫ = 0.001 otherwise) forthe datasets presented in Figures 1(a)-1(h).18Table 1: Relative DIC values above the minimum IC value (see the last column) for thepatient O data.N 10 11 12 13 14 15 16 17 18 19 20 min ICDIC 8 3 0 1 0 2 2 4 4 7 8 5963Table 2: Relative DIC values above the minimum IC value (see the last column) for thepatient R data.N 20 21 22 23 24 25 26 27 28 29 30 min ICDIC 12 8 4 3 5 2 0 2 1 4 7 4681and marginal) would share, at least approximately, the same target distribution. However,it is of interest to investigate how large we can set pǫ in order to save on computation timewithout compromising on the quality of the estimation of the posterior distribution of N .To investigate this issue we considered three values of pǫ (0.001, 0.01 and 0.025) and appliedthe DA approach with each value of pǫ and recorded both the computing time (in hours, basedon using a single processing unit) and the posterior distribution of N . These comparisonsare provided for the 20 unit simulated data (see Figure 4), patient O data (see Figure 5) andpatient R data (see Figure 6). The figures show the posterior distribution of N and the timetaken to run the method in the subfigure labels for different values of pǫ.The inference on N for the 20 unit simulated data showed little dependence on the value ofpǫ. The results are very similar for pǫ = 0.001 and pǫ = 0.01. For pǫ = 0.025 the modal valueof N becomes 18 but its probability is still very close to that of N = 19. There is a steadydecrease in computer time as pǫ increases.For the patient O data, the inferences on N are not overly sensitive to pǫ. For each increasein pǫ the mode of N drops by 1 and there is a general shift of the posterior distribution of Ntowards lower numbers of MUs. There is a dramatic decrease in the time it takes to run thecode as pǫ increases. For the patient R data there is less sensitivity to pǫ. Indeed, the posteriordistributions of N are very similar for pǫ = 0.001 and pǫ = 0.01. However, for pǫ = 0.025 the16 17 18 19 20 21 22 23 2400.050.10.150.20.250.30.35NP(N|y)(a) pǫ = 0.001, 36 hrs16 17 18 19 20 21 22 23 2400.050.10.150.20.250.30.35NP(N|y)(b) pǫ = 0.01, 18 hrs16 17 18 19 20 21 22 23 2400.050.10.150.20.250.30.35NP(N|y)(c) pǫ = 0.025, 15 hrsFigure 4: Sensitivity analysis of pǫ for the 20 unit simulated data. The subfigure labels showthe time taken to run the marginal method for the value of pǫ shown.1912 14 16 18 20 22 2400.050.10.150.20.25NP(N|y)(a) pǫ = 0.001, 157 hrs12 14 16 18 20 22 2400.050.10.150.20.25NP(N|y)(b) pǫ = 0.01, 36 hrs11 12 13 14 15 16 17 18 19 20 21 22 23 2400.050.10.150.20.25NP(N|y)(c) pǫ = 0.025, 16 hrsFigure 5: Sensitivity analysis of pǫ for the patient O data. The subfigure labels show the timetaken to run the marginal method for the value of pǫ shown.18 20 22 24 26 28 30 3200.050.10.150.20.25NP(N|y)(a) pǫ = 0.001, 94 hrs18 19 20 21 22 23 24 25 26 27 28 29 30 31 3200.050.10.150.20.25NP(N|y)(b) pǫ = 0.01, 47 hrs18 20 22 24 26 28 30 3200.050.10.150.20.25NP(N|y)(c) pǫ = 0.025, 24 hrsFigure 6: Sensitivity analysis of pǫ for patient R data. The subfigure labels show the timetaken to run the marginal method for the value of pǫ shown.2016 18 20 22 24 2600.050.10.150.20.250.30.35NP(N|y)(a) M = 50015 16 17 18 19 20 21 22 23 24 2500.050.10.150.20.250.30.35NP(N|y)(b) M = 1000Figure 7: Posterior distribution of N for the 20 unit simulated dataset based upon the stochas-tic approximation of the observed likelihood withM = 500 (left) andM = 1000 (right) drawsfrom the prior.mode of N drops by 1. There is again a substantial decrease in computation time, but it isless dramatic compared with the patient O data.From these examples it seems clear that increasing pǫ above 0.01 biases the inferences of Ntowards lower values.5.5 Comparison with MCWMIn this section we trial instead the SA, which amounts to using the MCWM algorithm ofAndrieu and Roberts (2009). We use the patient O, R, A and 20 unit simulated datasetseach with either M = 500 or M = 1000 samples drawn from the prior distribution of firingindicators (conditional on current values of the excitability curves). Standard RJMCMC failedfor the patient A data.For the patient O and R data, it is clear that 500 simulations from the prior are sufficientenough to produce an accurate posterior distribution (it is very similar to the DA so the resultsare not shown). For the 20 unit simulated dataset, it is clear that 500 simulations from theprior are not enough to result in a precise estimate of the conditional distribution functionof these indicators. Some important firing combinations must be omitted from the marginallikelihood summation. From Figure 7 it is evident what the consequences are; the posteriordistribution is much flatter compared withM = 1000 (which is still somewhat inflated relativeto the corresponding DA posterior distribution). For the patient A data (Figure 8) it is againevident that the posterior of N for M = 500 is flatter. Even with M = 1000 the distributionof N is slightly more vague compared with the results of the DA.Finally, we investigated the guideline from Doucet et al. (2012) for choosing M , which wasdeveloped for the GIMH algorithm and mentioned earlier in our paper, in the context of our2122 24 26 28 30 3200.050.10.150.20.250.3NP(N|y)(a) M = 50021 22 23 24 25 26 27 28 29 30 31 3200.050.10.150.20.250.3NP(N|y)(b) M = 1000Figure 8: Posterior distribution of N for the patient A dataset based upon the stochasticapproximation of the observed likelihood with M = 500 (left) and M = 1000 (right) drawsfrom the prior.MCWM algorithm. For each of the datasets considered in this subsection, we generated 1000estimated log-likelihood values at a parameter value with reasonable posterior support. Thestandard deviation of the estimated log-likelihood withM = 500 for the patient O and R datawas roughly 0.74 and 0.76 respectively. Both of these values satisfy the guideline of beingless than 1, and as mentioned above this choice of M led to precise posterior distributions forN compared with the DA approach. For the 20 unit simulated data, the standard deviationbased on M = 500 and M = 1000 was approximately 1.56 and 1.11 respectively. We foundthat M = 500 was not sufficient, and we note that 1.56 is above the guideline. M = 1000produced a more accurate posterior distribution but there was still a small difference comparedwith the DA. The standard deviation in this case was slightly above 1. For the patient Adata, the standard deviations were 3.47 and 2.16 for M = 500 and M = 1000, respectively.Both of those are substantially greater than the guideline of 1. M = 500 gave a poor resultwhile M = 1000 did lead to a posterior for N that was closer to the DA but there are stillsome differences. However, it is important to note that the DA used pǫ = 0.01, so may not beas accurate as the other DAs where pǫ = 0.001. Therefore, based on this small study, thereis no indication yet that the guideline set by Doucet et al. (2012) for the GIMH algorithm isnot suitable for the MCWM algorithm.6 DiscussionRJMCMC is generally a difficult algorithm with which to achieve proper sampler performanceas it is often challenging to devise reasonable moves to cross varying dimensional parameterspaces. This problem is exacerbated when adjacent models differ substantially in dimension.The RJMCMC approach of Ridall et al. (2007) suffers from these issues since the model22involves N latent binary indicators in the likelihood function that need to be proposed foreach observation. In this paper we have demonstrated the utility of performing marginali-sation over these latent variables within RJMCMC to improve cross-dimensional acceptancerates. The implication of using marginalisation is that the dimensionality difference betweenadjacent models is relatively much smaller and furthermore the problem of devising efficientproposals for the parameters that were marginalised over is avoided. However the marginali-sation that we apply in the MUNE context is generally intractable, so we proposed suitableapproximations that improved the method’s applicability.Our deterministic approximation of the marginalisation considers only the B MUs out of Nthat were firing stochastically at each stimulus value. The approximation was very accurateand allowed for computationally tractable inferences to be performed on datasets collectedfrom patients. Substantial computational gains may be achieved since the likelihood for eachobservation could be computed in parallel, for example on a Graphics Processing Unit (seeLee et al. (2010) for an overview).A by-product of the marginalisation over our latent binary indicator variables is that it pro-duces the likelihood, which can then be used in model choice approaches such as DIC forexample. However, we demonstrated that both of these approaches could not produce infer-ences on N that were consistent with RJMCMC. This highlights the need for further devel-opment of our RJMCMC for MUNE so it can be applied to any dataset. We have made someprogress towards the applicability of the MCMC algorithm to a general dataset by developingwithin-model moves that do not require any tuning (see Appendix A). Further research is stillrequired to obtain a fast and reliable MUNE method.With reference to Ridall et al. (2007), we made minor simplifications to the statistical modeland substantial changes to the prior distributions to reflect aspects of the data more closelyand impose neurologists’ knowledge on the excitability curves. More specifically we replacedthe random effects with fixed effects and hence eliminated hyper parameters from the model,therefore not requiring MCMC updates for such parameters. Furthermore, we made substan-tial modifications to the within-model MCMC algorithm compared with Ridall et al. (2007)by working exclusively with the firing indicators rather than the thresholds and using slicesamplers which do not require tuning. In general we find the mixing of the algorithm issubstantially improved.In the model the value of the smallest possible MUAP, µmin, is a critical parameter of scien-tific importance Bromberg (2003). Its resolution within the neurophysiology literature is ofimportance as we find that inferences on N can be sensitive to this value.The issue of multimodality in our model may be evident with some datasets. As a solutionto this problem we ran each algorithm three times. Recent attempts to overcome the prob-lem that most MCMC methods converge to a single mode with multimodal posteriors haveinvolved population based methods (e.g. Jasra et al. (2007)), sequential Monte Carlo meth-ods (SMC) (e.g. Del Moral et al. (2006)) and, more recently, free energy SMC (Chopin andJacob, 2010). For future research we will consider such less ad-hoc approaches for escapingnon-optimal modes. However, none of these approaches is entirely successful and some involvea degree of inspirational discovery and experimentation as is the case with free energy SMC.Unfortunately the literature on global MCMC moves is not as advanced as local MCMC moves(see Girolami and Calderhead (2011) for example) and, as highlighted from this problem, is23an important area of research.MND is a difficult disease to assess due to the underlying pathology of active denervation andreinnervation of motor units. There is a need for a biomarker of the disease to assess diseaseprogression. The challenges in finding a reliable biomarker have meant that despite 40 yearsof research, no MUNE method is close to widespread applicability. A method needs to bereliable in giving an accurate answer to the number of motor units (or at least provide anindex) across the spectrum of motor unit loss, and yet be practical enough to be performedon standard EMG machines and be understood by clinicians. The method outlined in thispaper based on incorporating recent advances in Bayesian computation appears useful as ameans of assessing a moderate number of motor units.AcknowledgementsThe authors would like to thank Dr Fusun Baumann of UQ for assistance in the data collection.The authors are also indebted to Matt Armstrong who implemented some of the algorithmand suggested some of the modifications to the model. Dr Gareth Ridall provided usefulcomments and suggestions. The authors would also like to acknowledge the supercomputingresources of the high performance computing team of QUT.ReferencesAndrieu, C. and Roberts, G. O. (2009). The pseudo-marginal approach for efficient MonteCarlo computations. The Annals of Statistics, 37(2):697–725.Baumann, F., Henderson, R. D., Ridall, P. G., Pettitt, A. N., and McCombe, P. A. (2012).Quantitative studies of lower motor neuron degeneration in subjects with amyotrophiclateral sclerosis. To appear in Clinical Neurophysiology.Blok, J. H., Ruitenberg, A., Maathuis, E. M., and Visser, G. H. (2007). The electrophysio-logical muscle scan. Muscle & nerve, 36(4):436–446.Blok, J. H., Visser, G. H., de Graaf, S., Zwarts, M. J., and Stegeman, D. F. (2005). Sta-tistical motor unit number estimation assuming a binomial distribution. Muscle & Nerve,31(2):182–191.Bromberg, M. B. (2003). Consensus. In Bromberg, M. B., editor, Motor Unit NumberEstimation, Proceedings of the First International Symposium on MUNE, pages 333–338,Snowbird, Utah. Elsevier.Bromberg, M. B. (2007). Updating motor unit number estimation (MUNE). Clinical Neuro-physiology, 118(1):1–8.Casella, G. T. B., Almeida, V. W., Grumbles, R. M., Liu, Y., and Thomas, C. K. (2010).Neurotrophic factors improve muscle reinnervation from embryonic neurons. Muscle &Nerve, 42(5):788–797.24Celeux, G., Forbes, F., Robert, C. P., and Titterington, D. M. (2006). Deviance informationcriteria for missing data models. Bayesian Analysis, 1(4):651–674.Chopin, N. and Jacob, P. (2010). Free energy sequential Monte Carlo, application to mixturemodelling. In Bernardo, J. M., Bayarri, M. J., Berger, J. O., Dawid, A. P., Heckerman, D.,Smith, A. F. M., and West, M., editors, Bayesian Statistics 9 Ninth Valencia InternationalMeeting on Bayesian Statistics.Daube, J. R. (1995). Estimating the number of motor units in a muscle. Journal of ClinicalNeurophysiology, 12(6):585–594.Del Moral, P., Doucet, A., and Jasra, A. (2006). Sequential Monte Carlo samplers. Journalof the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436.Doucet, A., Pitt, M., and Kohn, R. (2012). Efficient implementation of Markov chain MonteCarlo when using an unbiased likelihood estimator. arXiv:1210.1871.Friedman, J., Hastie, T., and Tibshirani, R. (2001). The elements of statistical learning,volume 1. Springer Series in Statistics.Girolami, M. and Calderhead, B. (2011). Riemann manifold Langevin and Hamiltonian MonteCarlo methods (with discussion). Journal of the Royal Statistical Society: Series B (Sta-tistical Methodology), 73(2):123–214.Glasbey, C. (2007). Discussion of: Motor unit number estimation using reversible jumpMarkov chain Monte Carlo methods by P. G. Ridall et al. Journal of the Royal StatisticalSociety: Series C (Applied Statistics), 56:235–269.Henderson, R., Ridall, P., Hutchinson, N., Pettitt, A., and McCombe, P. (2007). Bayesianstatistical MUNE method. Muscle & Nerve, 36(2):206–213.Henderson, R. D., Ridall, G. R., Pettitt, A. N., McCombe, P. A., and Daube, J. R. (2006).The stimulus-response curve and motor unit variability in normal subjects and subjectswith amyotrophic lateral sclerosis. Muscle & Nerve, 34(1):34–43.Jasra, A., Stephens, D. A., and Holmes, C. C. (2007). On population-based simulation forstatic inference. Statistics and Computing, 17(3):263–279.Lee, A., Yau, C., Giles, M. B., Doucet, A., and Holmes, C. C. (2010). On the utility ofgraphics cards to perform massively parallel simulation of advanced Monte Carlo methods.Journal of Computational and Graphical Statistics, 19(4):769–789.Lomen-Hoerth, C. and Slawnych, M. P. (2003). Statistical motor unit number estimation:from theory to practice. Muscle & Nerve, 28(3):263–272.McComas, A. J., Fawcett, P. R., Campbell, M. J., and Sica, R. E. (1971). Electrophysiologicalestimation of the number of motor units within a human muscle. British Medical Journal,34(2):121–131.Neal, R. M. (2003). Slice sampling. Annals of Statistics, 31(3):705–741.25Pecher, C. (1939). La fluctuation d’excitabilite de la fibre nerveuse. Archives Of PhysiologyAnd Biochemistry, 49(2):129–152.Pericchi, L. R. (2005). Bayesian thinking: modeling and computation, chapter Model selectionand hypothesis testing based on objective probabilities and Bayes factors, pages 115–149.Elsevier B. V.Raftery, A. E. (1996). Approximate Bayes factors and accounting for model uncertainty ingeneralised linear models. Biometrika, 83(2):251–266.Ridall, P. G., Pettitt, A. N., Friel, N., McCombe, P. A., and Henderson, R. D. (2007).Motor unit number estimation using reversible jump Markov chain Monte Carlo methods(with discussion). Journal of the Royal Statistical Society: Series C (Applied Statistics),56(3):235–269.Ridall, P. G., Pettitt, A. N., Henderson, R. D., and McCombe, P. A. (2006). Motor unitnumber estimation - a Bayesian approach. Biometrics, 62(4):1235–1250.Robert, C. and Marin, J. M. (2008). On some difficulties with a posterior probability approx-imation technique. Bayesian Analysis, 3(2):427–442.Scha¨fer, C. and Chopin, N. (2011). Sequential Monte Carlo on large binary sampling spaces.Statistics and Computing, DOI: 10.1007/s11222-011-9299-z:1–22.Shefner, J. M. (2009). Statistical motor unit number estimation and ALS trials: the effect ofmotor unit instability. Supplements to Clinical Neurophysiology, 60:135–141.Spiegelhalter, D. J., Best, N. G., Carlin, B. P., and Van Der Linde, A. (2002). Bayesianmeasures of model complexity and fit. Journal of the Royal Statistical Society: Series B(Statistical Methodology), 64(4):583–639.Spiegelhalter, D. J. and Smith, A. F. M. (1982). Bayes factors for linear and log-linearmodels with vague prior information. Journal of the Royal Statistical Society: Series B(Methodological), pages 377–387.Vermaak, J., Andrieu, C., Doucet, A., and Godsill, S. J. (2004). Reversible jumpMarkov chainMonte Carlo strategies for Bayesian model selection in autoregressive processes. Journal ofTime Series Analysis, 25(6):785–809.A MCMC UpdateIn this appendix we provide details on the within-model moves that are used in all algorithms.An important aspect of the changes to the model and the MCMC moves we detail here is thatthe algorithm no longer requires any tuning and all within-model moves are accepted with aprobability of one (as opposed to Ridall et al. (2006)). The extra robustness in our algorithmsuggest that they are more likely to be applicable to a wider range of clinical datasets.26A.1 Within-model UpdatesFor a fixed value of N , a component-wise sampler is proposed to draw from the full posteriordistribution of the parameters, which produces marginal posterior samples of each parameteras a by-product. Our MCMC algorithm differs in several places compared with the within-model updates of Ridall et al. (2007) due to both changes in the model and those designed toimprove mixing.Our baseline parameters are fixed and the random effects are replaced by fixed effects. There-fore no updates are required for the baseline and hyper parameters. As has been previouslydone, ηt is drawn from its gamma full conditional distribution.In contrast to Ridall et al. (2007) who propose to update τk,t directly and hence sk,t indirectly,we update sk,t directly from its full conditional.Our updates for δk and mk differ from Ridall et al. (2007). The full conditionals for theseparameters can be expressed in terms of the thresholds, τk,t, or the firing indicators, sk,t. Forexample, the full conditional distribution of δk may be expressed asp(δk|rest) ∝ p(δk)T∏t=1p(τk,t|mk, δk) orp(δk|rest) ∝ p(δk)T∏t=1p(sk,t|mk, δk),as the sk,ts can be derived deterministically from the τk,ts. The full conditionals in terms ofthe τk,ts are appealing since they produce full conditionals for δk and mk which are gammaand truncated normal respectively (Ridall et al., 2007). However, we find that such updatesresult in very slow mixing of the Markov chain. We obtain substantially larger jumps whenconditioning on the firing indicators, sk,t, at the expense of the full conditionals not havinga closed form expression. To circumvent this we draw from the full conditionals using a slicesampler (Neal, 2003) rather than perfect sampling.The slice sampler is an auxiliary variable technique where the auxiliary variable, u, is drawnfrom a uniform distribution between 0 and the target density (known up to a normalisingconstant) at the current value, say π(x) where x is a realisation of a random variable X. Thetarget distribution is cut by a horizontal slice at π(u) and the slice interval, S, is defined as alla in the range of X such that π(a) ≥ π(u). The next iterate is drawn uniformly from that sliceinterval. The difficulty is obtaining the slice interval. However Neal (2003) proposes to stepout until a range is found that contains S and to shrink the range if a random variate is drawnoutside S (see Neal (2003) for more detail and the proof of validity of such moves). For themkparameters the slice interval we use is given directly by the prior range (mk−1,mk+1) (whichis fixed in the update for mk) and we shrink the interval if a proposal is not found withinthe slice. For the slice sampler involving δk we perform the step out and shrink procedures.The slice sampler is practically useful as it has an acceptance probability of one and hencerequires no tuning (the step out and shrink procedure can be tuned to increase the speed butit does not affect the rate of convergence).Owing to the introduction of the improper prior for σ2, the full conditional distribution ofσ2+σ2b has an inverse gamma distribution, thereby obtaining a sample of σ2 by subtracting the27fixed σ2b . In addition to removing the subjectivity, this improper prior allows us to eliminatethe Metropolis-Hastings step required in Ridall et al. (2007) where an independent proposalfrom a gamma distribution with a mode and variance determined from the derivatives of the(non-closed form) full conditional is applied.Ridall et al. (2007) factorise the full conditional for µk into the product of a normal densityand the gamma prior. Our prior on µk is uniform therefore the resulting full conditional is(truncated) normal and the Metropolis-Hastings step can be avoided.One iteration of the MCMC update we use is given below. Here, 1(·) denotes the indicatorfunction.A.2 Update for sk,tThe firing indicators sk,t can be updated individually by drawing from the full conditionalp(sk,t|rest) ∝ p(yt|St,µ,η, st, σ)p(sk,t|mk, δk).A.3 Update for µkHere we draw an update for µk straight from the truncated normal full conditional given byp(µk|rest) ∝ N(µk;∑Tt=1sk,tVt(yt − (µt − µk))∑Tt=1sk,tVt,1∑Tt=1sk,tVt)1(µmin < µk < µmax).A.4 Update for σ2Here we draw σ2 + σ2b from the full conditional distributionp(σ2 + σ2b |rest) ∝ Inv. Gamma(σ2 + σ2b ;∑Tt=1 1(nt > 0)2,12T∑t=11(nt > 0)ηt(yt − µt)2)1(σ2 > 0),where nt is the number of MUs firing at stimulus St. A draw from σ2 is simply obtained bysubtracting the fixed σ2b from the inverse gamma random variate provided σ2 > 0 otherwiseanother draw is made.A.5 Update for δkThe full conditional for δk is given byp(δk|rest) ∝ p(δk)T∏t=1p(sk,t|mk, δk).We draw an approximate sample from this density using a slice sampler.28A.6 Update for mkThe full conditional for mk is given byp(mk|rest) ∝ 1(mk−1 < mk < mk+1)T∏t=1p(sk,t|mk, δk),where m0 = Snone and mN+1 = Sall. We draw an approximate sample from this density usinga slice sampler.A.7 Update for ηtA component-wise update for ηt is to draw directly from the full conditionalp(ηt|rest) = Gamma(ηt; ǫ+12, ǫ+(yt − µt)22(σ2b + σ21(nt > 0))).B Between Model UpdatesIn the birth move, to match dimensions, we generate the new δ parameter from its nowinformative prior. The bijection for the birth move is given by(δN+1i δN+1i+1 ) =(δNi δ∗δ∗ δNi).The two rows on the right hand side correspond to two different possible moves which areproposed with equal probability. As specified before, δ∗ ∼ Gamma(αδ , βδ). This approachavoids the extra tuning parameter required for dimension matching of the δ parameters inRidall et al. (2007). Furthermore, the Jacobian term of this part of the transformation isequal to one. The bijections for the m and µ parameters are the same as that in the one totwo unit moves of Ridall et al. (2007) and for completeness are given by(mN+1i mN+1i+1 ) =(mNi−1 + u1(mNi −mNi−1) mNimNi mNi + u1(mNi+1 −mNi ))(3)(µN+1i µN+1i+1 ) = (u2(µNi − µmin) µNi − u2(µNi − µmin)),where u1, u2 ∼ U(0, 1). The two rows of the first equation in (3) correspond to different moveswhich are proposed with equal probability. If either of µN+1i or µN+1i+1 is below µmin then thebirth move is rejected. As per Ridall et al. (2007) the σ and η parameters remain unchangedin the between model moves.Ridall et al. (2007) choose a unit, i ∈ {1, . . . , N}, at random for the split and merge moves.We note that it is not possible to split a unit with a µ less than 2µmin. Furthermore, itseems plausible that the most uncertainty in N comes in stimulus ranges where a substantialamount of excitability curve overlapping occurs. Therefore we attempt to bias the probabilityof selecting the ith unit to perform a split (pspliti ) and a merge move (pmergei ) appropriately.29In either the split (N → N + 1) or merge (N + 1→ N) moves we compute, for the ith unit,the number of MUs firing stochastically (including itself) according to some threshold. Wecomputepij = P (mi < Xj) for j = 1, 2, . . . , N,where Xj ∼ N(mj, 1/δ2j ) based on current and proposed values of the excitability parameters(whatever is current and proposed depends on whether a split or merge move is being at-tempted). Then the probability of selecting the ith unit for a split/merge, pi, is proportionalto the cardinality of {pij : 0.01 < pij < 0.99 for j = 1, . . . , N}. Clearly under this constructioneach unit still has a non-zero chance of being selected as pii = 0.5. When the split move isconsidered, there is an extra complexity. As mentioned earlier, a unit with a mean MUAPless than 2µmin cannot be split. We do not consider such guaranteed rejections. Thereforethe probabilities of selecting the ith unit for a split and merge step are given respectively bypspliti ∝ 1(µi > 2µmin)N∑j=11(0.01 < pij < 0.99), i = 1, . . . , N,pmergei ∝N∑j=11(0.01 < pij < 0.99), i = 1, . . . , N.Given current parameters (mN , δN , µN ) for the N model and proposed parameters (mN+1,δN+1, µN+1) for the N + 1 model the acceptance probability of a birth from N to N + 1MUs using the marginal approach is given byα1→2 = min(1,pˆ(y|mN+1, δN+1,µN+1,η, σ,N + 1)pˆ(y|mN , δN ,µN ,η, σ,N)1µmax − µmin(N + 1)Sall − SnonepmergeipsplitiJµJm),where the use of pˆ denotes the fact that we are using some approximation to the likelihood.Here the J represents the Jacobian part of the acceptance probability. Here Jµ = µNi − µminand Jm equals mNi − mNi−1 or mNi+1 − mNi depending on which row in the right hand sideof equation (3) is selected. The prior on N is discrete uniform over a wide enough range toencompass all plausible values of N . The dimension matching variables u1 and u2 have adensity equal to one. The dimension matching variable δ∗ cancels with the prior distributionfor δ. The corresponding merge move has a Metropolis-Hastings ratio that is the reciprocalof the above Metropolis-Hastings ratio.The most substantial difference in the reversible jump algorithm we apply here is that we(approximately) integrate the firing indicators out of the target distribution. We can use thedeterministic approximation (Section 4.2.2) or the stochastic approximation (Section 4.2.1).However, unlike Section 4.1, the extra normal random variates remain.Upon acceptance of a cross-dimensional move with the marginal approach, we need to generatea new set of indicators since these are required in the within-model updates. In the DA, weform an approximation to the full conditional distribution of the latent binary indicators foreach observation, which is exact when pǫ = 0. We therefore draw the indicators for eachobservation from this approximate full conditional distribution. In the SA, the proposals aregenerated from the prior of the indicators conditional on the current excitability parameters.30For each observation, we compute the conditional probability of those indicators (including thelikelihood of the observation) and draw one of these indicators from this proportional to theirprobabilities. It is unclear whether or not drawing from this prior would create a distributionof indicators that is close to the full conditional, although it would be more accurate with moredraws from the prior. It should be noted that neither of these approaches are theoreticallyvalid from an MCMC point of view as neither is from the true full conditional. However wecan be confident of obtaining a reasonable approximation for small pǫ. In fact, for small pǫthis joint move is likely to be better than the within-model updates for the indicators, whichjust updates the indicator for each unit within each observation one at a time. This is anotherbenefit of the approach.For each iteration of the reversible jump algorithms, we perform one within-model updatefollowed by a proposed cross-dimensional move with a birth or death move equally likely tobe proposed.31",
    "id": 10914192,
    "identifiers": {
        "doi": "10.1016/j.csda.2013.11.003",
        "oai": "oai:eprints.qut.edu.au:54864"
    },
    "title": "Marginal reversible jump Markov chain Monte Carlo with application to motor unit number estimation",
    "language": {
        "code": "en",
        "name": "English"
    },
    "publishedDate": "2014-01-01T00:00:00+00:00",
    "publisher": "Elsevier",
    "references": [],
    "sourceFulltextUrls": [
        "http://eprints.qut.edu.au/54864/"
    ],
    "updatedDate": "",
    "yearPublished": "2014",
    "links": [
        {
            "type": "download",
            "url": "https://core.ac.uk/download/10914192.pdf"
        },
        {
            "type": "reader",
            "url": "https://core.ac.uk/reader/10914192"
        },
        {
            "type": "thumbnail_m",
            "url": "https://core.ac.uk/image/10914192/medium"
        },
        {
            "type": "thumbnail_l",
            "url": "https://core.ac.uk/image/10914192/large"
        },
        {
            "type": "display",
            "url": "https://core.ac.uk/outputs/10914192"
        }
    ],
    "abstract": "Motor unit number estimation (MUNE) is a method which aims to provide a quantitative indicator of progression of diseases that lead to loss of motor units, such as motor neurone disease. However the development of a reliable, repeatable and fast real-time MUNE method has proved elusive hitherto. Ridall et al. (2007) implement a reversible jump Markov chain Monte Carlo (RJMCMC) algorithm to produce a posterior distribution for the number of motor units using a Bayesian hierarchical model that takes into account biological information about motor unit activation. However we find that the approach can be unreliable for some datasets since it can suffer from poor cross-dimensional mixing. Here we focus on improved inference by marginalising over latent variables to create the likelihood. In particular we explore how this can improve the RJMCMC mixing and investigate alternative approaches that utilise the likelihood (e.g. DIC (Spiegelhalter et al., 2002)). For this model the marginalisation is over latent variables which, for a larger number of motor units, is an intractable summation over all combinations of a set of latent binary variables whose joint sample space increases exponentially with the number of motor units. We provide a tractable and accurate approximation for this quantity and also investigate simulation approaches incorporated into RJMCMC using results of Andrieu and Roberts (2009)",
    "tags": [
        "Contribution to Journal",
        "Amyotrophic lateral sclerosis",
        "Marginalisation",
        "Markov chain Monte Carlo",
        "Model choice",
        "Motor Neurone disease",
        "Motor unit number estimation",
        "Neurophysiology",
        "Reversible jump"
    ],
    "fulltextStatus": "enabled",
    "subjects": [
        "Contribution to Journal"
    ],
    "oai": "oai:eprints.qut.edu.au:54864",
    "deleted": "ALLOWED",
    "disabled": false,
    "journals": null,
    "repositories": {
        "id": "310",
        "openDoarId": 0,
        "name": "Queensland University of Technology ePrints Archive",
        "urlHomepage": null,
        "uriJournals": null,
        "physicalName": "noname",
        "roarId": 0,
        "baseId": 0,
        "pdfStatus": null,
        "nrUpdates": 0,
        "lastUpdateTime": null
    },
    "repositoryDocument": {
        "id": 10914192,
        "depositedDate": "2014-04-01T00:00:00+01:00",
        "publishedDate": "2014-01-01T00:00:00+00:00",
        "updatedDate": "2024-01-30T08:39:38+00:00",
        "acceptedDate": "2013-11-09T00:00:00+00:00",
        "createdDate": "2013-07-02T14:31:55+01:00"
    },
    "urls": [],
    "lastUpdate": "2024-01-30T08:39:38+00:00",
    "setSpecs": []
}