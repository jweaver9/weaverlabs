{
    "acceptedDate": "2014-05-03T00:00:00+01:00",
    "authors": [
        {
            "name": "Hastie, DI"
        },
        {
            "name": "Liverani, S"
        },
        {
            "name": "Richardson, S"
        }
    ],
    "contributors": [],
    "createdDate": "2015-05-18T18:12:54+01:00",
    "dataProvider": {
        "id": 14,
        "name": "Brunel University Research Archive",
        "url": "https://api.core.ac.uk/v3/data-providers/14",
        "logo": "https://api.core.ac.uk/data-providers/14/logo"
    },
    "depositedDate": "2014-05-03T00:00:00+01:00",
    "documentType": "research",
    "doi": "10.1007/s11222-014-9471-3",
    "downloadUrl": "https://core.ac.uk/download/29139326.pdf",
    "fullText": "Noname manuscript No.(will be inserted by the editor)Sampling from Dirichlet process mixture models with unknownconcentration parameter: Mixing issues in large data implementationsDavid I. Hastie∗ · Silvia Liverani∗ · Sylvia RichardsonReceived: date / Accepted: dateAbstract We consider the question of Markov chain MonteCarlo sampling from a general stick-breaking Dirichlet pro-cess mixture model, with concentration parameter α. Thispaper introduces a Gibbs sampling algorithm that combinesthe slice sampling approach of Walker (2007) and the retro-spective sampling approach of Papaspiliopoulos and Roberts(2008). Our general algorithm is implemented as efficientopen source C++ software, available as an R package, and isbased on a blocking strategy similar to that suggested by Pa-paspiliopoulos (2008) and implemented by Yau et al (2011).We discuss the difficulties of achieving good mixing inMCMC samplers of this nature in large data sets and investi-gate sensitivity to initialisation. We additionally consider thechallenges when an additional layer of hierarchy is addedsuch that joint inference is to be made on α. We introducea new label-switching move and compute the marginal par-tition posterior to help to surmount these difficulties. Ourwork is illustrated using a profile regression (Molitor et al,2010) application, where we demonstrate good mixing be-haviour for both synthetic and real examples.Keywords Dirichlet process · mixture model · profileregression · Bayesian clusteringDavid I. HastieImperial College London, UKSilvia LiveraniImperial College London, UK andMRC Biostatistics Unit, Cambridge, UKSylvia RichardsonMRC Biostatistics Unit, Cambridge, UKE-mail: sylvia.richardson@mrc-bsu.cam.ac.uk∗ Joint first authors1 IntroductionFitting mixture distributions to model some observed data isa common inferential strategy within statistical modelling,used in applications ranging from density estimation to re-gression analysis. Often, the aim is not only to fit the mix-ture, but additionally to use the fit to guide future predic-tions. Approaching the task of mixture fitting from a para-metric perspective, the task to accomplish is to cluster theobserved data and (perhaps simultaneously) determine thecluster parameters for each mixture component. This task issignificantly complicated by the need to determine the num-ber of mixture components that should be fitted, typicallyrequiring complicated Markov chain Monte Carlo (MCMC)methods such as reversible jump MCMC techniques (Richard-son and Green, 1997) or related approaches involving paral-lel tempering methods (Jasra et al, 2005).An increasingly popular alternative approach to para-metric modelling is to adopt a Bayesian non-parametric ap-proach, fitting an infinite mixture, thereby avoiding deter-mination of the number of clusters. The Dirichlet process(Ferguson, 1973) is a well studied stochastic process thatis widely used in Bayesian non-parametric modelling, withparticular applicability for mixture modelling. The use ofthe Dirichlet process in the context of mixture modelling isthe basis of this paper and we shall refer to the underlyingmodel as the Dirichlet process mixture model, or DPMM forbrevity.The idea of sampling from the DPMM is not new andhas been considered by a number of authors including Es-cobar and West (1995), Neal (2000), Ishwaran and James(2001), and Yau et al (2011). While the continual evolutionof samplers might implicitly suggest potential shortcomingsof previous samplers, new methods are often illustrated onsynthetic or low dimensional datasets which can mask is-sues that might arise when using the method on problemsarXiv:1304.1778v2  [stat.CO]  20 Feb 20142 David I. Hastie∗ et al.of even modest dimension. In fact, it appears that little ex-plicit discussion has been presented detailing the inherentdifficulties of using a Gibbs (or Metropolis-within-Gibbs)sampling approach to update such a complex model space,although there are some exceptions, for example Jain andNeal (2007), in the context of adding additional split-mergetype moves into their sampler.For real (rather than synthetic) data applications of theDPMM, the state space can be highly multimodal, with wellseparated regions of high posterior probability co-existing,often corresponding to clusterings with different number ofcomponents. We demonstrate that such highly multimodalspaces present difficulties for the existing sampling meth-ods to escape the local modes, with poor mixing resulting ininference that is influenced by sampler initialisation. In themost serious case, this can be interpreted as non-convergenceof the MCMC sampler. A primary contribution of this paperis to demonstrate these issues, highlighting that if only cer-tain marginals are used to determine convergence they mayfail to identify any issue. To address this we introduce theMarginal Partition Posterior as a more robust way of moni-toring convergence.A secondary (and more subtle) mixing issue relates tothe mixing across the ordering of clusters in a particularclustering process, when a stick breaking construction is used.As we shall detail, such issues are particularly importantwhen simultaneous inference is desired for the concentrationparameter α, as defined in the following section. This mix-ing issue was highlighted by Papaspiliopoulos and Roberts(2008) who observed that the inclusion of label-switchingmoves can help to resolve the problem. We demonstrate thatthe moves that they propose offer only a partial solution tothe problem, and we suggest an additional label-switchingmove that appears to enhance the performance of our ownimplementation of a DPMM sampler.In the following section, we present the further detailsof the DPMM. Section 3 discusses some of the mixing is-sues with DPMM samplers, including Section 3.2 where weintroduce the new label-switching move. This is followedby Section 4 where we present a method that we have founduseful for determining sampler convergence. The implemen-tation of our sampler is briefly summarised in Section 5 be-fore Section 6 demonstrates some of the earlier ideas in thecontext of a real data example.2 Dirichlet process mixture modelsA variety of ways have been used to show the existence ofthe Dirichlet Process, using a number of different formu-lations (Ferguson, 1973; Blackwell and MacQueen, 1973).In this paper we focus on Dirichlet process mixture models(DPMM), based upon the following constructive definitionof the Dirichlet process, due to Sethuraman (1994). IfP =∞∑c=1ψcδΘc ,Θc ∼ PΘ0 for c ∈ Z+,ψc = Vc∏l<c(1− Vl) for c ∈ Z+ \\ {1}, (1)ψ1 = V1, andVc ∼ Beta(1, α) for c ∈ Z+,where δx denotes the Dirac delta function concentrated atx, then P ∼ DP(α, PΘ0). This formulation for V and ψis known as a stick-breaking distribution. Importantly, thedistribution P is discrete, because draws Θ˜1, Θ˜2, . . . fromP can only take the values in the set {Θc : c ∈ Z+}.It is possible to extend the above formulation to moregeneral stick-breaking formulations (Ishwaran and James,2001; Kalli et al, 2011; Pitman and Yor, 1997).2.1 Sampling from the DPMMFor the DPMM, the (possibly multivariate) observed dataD = (D1, D2, . . . , Dn) follow an infinite mixture distribu-tion, where component c of the mixture is a parametric den-sity of the form fc(·) = f(·|Θc, Λ) parametrised by somecomponent specific parameter Θc and some global parame-ter Λ. Defining (latent) parameters Θ˜1, Θ˜2, . . . , Θ˜n as drawsfrom a probability distribution P following a Dirichlet pro-cess DP (α, PΘ0) and again denoting the dirac delta func-tion by δ, this system can be written,Di|Θ˜i, Λ ∼ f(Di|Θ˜i, Λ) for i = 1, 2, . . . , n, (2)Θ˜i ∼∞∑c=1ψcδΘc for i = 1, 2, . . . , n.When making inference using mixture models (either fi-nite or infinite) it is common practice to introduce a vectorof latent allocation variables Z. Such variables enable us toexplicitly characterise the clustering and additionally facili-tate the design of MCMC samplers. Adopting this approachand writing ψ = (ψ1, ψ2, . . .) and Θ = (Θ1, Θ2, . . .), were-write Equation 2 asDi|Z,Θ, Λ ∼ f(Di|ΘZi , Λ) for i = 1, 2, . . . , n,Θc ∼ PΘ0 for c ∈ Z+,P(Zi = c|ψ) = ψc for c ∈ Z+, i = 1, 2, . . . , n. (3)We refer to the model in Equation 3, with no variables in-tegrated out, as the full stick-breaking DPMM or even theFSBDPMM for conciseness.Historically, methods to sample from the DPMM (Esco-bar and West, 1995; Neal, 2000) have simplified the sam-ple space of the full stick-breaking DPMM by integratingout the mixture weights ψ. Collectively, such samplers haveSampling from Dirichlet process mixture models with unknown concentration parameter: Mixing issues in large data implementations 3been termed Polya` Urn samplers. Ishwaran and James (2001)presented a number of methods for extending Polya` Urnsamplers, and additionally suggested a truncation approachfor sampling from the full stick-breaking DPMM with novariables integrated out.More recently, two alternative innovative approaches tosample directly from the FSBDPMM have been proposed.The first, introduced by Walker (2007) and generalised byKalli et al (2011), uses a novel slice sampling approach, re-sulting in full conditionals that may be explored by the useof a Gibbs sampler. The second distinct MCMC samplingapproach was proposed in parallel by Papaspiliopoulos andRoberts (2008). The proposed sampler again uses a Gibbssampling approach, but is based upon an idea termed ret-rospective sampling, allowing a dynamic approach to thedetermination of the number of components (and their pa-rameters) that adapts as the sampler progresses. The costof this approach is an ingenious but complex Metropolis-within-Gibbs step, to determine cluster membership. De-spite the apparent differences between the two strategies, Pa-paspiliopoulos (2008) noted that the two algorithms can beeffectively combined to yield an algorithm that improves ei-ther of the originals. The resulting sampler was implementedand presented by Yau et al (2011), and a similar version wasused by Dunson (2009).The current work presented in this paper uses our ownsampler (described further in Section 5) based upon our in-terpretation of these ideas, implemented using our own block-ing strategy. Our blocking strategy may or may not be origi-nal (we are unable to say given that the full blocking strategyadopted by Yau et al (2011) is not explicitly detailed), butwe expect our approach to be based upon a sufficiently sim-ilar strategy such that the mixing issues that we demonstratewould apply equally to other authors’ implementations.2.2 An example modelEquation 3 is of course very general, indicating that sam-pling from the DPMM has wide scope across a variety ofapplications. However, it is perhaps equally instructive toconsider a specific less abstract example, that can be used tohighlight the issues raised in later sections.Profile regression Recent work has used the DPMM as analternative to parametric regression, non-parametrically link-ing a response vector Y with covariate data X by allocat-ing observations to clusters. The clusters are determined byboth the X and Y , allowing for implicit handling of po-tentially high dimensional interactions which would be verydifficult to capture in traditional regression. The approachalso allows for the possibility of additional “fixed effects”W which have a global (i.e. non-cluster specific) effect onthe response. The method is described in detail by Moli-tor et al (2010), Papathomas et al (2011), and Molitor et al(2011), who use the term profile regression to refer to theapproach. A similar model has independently been used byDunson et al (2008) and Bigelow and Dunson (2009).Using the notation introduced earlier in this Section, thedata becomes D = (Y ,X), and is modelled jointly as theproduct of a response model and and a covariate model re-sulting in the following likelihood:p(Di|Zi,Θ, Λ,Wi) = fY (Yi|ΘZi , Λ,Wi)fX(Xi|ΘZi , Λ).Discrete covariates with binary response Consider the casewhere for each observation i, Xi is a vector of J locally in-dependent discrete categorical random variables, where thenumber of categories for covariate j = 1, 2, . . . , J is Kj .Then definingΦc = (φc,1,1, . . . , φc,1,K1 , . . . , φc,J,1, . . . , φc,J,KJ ),we specify the covariate model as:P(Xi|Zi, ΦZi) =J∏j=1φZi,j,Xi,j .Suppose also that Yi is a binary response, such thatlogit{P(Yi = 1|θZi , β,Wi)} = θZi + βTWi,for some vector of coefficients β.This is simply an example of profile regression, withΘc = (Φc, θc) and Λ = β, such thatfY (Yi|ΘZi , Λ,Wi) = P(Yi|θZi , β,Wi), andfX(Xi|ΘZi , Λ) = P(Xi|Zi, ΦZi).We use this specific profile regression model to illustrateour results in this paper, both for the simulated dataset andthe real-data example. Suitable prior distributions for mak-ing inference about such a model are discussed in Molitoret al (2010) and we adopt the same priors for the examplespresented below. We note however that our conclusions andthe behaviour we report typically hold more broadly acrossthe range of models that we have tested.Simulated datasets One of the key messages of our work isthat DPMM samplers can perform well on simulated datasetsbut this does not necessarily carry through to real-data ex-amples. We present in-depth results for a real-data examplein Section 6, but to highlight the contrasting performancetwo simple simulated dataset are also used. Our first simu-lated data is from a profile regression model with 10 discretecovariates and a binary response variable. The dataset has1000 observations, partitioned at random into 5 groups ina balanced manner. The covariate and response distributions4 David I. Hastie∗ et al.corresponding to each partition were selected to be well sep-arated. The second simulated dataset is also from a profileregression model, but uses 10 discrete covariates, each with5 categories, as well as 10 fixed effects and a Bernoulli out-come. However, in this case, the data is sampled by mixingover values of α from its Gamma prior, Gamma(9, 0.5). Anexplicit description of the simulation methodology is pro-vided in the Supplementary Material.3 Mixing of MCMC algorithms for the DPMMSampling from a DPMM is a non-trivial exercise, as evi-denced by the number of different methods that have beenintroduced to address a wide array of issues. For Polya` Urnsamplers, with mixture weights ψ integrated out, a primarylimitation is that the conditional distribution of each clusterallocation variable depends explicitly upon all other clus-ter allocation variables. This means that the commonly usedGibbs samplers which typically update these variables oneat a time suffer from poor mixing across partition space. Us-ing Metropolis-within-Gibbs steps and bolder split-mergemoves (Jain and Neal, 2004) can improve results, but inhigh dimensional real-data applications, designing efficientmoves of this type is far from straightforward.The challenges associated with methods which samplefrom the FSBDPMM (most recently Yau et al, 2011 andKalli et al, 2011) have been perhaps less well documented.This is partially because the innovative and ingenious meth-ods that have facilitated such sampling have required signif-icant attention in their own right, with the consequence thatthe methods are often illustrated only on relatively simpledatasets.The purpose of the remainder of this Section, and themain contribution of our work, is to use our practical expe-rience to further understanding of the behaviour of this newtype of samplers, with particular emphasis on some of thechallenges of sampling from the FSBDPMM for real dataproblems.3.1 Initial number of clustersA difficulty that persists even with the inclusion of the inno-vative techniques that allow MCMC sampling directly fromthe FSBDPMM is being able to effectively split clusters andthereby escape local modes. This is partially due to the in-trinsic characteristics of partition spaces and the extremelyhigh number of possible ways to split a cluster, even if itonly has a small number (for example, 50 or more) sub-jects in it. Although sampling directly from the FSBDPMM(rather than integrating out the mixture weights) does im-prove mixing when updating the allocation variables, anyGibbs moves that update allocations and parameters individ-ually (or even in blocks) struggle to explore partition space.On the other hand, constructing more ambitious Metropolis-Hastings moves that attempt to update a larger number ofparameters simultaneously is also a very difficult task due tothe difficulty in designing moves to areas of the model spacewith similar posterior support.Rather than subtly ignoring the problem and reportingover confident inference when analysing case studies, wesuggest that, if used with caution, a FSBDPMM sampler stillprovides a useful inferential tool, but that its limitations mustbe realised and acknowledged. For example, because of thedifficulty that the sampler has in increasing the number ofclusters for situations involving data with weak signal, it isimportant to initialise the algorithm with a number of clus-ters which is greater than the anticipated number of clustersthat the algorithm will converge to. This necessarily involvesan element of trial and error to determine what that numberis, where multiple runs from different initialisations must becompared (for example using the ideas presented in Section4). This is demonstrated in Section 6.3.2 Cluster ordering, α and label-switchingA secondary area where mixing of a full DPMM sampler re-quires specific attention is the mixing of the algorithm overcluster orderings. In particular, whilst the likelihood of theDPMM is invariant to the order of cluster labels, the priorspecification of the stick breaking construction is not. Asdetailed by Papaspiliopoulos and Roberts (2008), the defi-nition of ψc in terms of Vc, imposes the relation E[ψc] >E[ψc+1] for all c. This weak identifiability, discussed in moredetail by Porteous et al (2006), also manifests itself throughthe result P (ψc > ψc+1) > 0.5 for all c, a result that weprove in Appendix A.1.The importance of whether the FSBDPMM algorithmmixes sufficiently across orderings depends partially uponthe object of inference. Specifically, since P (ψc > ψc+1)depends upon the prior distribution of α, if inference is tobe simultaneously made about α (as is the scenario consid-ered in this paper), it is very important that the algorithm ex-hibits good mixing with respect to α. If this was not the case,the posterior marginal distribution for α would not be ade-quately sampled, and since α is directly related to the num-ber of non-empty clusters (see Antoniak,1974 for details),poor mixing across ordering may further inhibit accurate in-ference being made about the number of non-empty clus-ters. This situation would be further exaggerated for moregeneral stick breaking constructions (of the sort mentionedin the introduction). While it is possible to set a fixed valueof α, more generally we wish to allow α to be estimated.To ensure adequate mixing across orderings, it is impor-tant to include label-switching moves, as observed by Pa-Sampling from Dirichlet process mixture models with unknown concentration parameter: Mixing issues in large data implementations 5paspiliopoulos and Roberts (2008). Without such moves, theone-at-a-time updates of the allocations Zi, mean that clus-ters rarely switch labels, and consequentially the orderingwill be largely determined by the (perhaps random) initial-isation of the sampler. For all choices of α, the posteriormodal ordering will be the one where the cluster with thelargest number of individuals has label 1, that with the sec-ond largest has label 2 and so on. However, α affects therelative weight of other (non-modal) orderings, and a prop-erly mixing sampler must explore these orderings accordingto their weights.We adopt the label-switching moves suggested by Pa-paspiliopoulos and Roberts (2008), and details can be foundtherein. However, in our experience, while these moves mayexperience high acceptance rates early on in the life of thesampler, once a “good” (in terms of high posterior support)ordering is achieved, the acceptance rates drop abruptly (seeSection 6, Figure 7) . This means that there is little furthermixing in the ordering space. Our concern is that while theselabel-switching moves appear to encourage a move towardsthe modal ordering, once that ordering is attained, the sam-pler rarely seems to escape too far from this ordering.Our solution is to introduce a third label-switching movethat we describe here. In brief, the idea is to simultaneouslypropose an update of the new cluster weights so they aresomething like their expected value conditional upon thenew allocations. Specifically, defining Z? = max1≤i≤n Ziand A = {1, . . . , Z?} the move proceeds as follows: firstchoose a cluster c randomly from A \\ {Z?}. Propose newallocationsZ ′i =c+ 1 i : Zi = cc i : Zi = c+ 1Zi otherwise.(4)and switch parameters associated to these clusters such thatΘ′l =Θc+1 l = cΘc l = c+ 1Θl otherwise.(5)Additionally, propose new weightsψ′c andψ′c+1 for com-ponents c and c+ 1 such thatψ′l =ψc+1ψ+Ψ ′E[ψc|Z′,α]E[ψc+1|Z,α] l = cψcψ+Ψ ′E[ψc+1|Z′,α]E[ψc|Z,α] l = c+ 1ψl otherwise,and (6)where ψ+ = ψc + ψc+1 andΨ ′ = ψc+1E[ψc|Z ′, α]E[ψc+1|Z, α] + ψcE[ψc+1|Z ′, α]E[ψc|Z, α] ,by settingV ′l =ψ′c∏l<c(1−Vl) l = cψ′c+1(1−V ′c )∏l<c(1−Vl) l = c+ 1Vl otherwise.(7)All other variables are left unchanged. Assuming that thereare nc and nc+1 individuals in clusters c and c + 1 respec-tively at the beginning of the update, the acceptance proba-bility for this move is then given by min{1, R} whereR =(ψ+ψc+1R1 + ψcR2)nc+nc+1Rnc+11 Rnc2 , where (8)R1 =1 + α+ nc+1 +∑l>c+1 nlα+ nc+1 +∑l>c+1 nl, and (9)R2 =α+ nc +∑l>c+1 nl1 + α+ nc +∑l>c+1 nl. (10)More details can be found in Appendix A.2.4 Monitoring convergenceAccepting that the challenge of convergence persists, it isclearly important that the user has diagnostic methods to as-sess whether convergence can be reasonably expected. Dueto the nature of the model space, many traditional techniquescannot be used in this context. For our hierarchical model, asdescribed in Equations 1 and 3, there are no parameters thatcan be used to meaningfully demonstrate convergence ofthe algorithm. Specifically, parameters in the vector Λ tendto converge very quickly, regardless of the underlying clus-tering, as they are not cluster specific and therefore are nota good indication of the overall convergence. On the otherhand the cluster parameters Θc, cannot be tracked, as theirnumber and interpretation changes from one iteration to thenext (along with the additional complication that the labelsof clusters may switch between iterations). While the con-centration parameter α may appear to offer some informa-tion, using this approach can be deceiving, since a samplerthat becomes stuck in a local mode in the clustering spacewill appear to have converged. Hence, monitoring the dis-tribution of α across multiple runs initialised with differentnumbers of clusters is advisable, but in our experience find-ing a broad enough spectrum of initialisations is not easyto determine in advance. Therefore, relying solely on α tomonitor convergence might lead to misplaced confidence.Based upon our experience with real datasets, we sug-gest that to better assess convergence, it is also important tomonitor the marginal partition posterior in each run, a cal-culation that we detail in the following section.6 David I. Hastie∗ et al.Fig. 1: Log marginal partition posterior for the first simu-lated dataset with different initial number of clusters.4.1 Marginal partition posteriorWe define the marginal partition posterior as p(Z|D). Thisquantity represents the posterior distribution of the alloca-tions given the data, having marginalised out all the otherparameters.In general computation of p(Z|D) is not possible in closedform, and requires certain assumptions and approximations.One such simplification is to fix the value of α in the calcula-tion, rather than integrating over the distribution. Typically,we advise choosing one or several values of α to conditionon, based on experimental runs on the dataset under studywith α allowed to vary.With the value of α fixed, whether or not p(Z|D) can becomputed directly depends upon whether conjugate priorsare adopted for all other parameter that must be integratedout. For the example of profile regression with logistic linkintroduced above this is typically not possible, as there is nonatural conjugate for this response model. In such cases, in-tegrating out such variables can be achieved using Laplaceapproximations. Using such an approximation appears to besufficient for discerning differences between runs that per-haps indicate convergence problems. Details on the compu-tations of p(Z|D) can be found in the Supplementary Mate-rial.Figure 1 demonstrates that the strong signal in our firstsimulated dataset means that the sampler converges regard-less of the initial number of clusters. In contrast, Section 6(Figure 3) demonstrates that for our real dataset convergenceis not always achieved.Computing the marginal partition posterior for each runof the MCMC and comparing between runs has proven tobe a very effective tool for our real examples, particularlyto identify runs that were significantly different from others,perhaps due to convergence issues.Whereas comparing the marginal distribution of a pa-rameter such as α between MCMC runs might help diag-nose non-convergence if used with a wide range of initiali-sations, it gives no indication of which run has explored theregions of higher posterior probability. On the other hand,comparing the marginal partition posterior between two dif-fering runs immediately indicates which run explored thehigher posterior probability regions. This means that evenif we are not able to make fully Bayesian inference aboutthe parameters, we are able to draw some conclusions aboutthose parameters which are more likely.5 Our implementation of a DPMM samplerTo demonstrate the behaviour discussed within this paper,we have used our own implementation of a Gibbs sampler(with Metropolis-within-Gibbs steps) for the FSBDPMM.The core of the sampler is implemented as efficient C++code, interfaced through the PReMiuM R package (Liveraniet al, 2013).The sampler was originally written specifically for anal-ysis of profile regression problems (as presented in Section2.2) across a variety of applications. For such models, thepackage includes Bernoulli, Binomial, Poisson, Normal andcategorical response models, as well as Normal and dis-crete covariates. It is also possible to run the sampler withno response model, allowing the consideration of more tra-ditional mixture models. Additionally, the sampler imple-ments a type of variable selection, allowing inference to bemade in the case of data where the clustering might be de-termined with reference to only a subset of covariates Thistype of problem is discussed in detail by Papathomas et al(2012).Extensive details of the algorithm can be found in (Liv-erani et al, 2013), including the blocking strategy that is in-tegral for allowing sampling from the FSBDPMM. We notesome brief details that are relevant to the current work be-low.5.1 Post processingAn optimal partition Given a sample of partitions from theposterior distribution of a Bayesian cluster model (for exam-ple from a DPMM sampler where the sample is the outputof an MCMC algorithm) it is often desirable to summarisethe sample as a single representative clustering estimate. TheSampling from Dirichlet process mixture models with unknown concentration parameter: Mixing issues in large data implementations 7benefits of having a single estimate of the partition often suf-ficiently outweigh the fact that the uncertainty of the cluster-ing is lost by such a point estimate, although it should alwaysbe communicated that this uncertainty may be considerable.One benefit of using an optimal partition is that ques-tions of how to account for unambiguous labelling of clus-ters between MCMC sweeps can be avoided, which wouldnot be the case if we wished to provide certain kinds of dis-tributional summary of the partition space. We emphasisethat the term label-switching is often used in this contextto refer to the complicating impact on inference of not hav-ing ways of “tracking” clusters between iterations. This isin contrast to the deliberate label-switching moves as intro-duced in Section 3.2 which use label-switching as a tech-nique to better explore partition space and avoid undue influ-ence of the ordering. Note that our inferential methods (e.g.determining an optimal partition or the predictive methoddescribed in the following section) are not affected by label-switching.There are many different ways to determine a point es-timate of the partition, for example something as simple asthe maximum a posteriori (MAP) estimate (the partition inthe sample with the highest value of the marginal partitionposterior). We prefer methods based on the the construction(as a post-processing step) of a posterior similarity matrix, amatrix containing the posterior probabilities (estimated em-pirically from the MCMC run) that the observations i andj are in the same cluster. The idea is then to find a par-tition which maximises the sum of the pairwise similari-ties. We find that methods based on the posterior similaritymatrix are less susceptible to Monte Carlo error, especiallywhen the optimal partition is not constrained to be in sample,but might be obtained using additional clustering methods,such as partitioning around medoids, that take advantage ofthe whole MCMC output. Note that once a representativepartition is chosen, full uncertainty about its characteristicfeatures can be recovered from postprocessing of the fullMCMC output. See (Molitor et al, 2010) for a full discus-sion.Making predictions While an optimal partition can be veryhelpful in some cases (particularly when it is the cluster-ing itself that is the primary object of inference) difficultiesare faced in understanding or conveying the uncertainty ofthe partitioning. Due to the complexity and sheer size of themodel space, the optimal partitions tend to differ betweenruns of the MCMC, and it is not an easy task to assesswhether convergence has been achieved based on this ap-proach alone.A common target of inference is not necessarily the par-tition itself, but how the estimated parameters might allow usto make predictions for future observations. For example wemight want to group new observations with existing obser-vations, or, in the case of profile regression, make a predic-tion about the response if only the covariates of a new obser-vation had been observed. One way to do this is to use poste-rior predictions, where posterior predictive distributions forquantities of interest can be derived from the whole MCMCrun, taking the uncertainty over clustering into account.Depending on the quantity of interest, the posterior pre-dictive distribution can often be relatively robust even acrossruns with noticeably different optimal partitions. While thismay not help us to determine if the algorithm has suffi-ciently explored the partition-space, if the purpose of theinference is to make predictions, this robustness can be reas-suring. Moreover, by allowing predicted values to be com-puted based on probabilistic allocations (i.e. using a Rao-Blackwellised estimate of predictions) results can be furtherdesensitised to a single best partition.6 Investigation of the algorithm’s properties in a largedata applicationIn this section, we report the results of using our FSBDPMMsampler in a profile regression application with discrete co-variates and a binary response, applied to a real epidemio-logical dataset with 2,639 subjects.The analysis of real data presents a number of challenges:it requires care in ensuring convergence, as the signal isnot as strong as in a simulation study. However, these arechallenges that might be encountered more widely by userswishing to apply the methods to real data, and by presentingan example it allows us to highlight and discuss the issuesthat arise.6.1 The dataOur dataset is a subset taken from an epidemiological case-control study, the analysis of which has provided the moti-vation of most of the work presented in this paper (see ?, ?).In the illustrative example we have 2,639 subjects, and use6 discrete covariates each with 5 categories, and 13 fixedeffects. The response is binary and we use the model speci-fications detailed in Section 2.2 to analyse this data set. Thelow signal contained in the data poses issues with conver-gence of the MCMC, as we illustrate below.Our results are based upon running multiple chains eachfor 100,000 iterations after a burn-in sample of 50,000 itera-tions. In some cases, behaviour within this burn-in period isillustrated.6.2 ResultsMarginal partition posterior and number of clusters As dis-cussed in Section 3 we run multiple MCMC runs, starting8 David I. Hastie∗ et al.Fig. 2: Log marginal partition posterior for the real epidemi-ological dataset with different initial number of clusters.each with very different numbers of initial clusters. For thisdataset, initialising the sampler with fewer than 20 clustersresults in marginal partition posterior distributions that aresignificantly different between runs. This is illustrated inFigure 2, where initialisations with small number of clustersresult in much lower marginal partition posterior values thancan be achieved with a higher initial number of clusters. It isapparent that there is a cut-off at 20 clusters, where increas-ing the number of initial clusters further does not result in anincrease in the marginal partition posterior, suggesting thatwith 20 clusters or more the sampler is able to visit areas ofthe model space with the highest posterior support.Posterior distribution ofα Figure 3 shows the boxplot of theposterior distribution of α as a function of the initial num-ber of clusters. For each different initial number of clusters,three different runs with random initialisations of other pa-rameters were performed. We can see that the posterior dis-tribution of α only stabilises when the initial number of clus-ters is high, around 50 in our case. Thus, we would recom-mend carrying out such checks as part of the investigation ofconvergence strategy. Note that while it is advisable to startwith a large number of initial clusters, starting with manymore clusters than necessary can result in a larger numberof iterations required for convergence.Posterior distribution of the number of clusters The need toinitialise the sampler with a sufficiently high number of clus-ters is also supported by looking at the posterior distributionof the number of clusters. Figure 4 contrasts the behaviourof the sampler between the first 500 iterations of the burn in012345Initial number of clustersPosterior distribution of alpha1 10 30 50012345012345012345012345012345012345012345012345012345012345012345012345Fig. 3: Posterior distribution of α for different number ofinitial clusters with three repetitions per initialisation: box-plots for the distribution for 50,000 sweeps after a burn-inof 50,000 samples.period and 500 iterations after the first 15,000, for a run with31 initial clusters.In the initial iterations, the space is explored by modify-ing and merging clusters, with the number of clusters chang-ing frequently, in a general downward trend. On the otherhand, once the MCMC has converged to the model spacearound a mode, the algorithm attempts to split clusters reg-ularly, but the number of changes in the number of clustersare few, and increases in the number of clusters are almostimmediately reversed in the following iteration.The posterior distributions for the number of clusters isshown in Figure 5 for runs with different initial numbers ofclusters. Five chains have been ran, initialised with 1, 5, 10,30 and 50 clusters respectively. The size and shading of eachcircle in Figure 5 represents the posterior frequency of thenumber of clusters for each of the chains. As can be seenfrom this figure, with 30 or more initial clusters the samplerhas converged to a common area of posterior support, butwith fewer than this the sampler might not visit this regionof the model space, despite it having increased posterior sup-port. Taken together, the plots in Figures2, 3 and 5 provideconcurring evidence that for our real data case, starting with50 or more clusters leads to reproducible conclusions.Label-switching moves This example also demonstrates theneed for the new label-switching move discussed in Sec-tion 3.2 to ensure good mixing. Figure 6 demonstrates thedecrease in acceptance rate that is evidenced for the label-switching moves, if only the moves that PapaspiliopoulosSampling from Dirichlet process mixture models with unknown concentration parameter: Mixing issues in large data implementations 9SweepsDraws from the distribution of number of clusters1620253035401 200 400 14600 14800 15000Fig. 4: The trace of the posterior of the number of clustersfor the first 500 iterations and after 15,000 iterations of theMCMC sampler.81012141618Initial number of clustersPosterior of number of clusters1 5 10 30 50llllllllllllllllllllFig. 5: The posterior distribution of the number of clustersfor 50,000 sweeps after a burn-in of 50,000 iterations.and Roberts (2008) propose are included. For the first of themoves that Papaspiliopoulos and Roberts (2008) propose,where the labels of two randomly selected clusters are ex-changed, we observed acceptance rates below 10% for anysample of 500 sweeps. For the second of the moves, wherethe labels of two neighbouring clusters are swapped, alongwith the corresponding Vc, Vc+1 the acceptance rate drops2000 4000 6000 8000 100000.00.20.40.60.81.0Acceptance Rate for move 1, with moves 1−2 implementedSweepsAcceptance Rate2000 4000 6000 8000 100000.00.20.40.60.81.0Acceptance Rate for move 2, with moves 1−2 implementedSweepsAcceptance Rate0 2000 4000 6000 8000 10000051015Samples from posterior distribution of alphaSweepsPosterior alpha distributionFig. 6: Acceptance rate for intervals of 500 sweeps for thetwo label-switching moves proposed by Papaspiliopoulosand Roberts (2008) and comparison with samples from theposterior distribution of α (bottom).considerably after initially being very high. This decreasecan be explained by the observation (made by the originalauthors) that the second move type is always accepted ifone of the clusters is empty, which can happen often in ini-tial cluster orderings with low posterior support. Note thatα stabilises after 5,000 iterations for the example shown. Ifonly the first of the two moves is implemented, α moves ex-tremely slowly (more than 50,000 iterations are not enoughto have a stable trace; not shown) while if only the secondof the two moves is implemented, for this example, 17,000iterations are necessary for α to stabilise (not shown).Comparing Figure 7 to Figure 6, we can see that the newlabel-switching move suffers from no drop off in acceptanceat any point throughout the run. Figure 8 shows the accep-tance rate for our new label-switching move, when the othertwo switching label is not implemented. While the perfor-mance is worse than using all three moves, it is the mosteffective single label-switching move (see Section 3.2).10 David I. Hastie∗ et al.2000 4000 6000 8000 100000.00.20.40.60.81.0Acceptance rate for move 1, with moves 1−3 implementedSweepsAcceptance Rate2000 4000 6000 8000 100000.00.20.40.60.81.0Acceptance rate for move 2, with moves 1−3 implementedSweepsAcceptance Rate2000 4000 6000 8000 100000.00.20.40.60.81.0Acceptance rate for move 3, with moves 1−3 implementedSweepsAcceptance Rate0 2000 4000 6000 8000 10000051015Samples from posterior distribution of alphaSweepsPosterior alpha distributionFig. 7: Acceptance rates with the new label-switching move.To further assess how the new label-switching move af-fects mixing and the ability to recover the posterior distri-bution of α, we used our second simulated dataset. Start-ing with 100 clusters, we performed 10 runs of the samplerusing only moves 1 and 2 for label-switching, and 10 runsadding in our third label-switching move. In each case weran the chain for 100,000 iterations after a burn-in sampleof 100,000 iterations. Figure 9 shows the performance ofthe sampler in retrieving the distribution of α that was usedto simulate the data with and without using our new label-switching move. It is clear that this distribution is not wellrecovered when using exclusively moves 1 and 2, while the2000 4000 6000 8000 100000.9920.9961.000Acceptance Rate for move 3, with only move 3 implementedSweepsAcceptance Rate0 2000 4000 6000 8000 10000051015Samples from posterior distribution of alphaSweepsPosterior alpha distributionFig. 8: Acceptance rates for the new label-switching move.0 5 10 150.00.10.20.30.40.5DensityGenerating alphaMoves 1−2−3Moves 1−2Fig. 9: Recovered posterior density of α from multipleMCMC runs with and without the new label-switching movecompared with generating density of α for the second simu-lated dataset.addition of our third label-switching move is clearly benefi-cial.7 ConclusionsBy demonstrating some of the challenges that occur whensampling from the DPMM, we hope to have raised aware-Sampling from Dirichlet process mixture models with unknown concentration parameter: Mixing issues in large data implementations 11ness that continued research into the DPMM sampling method-ology is required. Our implementation of a FSBDPMM sam-pler, synthesises many of the most recent and innovativetechniques introduced by other authors, such as parameterblocking, slice sampling, and label-switching. However, dueto the complex model space that is inherent with the FSB-DPMM, many issues persist.In previous work by other authors, considerable progresshas been made evolving the samplers through innovativestrategies and approaches. Nonetheless, discussion of manyof the residual difficulties is avoided through demonstrat-ing the methods only on simulated data, or for datasets withstrong signal. In practice however, with real datasets, theuser does not have the option of simply avoiding these is-sues, as illustrated by our analysis of the mixing perfor-mance of an epidemiological data set with low signal tonoise ratio.In this paper we have attempted to highlight the difficul-ties that a user may face in practice. We have added a newfeature in the form of an additional label-switching move tobuild upon this previous research and further alleviate someof the challenges that are involved when trying to samplesuch a complex posterior space. We have also provided prac-tical guidelines based on our experience, on how to makeuseful inference in the face of these limitations.As a consequence of discussing these challenges explic-itly, we hope that our work will motivate further develop-ments in this area to take additional steps to improve sam-pler efficiency. The challenge of designing MCMC movesthat are able to escape local well-separated modes is consid-erable, but equally, so is the imagination and innovation ofmany researchers developing new MCMC sampling method-ologies. Encouragingly research continues, and drawing onalternative techniques which might be better designed formulti-modality, such as sequential Monte Carlo (see for ex-ample Ulker et al, 2011) may yield further improvements.In the meantime, practitioners may benefit from observ-ing the difficulties we have presented here, allowing themto recognise and communicate potential limitations of theiranalyses.AcknowledgementsDavid I. Hastie acknowledges support from the INSERMgrant (P27664). Silvia Liverani acknowledges support fromthe Leverhulme Trust (ECF-2011-576). Sylvia Richardsonacknowledges support from MRC grant G1002319. We aregrateful for helpful discussions with Sara K. Wade and LamiaeAzizi, and to Isabelle Stu¨cker for providing the epidemio-logical data. We would like to the thank the editor and re-viewers for their helpful comments that have allowed us toimprove this paper.A AppendicesA.1We provide the following proposition concerning the relationship be-tween the ordering and α.Proposition 1 Suppose that we have a model with posterior as givenin Equation 1. Then P(ψc > ψc+1|α) is a function of α, and further-more P(ψc > ψc+1) > 0.5.Proof If ψc > ψc+1 then Vc > Vc+1(1−Vc), which implies Vc+1 <Vc/(1− Vc). ThusP( ψc > ψc+1|α) = P(Vc+1 < Vc/(1− Vc)|α)=∫ 0.50∫ V1/(1−V1)0α2(1− V1)α−1(1− V2)α−1dV2dV1+∫ 10.5∫ 10α2(1− V1)α−1(1− V2)α−1dV2dV1=∫ 0.50[α(1− V1)α−1 − α(1− V1)α−1(1− 2V11− V1)α]dV1+∫ 10.5α(1− V1)α−1dV1=∫ 10α(1− V1)α−1dV1−∫ 0.50α(1− 2V1)α1− V1dV1= 1−∫ 0.50α(1− 2V1)α1− V1dV1.Now since, (1− 2V1)α/(1− V1) < (1− 2V1)α−1α∫ 0.50(1− 2V1)α1− V1dV1 < α∫ 0.50(1− 2V1)α−1dV1 = 0.5.So P(ψc > ψc+1|α) > 0.5 for all α. Finally,P(ψc > ψc+1) =∫P(ψc > ψc+1|α)p(α)dα>∫0.5p(α)dα = 0.5.A.2Proposition 2 Consider the label-switching move defined in Equa-tions 4 to 7 in Section 3.2. Then:(i) (ψ+)′ := ψ′c + ψ′c+1 = ψc + ψc+1 = ψ+;(ii) (1− V ′c )(1− V ′c+1) = (1− Vc)(1− Vc+1);(iii) The proposal mechanism is its own reverse;(iv)E(ψc|Z′, α)E(ψc+1|Z, α)=1 + α+ nc+1 +∑l>c+1 nlα+ nc+1 +∑l>c+1 nlandE(ψc+1|Z′, α)E(ψc|Z, α)=α+ nc +∑l>c+1 nl1 + α+ nc +∑l>c+1 nl; and(v) the acceptance probability for this move is given by min{1, R},where the acceptance ratio R is given in Equation 8.12 David I. Hastie∗ et al.Proof (i) By definition(ψ+)′ := ψ′c + ψ′c+1=ψ+Ψ ′(ψc+1E[ψc|Z′, α]E[ψc+1|Z, α]+ ψcE[ψc+1|Z′, α]E[ψc|Z, α])=ψ+Ψ ′Ψ ′ = ψ+;(ii) From (i),ψ′c + ψ′c+1 = ψc + ψc+1implies[V ′c + V′c+1(1− V ′c )] ∏l<c(1− V ′l )= [Vc + Vc+1(1− Vc)]∏l<c(1− Vl).By Equation 7, V ′l = Vl for all l < c,⇒ V ′c + V ′c+1(1− V ′c ) = Vc + Vc+1(1− Vc)⇒ (1− V ′c )(1− V ′c+1) = (1− Vc)(1− Vc+1).The importance of this result is that it provides confirmation thatour proposed ψ′ in Equation 6 can be achieved with the V de-fined in Equation 7. In particular, with this choice of V ′, the onlyweights that are changed are those associated with components cand c+ 1, as desired.(iii) Suppose that the Markov chain is currently in the proposed statedefined in Equations 4 to 7 i.e. (V ′,Θ′,Z′,U , α, Λ). We showthat applying the proposal mechanism to this state, for componentc and c+ 1, the proposed new state is the original state(V ′′,Θ′′,Z′′,U , α, Λ) = (V ,Θ,Z,U , α, Λ.)The parameters U , α and Λ are unchanged by design of the pro-posal mechanism. Also, by design, the allocations Z and clus-ter parameters Θ are simply swapped for the selected compo-nents, so trivially Z′′ = Z and Θ′′ = Θ. Since V ′′l is un-changed for l 6∈ {c, c + 1}, it remains only to show V ′′c = Vcand V ′′c+1 = Vc+1, or equivalently ψ′′c = ψc and ψ′′c+1 = ψc+1.To confirm,ψ′′c = ψ′c+1(ψ+)′Ψ ′′E[ψc|Z′′]E[ψc+1|Z′, α]= ψcψ+Ψ ′′ψ+Ψ ′E[ψc+1|Z′, α]E[ψc|Z, α]E[ψc|Z′′, α]E[ψc+1|Z′, α](by (i) and Equation 6) (11)= ψc(ψ+)2Ψ ′′Ψ ′since Z′′ = Z. (12)However,Ψ ′′ = ψ′c+1E[ψc|Z′′]E[ψc+1|Z′, α]+ ψ′cE[ψc+1|Z′′, α]E[ψc|Z′, α]=ψ+Ψ ′(ψc + ψc+1)(from Equation 6 and since Z′′ = Z)=(ψ+)2Ψ ′.Substituting this into Equation 12 we get ψ′′c = ψc. The result forψ′′c+1 can be shown by simply following identical logic.(iv) From Equation 1, we haveE[ψc|Z, α] = E[Vc∏l<c(1− Vl)|Z, α]= E[Vc|Z, α]∏l<cE[(1− Vl)|Z, α]=(1 + nc1 + α+ nc +∑l>c nl)(13)×∏l<c(α+∑l′>l nl′1 + α+ nl +∑l′>l nl′). (14)Similarly,E[ψc+1|Z, α] =(1 + nc+11 + α+ nc+1 +∑l>c+1 nl)×(α+∑l>c nl1 + α+ nc +∑l>c nl)(15)×∏l<c(α+∑l′>l nl′1 + α+ nl +∑l′>l nl′).By definition of Z′ in Equation 4, we haven′l =nc+1 l = cnc l = c+ 1nl otherwise.(16)This means from Equations 13 and 15 we haveE[ψc|Z′, α]E[ψc+1|Z, α]=(1 + n′c1 + α+ n′c + n′c+1 +∑l>c+1 nl)×(1 + α+ nc+1 +∑l>c+1 nl1 + nc+1)(17)×(1 + α+ nc + nc+1 +∑l>c+1 nlα+ nc+1 +∑l>c+1 nl)Substituting Equation 16 into 17 and simplifying gives the desiredresults. The result for E[ψc+1|Z′,α]E[ψc|Z,α] follows in the same fashion.(v) By (iii) and the deterministic nature of the proposal mechanism,the only random feature of the proposal is the choice of componentc. The probability of this choice is the same for the move and itsreverse and so cancels. Therefore the only contribution to the ac-ceptance ratio is the ratio of posteriors. By design, the likelihoodis unchanged, and by (ii) the only change in posterior is down tothe change in weights of components c and c + 1. Therefore wehave,R =(ψ′c)n′c(ψ′c+1)n′c+1ψncc ψnc+1c+1(18)=(ψ′c+1ψc)nc ( ψ′cψc+1)nc+1by Equation 16. (19)Substituting in Equation 6 and the results in (iv), we obtain thedesired acceptance ratio.ReferencesAntoniak CE (1974) Mixtures of Dirichlet processes withapplications to Bayesian nonparametric problems. Annalsof Statistics 2(6):1152–1174Bigelow JL, Dunson DB (2009) Bayesian SemiparametricJoint Models for Functional Predictors. Journal of theAmerican Statistical Association 104(485):26–36Blackwell D, MacQueen JB (1973) Ferguson distributionsvia Polya Urn Schemes. Annals of Statistics 1(2):353–355Dunson DB (2009) Nonparametric Bayes local partitionmodels for random effects. Biometrika 96(2):249–262Dunson DB, Herring AB, Siega-Riz AM (2008) BayesianInference on Changes in Response Densities Over Pre-dictor Clusters. Journal of the American Statistical Asso-ciation 103(484):1508–1517Sampling from Dirichlet process mixture models with unknown concentration parameter: Mixing issues in large data implementations 13Escobar MD, West M (1995) Bayesian density estimationand inference using mixtures. Journal of the AmericanStatistical Association 90(430):577– 588Ferguson TS (1973) A Bayesian analysis of some nonpara-metric problems. Annals of Statistics 1(2):209–230Ishwaran H, James LF (2001) Gibbs sampling methods forstick-breaking priors. Journal of the American StatisticalAssociation 96(453):161–173Jain S, Neal RM (2004) A split-merge Markov chainMonte Carlo procedure for the Dirichlet process mixturemodel. Journal of Computational and Graphical Statistics13:158–182Jain S, Neal RM (2007) Splitting and Merging Componentsof a Nonconjugate Dirichlet Process Mixture Model.Bayesian Analysis 2(3):445–472Jasra A, Holmes CC, Stephens DA (2005) Markov chainMonte Carlo methods and the label switching problem inBayesian mixture modeling. Statistical Science 20(1):50–67Kalli M, Griffin JE, Walker SG (2011) Slice sampling mix-ture models. Statistics and Computing 21(1):93–105Liverani S, Hastie DI, Richardson S (2013) PReMiuM: AnR Package for Profile Regression Mixture Models usingDirichlet Processes, preprint available at arXiv:1303.2836Molitor J, Papathomas M, Jerrett M, Richardson S (2010)Bayesian profile regression with an application tothe National Survey of Children’s Health. Biostatistics11(3):484–498Molitor J, Su JG, Molitor NT, Rubio VG, Richardson S,Hastie D, Morello-Frosch R, Jerrett M (2011) Identify-ing vulnerable populations through an examination of theassociation between multipollutant profiles and poverty.Environmental Science & Technology 45(18):7754–7760Neal RM (2000) Markov chain sampling methods forDirichlet process mixture models. Journal of Computa-tional and Graphical Statistics 9(2):249Papaspiliopoulos O (2008) A note on posterior samplingfrom Dirichlet mixture models. Tech. Rep. 8, CRISM Pa-perPapaspiliopoulos O, Roberts GO (2008) RetrospectiveMarkov chain Monte Carlo methods for Dirichlet processhierarchical models. Biometrika 95(1):169–186Papathomas M, Molitor J, Richardson S, Riboli E, Vineis P(2011) Examining the joint effect of multiple risk factorsusing exposure risk profiles: lung cancer in non-smokers.Environmental Health Perspectives 119:84–91Papathomas M, Molitor J, Hoggart C, Hastie DI, RichardsonS (2012) Exploring data from genetic association studiesusing Bayesian variable selection and the Dirichlet pro-cess : application to searching for gene × gene patterns.Genetic Epidemiology 6(36):663–74Pitman J, Yor M (1997) The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator.Annals of Probability 25(2):855–900Porteous I, Ihler A, Smyth P, Welling M (2006) Gibbs sam-pling for (coupled) infinite mixture models in the stickbreaking representation. In: Proceedings of the 22nd An-nual Conference on Uncertainty in Artificial Intelligence(UAI-06), AUAI Press, Arlington, VARichardson S, Green PJ (1997) On Bayesian analysis ofmixtures with an unknown number of components. Jour-nal of the Royal Statistical Society, Series B (Method-ological) 59(4):731–792Sethuraman J (1994) A constructive definition of Dirichletpriors. Statistica Sinica 4:639–650Ulker Y, Gunsel B, Cegil AT (2011) Annealed SMC Sam-plers for Nonparametric Bayesian Mixture Models. IEEESignal Processing Letters 18:3–6Walker SG (2007) Sampling the Dirichlet mixture modelwith slices. Communications in Statistics - Simulationand Computation 36:45–54Yau C, Papaspiliopoulos O, Roberts GO, Holmes C (2011)Bayesian non-parametric hidden Markov models with ap-plications in genomics. Journal of the Royal StatisticalSociety, Series B (Statistical Methodology) 73:37–57",
    "id": 29139326,
    "identifiers": {
        "doi": "10.1007/s11222-014-9471-3",
        "oai": "oai:bura.brunel.ac.uk:2438/9505"
    },
    "title": "Sampling from Dirichlet process mixture models with unknown concentration parameter: mixing issues in large data implementations",
    "language": {
        "code": "en",
        "name": "English"
    },
    "publishedDate": "2014-05-03T00:00:00+01:00",
    "publisher": "Springer US",
    "references": [
        {
            "id": 36998286,
            "title": "Exploring data from genetic association studies using Bayesian variable selection and the Dirichlet process : application to searching for gene  gene patterns.",
            "authors": [],
            "date": "2012",
            "doi": null,
            "raw": "Environmental Health Perspectives 119:84–91 Papathomas M, Molitor J, Hoggart C, Hastie DI, Richardson S (2012) Exploring data from genetic association studies using Bayesian variable selection and the Dirichlet process : application to searching for gene  gene patterns.",
            "cites": null
        },
        {
            "id": 36998290,
            "title": "Gibbs sampling for (coupled) inﬁnite mixture models in the stick breaking representation. In:",
            "authors": [],
            "date": "2006",
            "doi": null,
            "raw": "Annals of Probability 25(2):855–900 Porteous I, Ihler A, Smyth P, Welling M (2006) Gibbs sampling for (coupled) inﬁnite mixture models in the stick breaking representation. In: Proceedings of the 22nd Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-06), AUAI Press, Arlington, VA Richardson S, Green PJ (1997) On Bayesian analysis of mixtures with an unknown number of components. Journal of the Royal Statistical Society, Series B (Methodological) 59(4):731–792 Sethuraman J (1994) A constructive deﬁnition of Dirichlet priors. Statistica Sinica 4:639–650 Ulker Y, Gunsel B, Cegil AT (2011) Annealed SMC Samplers for Nonparametric Bayesian Mixture Models. IEEE Signal Processing Letters 18:3–6 Walker SG (2007) Sampling the Dirichlet mixture model with slices. Communications in Statistics - Simulation and Computation 36:45–54 Yau C, Papaspiliopoulos O, Roberts GO, Holmes C (2011) Bayesian non-parametric hidden Markov models with applications in genomics. Journal of the Royal Statistical Society, Series B (Statistical Methodology) 73:37–57",
            "cites": null
        },
        {
            "id": 36998279,
            "title": "Markov chain Monte Carlo methods and the label switching problem in Bayesian mixture modeling.",
            "authors": [],
            "date": "2005",
            "doi": null,
            "raw": "Bayesian Analysis 2(3):445–472 Jasra A, Holmes CC, Stephens DA (2005) Markov chain Monte Carlo methods and the label switching problem in Bayesian mixture modeling. Statistical Science 20(1):50– Kalli M, Grifﬁn JE, Walker SG (2011) Slice sampling mixture models. Statistics and Computing 21(1):93–105 Liverani S, Hastie DI, Richardson S (2013) PReMiuM: An R Package for Proﬁle Regression Mixture Models using DirichletProcesses,preprintavailableatarXiv:1303.2836 Molitor J, Papathomas M, Jerrett M, Richardson S (2010) Bayesian proﬁle regression with an application to the National Survey of Children’s Health. Biostatistics 11(3):484–498 Molitor J, Su JG, Molitor NT, Rubio VG, Richardson S, Hastie D, Morello-Frosch R, Jerrett M (2011) Identifying vulnerable populations through an examination of the association between multipollutant proﬁles and poverty.",
            "cites": null
        },
        {
            "id": 36998280,
            "title": "Markov chain sampling methods for Dirichlet process mixture models.",
            "authors": [],
            "date": "2000",
            "doi": null,
            "raw": "Environmental Science & Technology 45(18):7754–7760 Neal RM (2000) Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics 9(2):249 Papaspiliopoulos O (2008) A note on posterior sampling from Dirichlet mixture models. Tech. Rep. 8, CRISM Paper Papaspiliopoulos O, Roberts GO (2008) Retrospective Markov chain Monte Carlo methods for Dirichlet process hierarchical models. Biometrika 95(1):169–186 Papathomas M, Molitor J, Richardson S, Riboli E, Vineis P (2011) Examining the joint effect of multiple risk factors using exposure risk proﬁles: lung cancer in non-smokers.",
            "cites": null
        },
        {
            "id": 36998278,
            "title": "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems.",
            "authors": [],
            "date": "1974",
            "doi": null,
            "raw": "Antoniak CE (1974) Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. Annals of Statistics 2(6):1152–1174 Bigelow JL, Dunson DB (2009) Bayesian Semiparametric Joint Models for Functional Predictors. Journal of the American Statistical Association 104(485):26–36 Blackwell D, MacQueen JB (1973) Ferguson distributions via Polya Urn Schemes. Annals of Statistics 1(2):353– Dunson DB (2009) Nonparametric Bayes local partition models for random effects. Biometrika 96(2):249–262 Dunson DB, Herring AB, Siega-Riz AM (2008) Bayesian Inference on Changes in Response Densities Over Predictor Clusters. Journal of the American Statistical Association 103(484):1508–1517Sampling from Dirichlet process mixture models with unknown concentration parameter: Mixing issues in large data implementations 13 Escobar MD, West M (1995) Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association 90(430):577– 588 Ferguson TS (1973) A Bayesian analysis of some nonparametric problems. Annals of Statistics 1(2):209–230 Ishwaran H, James LF (2001) Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association 96(453):161–173 Jain S, Neal RM (2004) A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model. Journal of Computational and Graphical Statistics 13:158–182 Jain S, Neal RM (2007) Splitting and Merging Components of a Nonconjugate Dirichlet Process Mixture Model.",
            "cites": null
        },
        {
            "id": 36998288,
            "title": "The two-parameter PoissonDirichlet distribution derived from a stable subordinator.",
            "authors": [],
            "date": "1997",
            "doi": null,
            "raw": "Genetic Epidemiology 6(36):663–74 Pitman J, Yor M (1997) The two-parameter PoissonDirichlet distribution derived from a stable subordinator.",
            "cites": null
        }
    ],
    "sourceFulltextUrls": [
        "http://bura.brunel.ac.uk/bitstream/2438/9505/2/Fullpaper.pdf"
    ],
    "updatedDate": "",
    "yearPublished": "2014",
    "links": [
        {
            "type": "download",
            "url": "https://core.ac.uk/download/29139326.pdf"
        },
        {
            "type": "reader",
            "url": "https://core.ac.uk/reader/29139326"
        },
        {
            "type": "thumbnail_m",
            "url": "https://core.ac.uk/image/29139326/medium"
        },
        {
            "type": "thumbnail_l",
            "url": "https://core.ac.uk/image/29139326/large"
        },
        {
            "type": "display",
            "url": "https://core.ac.uk/outputs/29139326"
        }
    ],
    "abstract": "We consider the question of Markov chain Monte Carlo sampling from a general stick-breaking Dirichlet process mixture model, with concentration parameter (Formula presented.). This paper introduces a Gibbs sampling algorithm that combines the slice sampling approach of Walker (Communications in Statistics - Simulation and Computation 36:45-54, 2007) and the retrospective sampling approach of Papaspiliopoulos and Roberts (Biometrika 95(1):169-186, 2008). Our general algorithm is implemented as efficient open source C++ software, available as an R package, and is based on a blocking strategy similar to that suggested by Papaspiliopoulos (A note on posterior sampling from Dirichlet mixture models, 2008) and implemented by Yau et al. (Journal of the Royal Statistical Society, Series B (Statistical Methodology) 73:37-57, 2011). We discuss the difficulties of achieving good mixing in MCMC samplers of this nature in large data sets and investigate sensitivity to initialisation. We additionally consider the challenges when an additional layer of hierarchy is added such that joint inference is to be made on (Formula presented.). We introduce a new label-switching move and compute the marginal partition posterior to help to surmount these difficulties. Our work is illustrated using a profile regression (Molitor et al. Biostatistics 11(3):484-498, 2010) application, where we demonstrate good mixing behaviour for both synthetic and real examples. © 2014 The Author(s)",
    "tags": [
        "Article",
        "Bayesian clustering",
        "Dirichlet process",
        "Mixture model",
        "Profile regression"
    ],
    "fulltextStatus": "enabled",
    "subjects": [
        "Article"
    ],
    "oai": "oai:bura.brunel.ac.uk:2438/9505",
    "deleted": "ALLOWED",
    "disabled": false,
    "journals": [
        {
            "title": "Statistics and Computing",
            "identifiers": [
                "0960-3174",
                "issn:0960-3174"
            ]
        }
    ],
    "repositories": {
        "id": "14",
        "openDoarId": 0,
        "name": "Brunel University Research Archive",
        "urlHomepage": null,
        "uriJournals": null,
        "physicalName": "noname",
        "roarId": 0,
        "baseId": 0,
        "pdfStatus": null,
        "nrUpdates": 0,
        "lastUpdateTime": null
    },
    "repositoryDocument": {
        "id": 29139326,
        "depositedDate": "2014-05-03T00:00:00+01:00",
        "publishedDate": "2014-05-03T00:00:00+01:00",
        "updatedDate": "2024-02-19T04:42:02+00:00",
        "acceptedDate": "2014-05-03T00:00:00+01:00",
        "createdDate": "2015-05-18T18:12:54+01:00"
    },
    "urls": [
        "http://dx.doi.org/10.1007/s11222-014-9471-3",
        "http://bura.brunel.ac.uk/handle/2438/9505",
        "http://link.springer.com/article/10.1007%2Fs11222-014-9471-3"
    ],
    "lastUpdate": "2024-02-19T04:42:02+00:00",
    "setSpecs": []
}