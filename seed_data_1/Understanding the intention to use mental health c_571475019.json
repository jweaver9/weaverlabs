{
    "acceptedDate": "",
    "authors": [
        {
            "name": "Henkel, T."
        },
        {
            "name": "Linn, A.J."
        },
        {
            "name": "van der Goot, M.J."
        }
    ],
    "contributors": [
        "Følstad, A.",
        "Araujo, T.",
        "Papadopoulos, S.",
        "Law, E.L.-C.",
        "Luger, E.",
        "Goodwin, M.",
        "Brandtzaeg, P.B."
    ],
    "createdDate": "2023-07-08T23:21:47+01:00",
    "dataProvider": {
        "id": 15629,
        "name": "International Migration, Integration and Social Cohesion online publications",
        "url": "https://api.core.ac.uk/v3/data-providers/15629",
        "logo": "https://api.core.ac.uk/data-providers/15629/logo"
    },
    "depositedDate": "",
    "documentType": "",
    "doi": "10.1007/978-3-031-25581-6_6",
    "downloadUrl": "https://core.ac.uk/download/571475019.pdf",
    "fullText": "UvA-DARE is a service provided by the library of the University of Amsterdam (https://dare.uva.nl)UvA-DARE (Digital Academic Repository)Understanding the intention to use mental health chatbots among LGBTQIA+individualsTesting and extending the UTAUTHenkel, T.; Linn, A.J.; van der Goot, M.J.DOI10.1007/978-3-031-25581-6_6Publication date2023Document VersionFinal published versionPublished inChatbot Research and DesignLicenseArticle 25fa Dutch Copyright Act (https://www.openaccess.nl/en/in-the-netherlands/you-share-we-take-care)Link to publicationCitation for published version (APA):Henkel, T., Linn, A. J., & van der Goot, M. J. (2023). Understanding the intention to usemental health chatbots among LGBTQIA+ individuals: Testing and extending the UTAUT. InA. Følstad, T. Araujo, S. Papadopoulos, EL-C. Law, E. Luger, M. Goodwin, & P. B.Brandtzaeg (Eds.), Chatbot Research and Design: 6th International Workshop,CONVERSATIONS 2022, Amsterdam, The Netherlands, November 22–23, 2022 revisedselected papers (pp. 83-100). (Lecture Notes in Computer Science; Vol. 13815). Springer.https://doi.org/10.1007/978-3-031-25581-6_6General rightsIt is not permitted to download or to forward/distribute the text or part of it without the consent of the author(s)and/or copyright holder(s), other than for strictly personal, individual use, unless the work is under an opencontent license (like Creative Commons).Disclaimer/Complaints regulationsIf you believe that digital publication of certain material infringes any of your rights or (privacy) interests, pleaselet the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the materialinaccessible and/or remove it from the website. Please Ask the Library: https://uba.uva.nl/en/contact, or a letterto: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. Youwill be contacted as soon as possible.Download date:31 Aug 2023Understanding the Intention to Use MentalHealth Chatbots Among LGBTQIA+Individuals: Testing and Extending the UTAUTTanja Henkel(B) , Annemiek J. Linn , and Margot J. van der GootAmsterdam School of Communication Research (ASCoR), Nieuwe Achtergracht 166, 1018 WVAmsterdam, The Netherlandst.henkel@uva.nlAbstract. This empirical study aims to test and extend the unified theory of accep-tance and use of technology (UTAUT) in the context of mental health chatbotusage among LGBTQIA+ individuals. The proposed model uses UTAUT vari-ables (performance expectancy, effort expectancy and social influence) as wellas chatbot-related variables (willingness to self-disclose, perceived loss of pri-vacy, and trust) to predict the intention to use a mental health chatbot. The onlinesurvey (N = 305) indicates that performance expectancy, social influence, andwillingness to self-disclose positively predict chatbot usage intention, whereaseffort expectancy negatively influences this intention. Moreover, previous experi-ence with healthcare chatbots moderated the relationship between social influenceand intention, age moderated the relationship between willingness to self-discloseand intention, and gender identity moderated the relationship between perceivedloss of privacy and intention. Overall, the extended UTAUT proved to be use-ful in explaining technology acceptance of mental health chatbots among theLGBTQIA+ community.Keywords: Technology acceptance ·Mental health chatbots · UTAUT ·LGBTQIA+ community1 IntroductionMental health chatbots —empathic agents using natural language processing (NLP) todetect and reframe cognitive patterns of users [23]— offer great potential for individualswho suffer from mental health issues but lack access to treatments or are ashamed oftheir problems [1]. This is because chatbots are always available, easily accessible,cost-effective, offer a non-judgmental space and show both infinite patience as well asimmediate feedback [15]. First studies testing applications such asWysa [64] orWoebot[63] show promising results regarding the effectiveness of mental health chatbots inreducing feelings of stress [38], anxiety [21] and depression [20].A widely used model to predict people’s intention to use technology is the unifiedtheory of acceptance and use of technology (UTAUT) [60, 61]. This model combinesseveral variables derived from the technology acceptance model [17] and the theory of© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023A. Følstad et al. (Eds.): CONVERSATIONS 2022, LNCS 13815, pp. 83–100, 2023.https://doi.org/10.1007/978-3-031-25581-6_684 T. Henkel et al.planned behavior [2] and has been used and adopted in numerous contexts [11, 45, 56,62, 66]. There are at least two current-day trends that the UTAUT needs to be adaptedto. First, the traditional UTAUT cannot fully explain the intention to use mental healthchatbots as it neglects crucial chatbot-specific aspects like privacy, trust, and individ-uals’ willingness to self-disclose to a chatbot. Second, research explaining technologyacceptance has traditionally included gender as a dichotomous variable, whereas wenow live in a society where boundaries are increasingly blurred between male, female,non-binary, transgender, and genderfluid identities [8, 9, 12, 13]. Thus, models shouldtake these differential gender categories into account.The LGBTQIA+ (Lesbian, Gay, Bi, Trans, Queer or Questioning, Intersex, Asex-ual and other sexual orientations (+)) community could particularly benefit from mentalhealth chatbots. Research has repeatedly shown that this group runs a higher risk of devel-oping a mental illness compared to heterosexual individuals [18, 50, 54, 65] since theystill face bullying, harassment and violence [50]. At the same time, LGBTQIA+ individ-uals often lack the necessary social support and psychological assistance to understandtheir feelings and inclinations or are ashamed to seek help themselves [50]. Conse-quently, LGBTQIA+ users distinguish themselves in terms of technology use, becausethey have a heightened need for a safe, non-judgemental (online) space. Especially whenthey do not receive enough support from their family or friends, they more often usetechnologies and online platforms to search for like-minded individuals and other typesof support. Also, they generally have a stronger urge for anonymity and therefore poten-tially a higher willingness to disclose to a chatbot [41]. Hence, this paper aims to answerthe overarching research question to what extent the (extended) UTAUT can predictthe behavioral intention to use a mental health chatbot among LGBTQIA+ individuals.To be able to test our hypotheses, as well as to answer the more explorative researchquestions, we chose a survey design. In doing so this study will provide a new perspec-tive on the inclusion of chatbot-specific variables and gender identities into traditionalcommunication models such as the UTAUT.2 Theoretical Background2.1 The UTAUTThe UTAUTwas initially proposed by Venkatesh and colleagues [60]. In developing thismodel, the authors combined concepts from eight user acceptance models, among othersthe technology acceptance model [14, 17], the theory of reasoned action [19] and theinnovation diffusion theory [49]. This way, Venkatesh et al. [60] created a unified andtheory-based model that predicts user acceptance. According to the original UTAUT,three core variables predict the behavioral intention (BI) to use a certain technology:Performance Expectancy (PE; i.e., how useful one thinks the technology will be), EffortExpectancy (EE; i.e., howeasyone expects the technology tobe) andSocial Influence (SI;whether onebelieves that one’s social environment thinks one should use the technology).Moreover, in the UTAUT, these three relationships are moderated by age, previousexperience with the technology (not for PE), gender [59] and voluntariness of use [60].In the current study, the updated variable for gender is included in the extended model,and voluntariness of use is omitted because in the current study it is a constant (i.e.,Understanding the Intention to Use Mental Health Chatbots 85our research focuses on the voluntary usage of mental health chatbots [7]). Thus, theUTAUT hypotheses are:H1: PE positively influences BI to use a mental health chatbot among the LGBTQIA+community and this relationship is moderated by (a) age.H2: EE positively influences BI to use a mental health chatbot among the LGBTQIA+community and this relationship is moderated by (a) age and (b) previous experience.H3: SI positively influences BI to use a mental health chatbot among the LGBTQIA+community and this relationship is moderated by (a) age and (b) previous experience.2.2 Extending the Model: Willingness to Self-disclose, Perceived Loss of Privacy,Trust and Gender IdentityWillingness to Self-Disclose. We define WSD as the willingness of LGBTQIA+ indi-viduals to entrust personal information to a mental health chatbot [15]. It has been sug-gested that mental health chatbots can be highly beneficial for self-disclosure becausethey provide an anonymous space without stigmatizing the user [3]. This is in linewith studies that indicate high WSD to an empathic chatbot [10, 25, 32, 58]. Lucasand colleagues found that participants showed less fear of self-disclosure, more intenseexpressions of emotions, and overall, a higher WSD with a computer system as opposedto a human operator [37]. On the other hand, the lack of human empathy might decreasepeople’s willingness to disclose personal information [15]. In any case, it is logical toassume that the higher the WSD, the higher the intention to use a mental health chatbot.Moderations. We expect the effect of WSD on Behavioral Intention (BI) to be strongerfor younger compared to older LGBTQIA+ individuals because younger people areoftenmore familiarwithmodern technology and thereforemore likely to entrust personalinformation to a mental health chatbot [51]. To our knowledge, previous research did notyet explore themoderating role of previous experience and gender identity in the relationbetween WSD and BI. Therefore, the present study answers the following researchquestions and tests one hypothesis.H4: WSD positively influences BI to use a mental health chatbot among the LGBTQIA+community.H4a: The relationship between WSD and BI is moderated by age, such that the effect isstronger for younger LGBTQIA+ individuals than for older LGBTQIA+ individuals.RQ1: To what extent does previous experience with chatbots moderate the relationshipbetween WSD and BI?RQ2: To what extent does gender identity moderate the relationship between WSD andBI?Perceived Loss of Privacy. Perceived loss of privacy (LOP) is defined as the extent towhich individuals think smart healthcare services such as mental health chatbots violatetheir privacy [36, 57]. In mobile health applications, where people disclose sensitivedata, privacy is an important aspect to consider. One study did not find LOP to be asignificant direct predictor of BI [36]. However, other studies did find a negative, directeffect of LOP on the acceptance of chatbot applications [35, 44].86 T. Henkel et al.Moderations. An explanation of these mixed findings can be found in people’s level ofexperience with technology. Privacy concerns decrease with more Internet experience[5] which is in line with the findings of Bergström, who found that with most Internetsituations, experienced people were less concerned [6]. Regarding the moderating effectof age, previous research has been inconclusive. Some studies found no differences dueto research measurements [26, 55] or only small significant differences with youngerpeople being more concerned about privacy [6]. Guo and colleagues found that theeffect of privacy concerns on BI is stronger for younger users, whereas older users werenot affected [24]. In contrast, Shehaan proposed different user typologies, with olderconsumers being more alarmed in contrast to younger users [53]. Accordingly, we testthe following hypotheses and aim to answer the following research questions:H5: LOP negatively influences BI.H5a: The relationship between LOP and BI is moderated by experience, such that theeffect is stronger for less experienced (compared to more experienced) LGBTQIA+individuals.RQ3: To what extent does age moderate the relationship between LOP and BI?RQ4: To what extent does gender identity moderate the relationship between LOP andBI?Trust. Trust in a chatbot is defined as the degree to which LGBTQIA+ individualsperceive mental health chatbots as dependable, reliable, and trustworthy in improvingone’s mental health [36]. Trust is a crucial factor for establishing strong bonds withsomeone and has been shown to be equally important when it comes to human-computerinteractions [15, 34]. Several studies indicate that trust is an antecedent for BI [36, 48].Moderations. Schroeder and Schroeder investigated factors that influence trust in chat-bots and found that individuals who are more experienced with chatbots and who areyounger are more likely to trust a chatbot [51]. Simultaneously, transgender individu-als often seek social support online [41]. Considering this unmet need and high onlinepresence, transgender individuals may perceive mental health chatbots more positively,which in turn might increase their trust to use such a chatbot. To our knowledge, nostudy has yet examined how gender identity moderates the relation between trust andBI. Therefore, we expect and propose the following:H6: Trust positively influences BI.H6a: The relationship between Trust and BI is moderated by age, such that the effect isstronger for younger (compared to older) LGBTQIA+ individuals.H6b: The relationship between Trust and BI is moderated by experience, such thatthe effect is stronger for more experienced (compared to less experienced) LGBTQIA+individuals.RQ5: To what extent does gender identity moderate the relationship between trust andBI?The proposed extension of the UTAUT to the context of mental health chatbotacceptance among the LGBTQIA+ community is depicted in Fig. 1.Understanding the Intention to Use Mental Health Chatbots 873 Method3.1 SamplingEthical approval was granted by the university’s Ethics Review Board (project ID: 2021-PC-14159). The questionnaire was created in English to reach LGBTQIA+ individualsof different nationalities. We used purposive convenience sampling by sharing the sur-vey on the first author’s social media as well as posting a recruitment text in relevantLGBTQ+ Facebook groups and Reddit threads. Also, flyers with the survey QR codewere spread at a Dutch university. Eligible participants were individuals older than16 years who (potentially) identify as LGBTQIA+. Participation was completely vol-untary and anonymous. Respondents were not compensated. Because of the length ofthe questionnaire (~10 min), the dropout rate was quite high (32,18%). In total, 354valid responses were gathered. However, four respondents did not give consent, sixteenparticipants did not consider themselves as part of the LGBTQIA+ community, and fourrespondents were aged below 16. These respondents, together with those who did notpass the attention check (n = 28), were excluded from the data set. Additionally, weomitted one case whose answers indicated zero variance (straight liner). This leaves afinal sample of N = 305 participants.3.2 PretestThe questionnaire was pre-tested with eight LGBTQIA+ individuals. Pre-testers indi-cated difficulties with imagining what a mental health chatbot would look like. Wetherefore included a screen recording of an existing mental health chatbot application(Wysa). In the 1min 42 s video, respondents saw an interaction withWysa, during whichthe chatbot explains the importance of mental resilience and sends motivational GIFs(graphics interchange format – a series of pictures that can be static or dynamic [22]) andempathetic messages. Furthermore, participants saw which answer options are providedfor the user (pre-selected or typing freely) and how a conversation with a mental healthchatbot works in general.3.3 ProcedureData were collected between 9th–17th December 2021. Participants who clicked on thesurvey link or scanned the QR code were exposed to the information letter in the surveytool Qualtrics. Afterwards, participants gave informed consent. If participants did notgive consent, they were automatically led to the end of the survey. All participantswho agreed to the research terms were asked whether they consider themselves partof the LGBTQIA+ community. This question served to the exclusion of heterosexualand cisgender individuals. Next, respondents indicated their gender identity, age, levelof education, mental health, and previous experience with chatbots. Respondents saw ashort description and examples of chatbots, and were asked how often they have usedthese different types of chatbots in the past. Subsequently, we described the concept ofa mental health chatbot and showed the video. After that, participants were exposed tothe items concerning PE, EE, SI, BI, LOP, trust and WSD.88 T. Henkel et al.3.4 MeasurementsAppendix 1 provides an overview of the original items and adjusted items. PE, EE,SI and BI were adapted from Venkatesh and colleagues’ validated and widely testedscales [60]. Participants’ WSD to a chatbot was adapted from Croes and Antheunis [15].The scales for perceived LOP and trust were adapted from Liu and Tao [36]. All latentconstructs were measured on a 7-point Likert scale ranging from 1 (Strongly Disagree)to 7 (Strongly Agree). Appendix 2 shows (very) high Cronbach’s α values as well asMand SD of the main variables.Age was measured with an open text entry and recoded into three groups (1 SDbelow average, average, and 1 SD above average).Previous experience with chatbots wasmeasured with the question: “How often haveyou used one of these chatbots in the past?” For customer service chatbots, healthcarechatbots, social messaging chatbots and other chatbots, respondents indicated their pre-vious experience on a 5-point Likert scale ranging from “Never” to “A lot of times (>20times)”.Gender identitywasmeasuredwith the question: “Which of the followingmost likelydescribes you?” Participants could choose between “Female”, “Male”, “Non-binary”,“Transgender”, “Intersex”, “Queer or Questioning”, “I prefer not to say” and a text fieldfor individual specification.Level of education was measured with the question “What is the highest degree orlevel of education you have completed?”, ranging from “No schooling completed” to“Doctoral or equivalent level”.Respondents also had to indicate whether they coped with mental health issues and,if so, whether they received professional help. Lastly, one attention check item (“Pleaseclick on ‘Agree’”) was included between the items addressing BI to check whetherrespondents paid attention throughout the questionnaire.3.5 AnalysisData analyses were carried out in SPSS. To describe the sample, a frequency analysiswas conducted. By creating a scatterplot and histogram of the residuals, the assumptionsof linearity and homoscedasticity were checked. Afterward, all predictors and moder-ators were mean-centered. This simplifies the interpretation of interaction effects: allcoefficients account for respondents who score average on the predictor variables. Sub-sequently, interaction variables were created to test moderation effects. All hypotheses,the moderating role of previous experience on the relationship between WSD and BI,and the moderating role of age in the relationship between LOP and BI were testedwith regression analyses. First, the traditional UTAUT variables were included as inde-pendent variables (PE, EE, SE). Second, age and experience were added as interactionvariables. Third, we included the new variables WSD, LOP, trust, and the interactionvariables (WSD, LOP, trust, and gender identity). This enabled a comparison betweenthe initial UTAUT and the extended model. To answer the RQs with gender identity,dummy variables for gender identity were created (i.e., female, male, trans, non-binary)with female participants as the reference group. Next, a linear regression model wasconducted, in which only PE, EE, SI, WSD, LOP, trust, the dummy variables for males,Understanding the Intention to Use Mental Health Chatbots 89trans and non-binary individuals, and lastly the interaction variables for the respectivepredictor*gender identity effects were included.4 Results4.1 Sample CharacteristicsAppendices 3 and 4 show the sample characteristics. Ages ranged from 16 to 59 years(M = 24.69; SD = 7.28). For gender identity, the largest category was female (43,60%,n= 133). 10,80% specified their gender identity in a separate text field. There, commonanswers were “Agender”, “Genderfluid” and “Questioning”. When it comes to previ-ous experience with chatbots, respondents had the most experience (= used a chatbotvery often, often or sometimes) with customer service chatbots (39,70%) and socialmessaging chatbots (22,60%), followed by healthcare chatbots (7,60%). Furthermore,most participants coped with mental health issues without receiving professional help(39,70%). Remarkably, only 12,80% stated to not cope with mental health issues at all.Regarding respondents’ level of education, the largest category was “completed uppersecondary level” (34,80%).4.2 Model Fit and Hypothesis TestingMain Effects. The extended regression model with BI to use a mental health chatbot asdependent variable, with PE, EE, SI, WSD, LOP and Trust as independent variables andwith age and previous experience as moderators was significant, F(31, 304) = 20.17, p< .001, and explained 69,60% of variance in BI to use a mental health chatbot. It alsodemonstrated a slightly better fit than the initial UTAUT, where only PE, EE and SI wereconsidered as predictors, F(16, 304)= 35.91, p< .001, R2 = 66.60% (see Appendix 5).The extended regression model can therefore be used to predict the BI to use a mentalhealth chatbot among the LGBTQIA+ population.Only the effects for PE, EE, SI and WSD were significant. PE showed a significant,strong association with BI (b = 0.67, t = 11.70, p < .001, 95% CI [0.56, 0.79]). Thisindicates that people who believe that a mental health chatbot will help them increasetheir mental wellbeing, have a higher intention to use a mental health chatbot. Similarly,SI, b = 0.18, t = 3.29, p = .001, 95% CI [0.07, 0.28] showed a significant, weakassociation with BI. Hence, people who are more influenced by their social environmenthave a higher intention of using one. WSD showed a significant, weak association withBI (b = 0.21, t = 3.69, p < .001, 95% CI [0.10, 0.32]). We therefore found support forH1, H3 and H4.Surprisingly, EE showed a weak, negative relationship (b = −0.14, t = −2.40, p= .017, 95% CI [0.08, 0.29]), which is opposed to what we expected. This indicatesthat, the more people perceive a mental health chatbot as easy to use, the lower is theirintention to use such a chatbot. We therefore reject H2. Further, the results show thatLOP (b = 0.03, t = 0.91, p = .362, 95% CI [−0.04, 0.11]) and Trust (b = −0.03, t =−0.42, p = .672, 95% CI [−0.14, 0.09]) are no significant predictors of chatbot usage.Thus, H5 and H6 were rejected.90 T. Henkel et al.Moderating Effects. In terms of interaction effects, we found only three weak, sig-nificant interaction effects. Firstly, the effect of SI on BI is moderated by previousexperience with healthcare chatbots (b = −0.16, t = −2.14, p = .033, 95% CI [−0.31,−0.01]). This means that the effect of SI on BI becomes weaker the more experienceLGBTQIA+ individuals have with healthcare chatbots. However, this is only the casefor previous experience with healthcare chatbots. Previous experience with customerservice or messaging chatbots were no significant moderators. Thus, we found partialsupport for H3b.Secondly, the effect of WSD on BI seems to be very weakly moderated by age (b= −0.02, t = −2.43, p = .016, 95% CI [−0.04, −0.004]). As hypothesized, the effectis stronger for younger compared to older LGBTQIA+ individuals. H4a was thereforesupported.Thirdly, the relationship between LOP and BI was significantly and weakly moder-ated by gender identity, where the effect seems to be stronger for male individuals (b =0.19, t = 2.09, p = .037, 95% CI [0.01, 0.36]) than for females (RQ4).All other interactions turned out to be insignificant, which means H1a, H2a, H2b,H3a, H5a, H6a and H6b are rejected. In addition, previous experience with a chatbot isnot a significant moderator for the relationship betweenWSD and BI (RQ2), and we didnot find support for any other moderating effects of gender identity (RQ3, RQ4, RQ5).Figure 1 shows the significant relationships in the extended model.Fig. 1. Significant relationships in the extended modelUnderstanding the Intention to Use Mental Health Chatbots 915 DiscussionThis study aimed to take a critical perspective on the UTAUT by exploring whether itcan be tested and extended in the context of mental health chatbot usage intention amongLGBTQIA+ individuals. Through integrating the chatbot-specific variables willingnessto self-disclose (WSD), perceived loss of privacy (LOP), and trust, and by consideringgender identity, we were able to demonstrate that the extended UTAUT provides a betterunderstanding of mental health chatbot usage intention among LGBTQIA+ individualsthan the original model. Our findings do not only contribute to more inclusive technol-ogy acceptance models and the generalizability of the UTAUT, but also give valuableinsights into which aspects influence the intention to use a mental health chatbot amongLGBTQIA+ individuals.In the current survey, performance expectancy (PE), Social Influence (SI) and WSDsignificantly predicted behavorial intention (BI) to use a mental health chatbot. Unsur-prisingly, PE has shown to be the strongest positive predictor. PE has repeatedly been animportant predictor for technology acceptance in previous research [3, 56, 60]. Hence,the belief that a mental health chatbot would improve their mental health seems to bea crucial driver for the BI to use a mental health chatbot among LGBTQIA+ individu-als and should be highlighted in future chatbot interventions. Additionally, in line withprior research, the more a LGBTQIA+ individual believes that their social environmentthinks they should use a mental health chatbot (SI), the higher is their BI [56, 62]. Thiseffect seems to be stronger for less experienced people, which means that particularlywhen individuals have little experience, their social environment can have a significantimpact on their BI to use a mental health chatbot. It is worth noting that the effect ofSI on BI was very weak and, considering the strong community feeling of LGBTQIA+individuals, we expected this effect to be stronger. Especially since a study by Fish andcolleagues demonstrated that emotional and mental health topics were the most popu-lar themes discussed in a chat-based Internet community support programme [18], thusLGBTQIA+ individuals are generally willing to discuss mental health problems withtheir peers. It might be that the usage of mental health chatbots is not as widespread asonline communities [41] and that therefore SI is less important for the BI to use men-tal health chatbots. Overall, for future interventions, developers should emphasize thepotential benefits mental health chatbots have to improve mental health issues amongLGBTQIA+ individuals. In addition, social influence and community aspects should betaken into account, and people’s willingness to self-disclose should also be consideredas a crucial determinant for mental health chatbot usage intention.We did not expect the negative relationship between effort expectancy (EE) andBI. The easier the usage of a mental health chatbot seems, the lower is LGBTQIA+individuals’ intention to use it. This negative direction is contradicting existing literature.Some studies found a significant relationship [3, 29, 61, 62] and others did not find asignificant association due to common use of the technology under study [56]. However,all studies demonstrated a positive relation instead of a negative one. One explanationfor the current findings can be that our results show a high mean EE, which suggeststhat many participants perceived a mental health chatbot as easy to use anyway. Anotherpossible explanation could be that one of the contextual variables were suppressing theeffect of EE since it only became significant when the other variables were added.92 T. Henkel et al.Surprisingly, two chatbot-related variables -perceived loss of privacy and trust- wereno significant predictors of chatbot usage intention. Especially the results regarding trustdo not align with prior research. For Liu and Tao, for instance, trust was the strongestpredictor for BI to use a smart healthcare system [36]. Other studies have establishedtrust as a crucial antecedent for chatbot acceptance [43, 44]. A plausible explanationcould be that LOP and trust did not directly affect BI to use a mental health chatbot, butindirectly viaWSD. Schroeder and Schroeder found that trust positively influencesWSDto a chatbot [51]. Similarly, lower privacy concerns seem to increase trust in chatbots[24]. Since WSD directly influenced BI, future studies may consider LOP as antecedentof trust, and trust as predictor of WSD rather than direct predictors of BI.Moreover, this paper emphasized the importance of including gender identity into theUTAUT. Interestingly, the study did not find support for substantial differences amonggender identities. Apparently, LGBTQIA+ regardless of their gender identity, perceivemental health chatbots equally, even though transgender andnon-binary individuals showa higher online behavior compared to female or male individuals [41]. Only the effectof LOP on BI was stronger for males than females. This is interesting, as prior researchon online behavior revealed that women are more concerned about their privacy [27,53]. But then again, gender differences measured with non-LGBTQIA+ samples maydeviate from our sample. The present findings would suggest that future mental healthinterventions for LGBTQIA+ individuals do not need to consider different factors forrespective gender identities —except stressing privacy protection more among maleindividuals—, but this seems overly simplistic. The blurring boundaries between genderidentities that prevail in our current-day society do ask for an increased attention to thisin chatbot research, especially when the technology relates to mental health.In line with previous research, this study shows that age is a factor to keep takinginto account. While the relationship between WSD and BI remains equal for olderLGBTQIA+ individuals, younger individuals have a higher intention to use a mentalhealth chatbot when their WSD is also high. However, our sample was quite young (Ø24 years) and the amount of participants age> 40 years was rather limited. Thus, futureresearch needs to pay more explicit attention to the age factor.5.1 Limitations and Future ResearchOne major issue regarding the survey was that participants did not interact with a mentalhealth chatbot themselves. Unfortunately, it was not feasible to develop a properly func-tioning mental health chatbot in the available time frame, and using existing chatbotslike Wysa would have created privacy issues by involving third parties. At the sametime, 92% of the participants had never or rarely used a mental health chatbot before,and thus must have had a hard time imagining such an interaction, which could have ledto imprecise answers. This problem was already raised during the pre-test, which is whywe included a screen recording of a mental health chatbot conversation. Yet, we had nocontrol over whether participants actually watched this video.Secondly, actual usage of mental health chatbots was not included as a dependentvariable. A follow-up study could let participants test a mental health chatbot and, at theend of the study, provide a link to the chatbot application free for them to use. MeasuringUnderstanding the Intention to Use Mental Health Chatbots 93the click rate may reveal insights into the actual usage of the chatbot and lead to moreprecise results.Lastly, as this research topic has not been researched in depth so far, researchersshould consider applying a qualitative research design to gain an in-depth understandingof LGBTQIA+ individuals’ thoughts on mental health chatbots. Interestingly, especiallyon reddit, the recruitment text for this study caused elaborate discussions about whetherindividuals would use such chatbot or not (Tables 1, 2, 3, 4 and 5).Appendix 1Table 1. Operationalization of predictors and behavioral intentionVariable Items as used in previous literature[15, 36, 60]Adjusted items used in the currentstudyPerformanceexpectancy [60]PE1: I would find the system usefulin my jobPE2: Using the system increases myproductivityPE3: Using the system enables me toaccomplish tasks more quicklyPE4: If I use the system, I willincrease my chances of getting araisePE1: I would find such a mentalhealth chatbot useful in my daily lifePE2: Using such a mental healthchatbot would improve my mentalhealthPE3: Using such a mental healthchatbot would help me to improve mymental health more quicklyPE4: Using such a mental healthchatbot improves my mentalwell-beingEffortexpectancy [60]EE1: Learning to operate the systemis easy for meEE2: My interaction with the systemwould be clear and understandableEE3: I would find the system easy touseEE4: It would be easy for me tobecome skilful at using the systemEE1: Learning how to use such amental health chatbot is easy for meEE2: My interaction with such amental health chatbot would be clearand understandableEE3: I would find such a mentalhealth chatbot easy to useEE4: It would be easy for me tobecome skilful at using such a mentalhealth chatbotSocialinfluence [60]SI1: People who are important to methink that I should use the systemSI2: People who influence mybehavior think that I should use thesystemSI3: In general, the organization hassupported the use of the systemSI4: The senior management of thisbusiness has been helpful in the useof the systemSI1: People who are important to methink that I should use such a mentalhealth chatbotSI2: People who influence mybehavior think that I should use sucha mental health chatbotSI3: In general, my socialenvironment would support the use ofsuch a mental health chatbot(continued)94 T. Henkel et al.Table 1. (continued)Variable Items as used in previous literature[15, 36, 60]Adjusted items used in the currentstudyWillingness toself-disclose[15]WSD1: During the conversation Iwas able to share personalinformation about myselfWSD2: During the conversation Ifelt comfortable sharing personalinformationWSD3: During the conversation itwas easy to share personalinformationWSD4: During the conversation Ifelt that I could be openWSD1: I feel I could share personalinformation about myself with such amental health chatbotWSD2: I feel I would be comfortablesharing personal information withsuch a mental health chatbotWSD3: I feel it would be easy toshare personal information with sucha mental health chatbotWSD4: I feel that I could be openduring a conversation with such amental health chatbotWSD5: How likely are you to confidein an anonymous chatbot for mentalhealth issues?Perceived lossof privacy [36]LOP1: I am concerned that smarthealthcare services will collect toomuch personal information from meLOP2: I am concerned that smarthealthcare services will use mypersonal information for otherpurposes without my authorizationLOP3: I am concerned that smarthealthcare services will share mypersonal information with otherentities without my authorizationLOP1: I am concerned that such amental health chatbot will collect toomuch personal information from meLOP2: I am concerned that such amental health chatbot will use mypersonal information for otherpurposes without my authorizationLOP3: I am concerned that such amental health chatbot will share mypersonal information with otherentities without my authorizationTrust [36] TRU1: Smart healthcare services aredependableTRU2: Smart healthcare services arereliableTRU3: Overall, I can trust smarthealthcare servicesTRU1: Such a mental health chatbotis dependableTRU2: Such a mental health chatbotis reliableTRU3: Overall, I can trust such amental health chatbotBehavioralintention [60]BI1: I intent to use the system in thenext <n> monthsBI2: I predict I would use the systemin the next <n> monthsBI3: I plan to use the system in thenext <n> monthsBI1: I intend to use such a mentalhealth chatbot in the futureBI2: I will try to use such a mentalhealth chatbot in my daily lifeBI3: I plan to use such a mentalhealth chatbot frequentlyUnderstanding the Intention to Use Mental Health Chatbots 95Appendix 2Table 2. Eigenvalues, explained variance, Cronbach’s α, means and standard deviation of mainvariablesVariable Eigenvalue % of Variance Cronbach’s α Mean SDPerformance expectancy 3.43 85.65% .94 4.12 1.43Effort expectancy 2.69 67.34% .83 5.23 1.11Social influence 2.22 73.98% .81 3.44 1.23Willingness to self-disclose 3.94 78.82% .93 4.10 1.62Loss of privacy 2.78 92.60% .96 4.81 1.70Trust 2.25 74.88% .83 4.16 1.31Behavioral intention 2.73 91.13% .95 3.37 1.55Note. Factor analysis with direct oblimin rotation was used; M and SD refer to the mean variablesAppendix 3Table 3. Characteristics of the sample (N = 305)Characteristics N (%)Age16–23 157 (51,5%)24–30 89 (29,2%)31–35 37 (12,2%)36–40 10 (3,3%)>40 12 (3,8%)Gender IdentityMale 67 (22,0%)Female 133 (43,6%)Non-Binary 54 (17,7%)Transgender 13 (4,3%)Intersex 0 (0%)Other 33 (10,8%)Level of EducationNo schooling completed 3 (1,0%)Lower secondary level 31 (10,2%)(continued)96 T. Henkel et al.Table 3. (continued)Characteristics N (%)Upper secondary level 106 (34,8%)Vocational training 13 (4,3%)Bachelor’s or equivalent 96 (31,5%)Master’s or equivalent 36 (11,8%)Doctoral or equivalent 6 (2,0%)Other 9 (3,0%)Mental Health IssuesYes, receive professional help 94 (30,8%)Yes, do not receive professional help 121 (39,7%)No 39 (12,8%)I am not sure 51 (16,7%)Appendix 4Table 4. Frequency distribution for previous experience with chatbots (N = 305)Type of Chatbot Never Rarely Sometimes Often Very oftenCustomer service chatbots 82 (26,9%) 102 (33,4%) 97 (31,8%) 17 (5,6%) 7 (2,3%)Healthcare chatbots 224 (73,4%) 58 (19,0%) 15 (4,9%) 6 (2,0%) 2 (0,7%)Social messaging chatbots 152 (49,8%) 84 (27,5%) 46 (15,1%) 12 (3,9%) 11 (3,6%)Understanding the Intention to Use Mental Health Chatbots 97Appendix 5Table 5. Comparison of regression models to predict BI of mental health chatbot usageBehavioral intention to use mental health chatbotsUTAUT model Extended modelConstant 3.36*** 3.35***Performance expectancy 0.75*** 0.67***Effort expectancy −0.08 −0.14*Social influence 0.25*** 0.18**Willingness to self-disclose 0.19**Perceived loss of privacy 0.04Trust −0.01R2 0.67 0.70F 35.91*** 20.17***Note. * p < .05, ** p < .01, *** p < .001References1. Abd-alrazaq, A.A., Alajlani, M., Alalwan, A.A., Bewick, B.M., Gardner, P., Househ, M.: Anoverviewof the features of chatbots inmental health: a scoping review. Int. J.Med. Informatics132, 103978 (2019)2. Ajzen, I.: The theory of planned behavior. Organ. Behav.Hum.Decis. Process. 50(2), 179–211(1991)3. Almahri, F.A.J., Bell, D., Merhi, M.: Understanding student acceptance and use of chatbotsin the United Kingdom universities: a structural equation modelling approach. In: 2020 6thInternational Conference on Information Management (ICIM) (2020)4. Barak, A., Gluck-Ofri, O.: Degree and reciprocity of self-disclosure in online forums.Cyberpsychol. Behav. 10(3), 407–417 (2007)5. Bellman, S., Johnson, E.J., Kobrin, S.J., Lohse, G.L.: International differences in informationprivacy concerns: a global survey of consumers. Inf. Soc. 20(5), 313–324 (2004)6. Bergström, A.: Online privacy concerns: a broad approach to understanding the concerns ofdifferent groups for different uses. Comput. Hum. Behav. 53, 419–426 (2015)7. Brandtzaeg, P.B., Følstad, A.: Why people use chatbots. In: Kompatsiaris, I., et al. (eds.)INSCI 2017. LNCS, vol. 10673, pp. 377–392. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-70284-1_308. Carpenter,M.: The human rights of intersex people: addressing harmful practices and rhetoricof change. Reprod. Health Matters 24(47), 74–84 (2016)9. Carpenter, C.S., Eppink, S.T., Gonzales, G.: Transgender status, gender identity, andsocioeconomic outcomes in the United States. ILR Rev. 73(3), 573–599 (2020)10. Chaix, B., et al.: When chatbots meet patients: One-year prospective study of conversationsbetween patients with breast cancer and a chatbot. JMIR Cancer 5(1), e12856 (2019)98 T. Henkel et al.11. Chang, I.C., Hwang, H.G., Hung,W.F., Li, Y.C.: Physicians’ acceptance of pharmacokinetics-based clinical decision support systems. Expert Syst. Appl. 33(2), 296–303 (2007)12. Chazin, D., Klugman, S.: Clinical considerations in working with clients in the coming outprocess. Pragmat. Case Stud. Psychother. 10(2), 132–146 (2014)13. Cheung, A.S., et al.: Non-binary and binary gender identity in Australian trans and genderdiverse individuals. Arch. Sex. Behav. 49(7), 2673–2681 (2020)14. Chocarro, R., Cortiñas, M., Marcos-Matás, G.: Teachers’ attitudes towards chatbots in edu-cation: a technology acceptance model approach considering the effect of social language,bot proactiveness, and users’ characteristics. Educ. Stud. 1–19 (2021)15. Croes, E.A.J., Antheunis, M.L.: 36 questions to loving a chatbot: Are people willing toself-disclose to a chatbot? In: Chatbot Research and Design, pp. 81–95 (2021)16. D’Alfonso, S.: AI in mental health. Curr. Opin. Psychol. 36, 112–117 (2020)17. Davis, F.D.: Perceived usefulness, perceived ease of use, and user acceptance of informationtechnology. MIS Q. 13(3), 319–339 (1989)18. Fish, J.N., et al.: Q chat space: assessing the feasibility and acceptability of an Internet-basedsupport program for LGBTQ youth. Prevention Sci. 23, 130–141 (2021)19. Fishbein, M., Ajzen, I.: Belief, Attitude, Intention and Behavior: An Introduction to Theoryand Research. Addison-Wesley, Reading (1975)20. Fitzpatrick, K.K., Darcy, A., Vierhile, M.: Delivering cognitive behavior therapy to youngadults with symptoms of depression and anxiety using a fully automated conversational agent(Woebot): a randomized controlled trial. JMIR Mental Health 4(2), e19 (2017)21. Fulmer, R., Joerin, A., Gentile, B., Lakerink, L., Rauws, M.: Using psychological artificialintelligence (Tess) to relieve symptoms of depression and anxiety: randomized controlledtrial. JMIR Mental Health 5(4), e64 (2018)22. GIF|Definition, Meaning, & Facts. Encyclopedia Britannica (n.d.). https://www.britannica.com/technology/GIF23. Goklani, B.: Chatbots in healthcare: top benefits, risks and challenges you need to know.Mindinventory, 15 September 2021. https://www.mindinventory.com/blog/chatbots-in-healthcare/24. Guo, X., Zhang, X., Sun, Y.: The privacy–personalization paradox in mHealth servicesacceptance of different age groups. Electron. Commer. Res. Appl. 16, 55–65 (2016)25. Ho, A., Hancock, J., Miner, A.S.: Psychological, relational, and emotional effects of self-disclosure after conversations with a chatbot. J. Commun. 68(4), 712–733 (2018)26. Hoofnagle, C.J., King, J., Li, S., Turow, J.: How different are young adults from older adultswhen it comes to information privacy attitudes and policies? (SSRN Scholarly Paper No. ID1589864). Social Science Research Network, Rochester, NY (2010)27. Hoy, M.G., Milne, G.: Gender differences in privacy-related measures for young adultfacebook users. J. Interact. Advert. 10(2), 28–45 (2010)28. Inkster, B., Sarda, S., Subramanian, V.: An empathy-driven, conversational artificial intelli-gence agent (Wysa) for digital mental well-being: real-world data evaluation mixed-methodsstudy. JMIR mHealth and uHealth 6(11), e12106 (2018)29. Isaias, P., Reis, F., Coutinho, C., Lencastre, J.A.: Empathic technologies for distance/mobilelearning. Interact. Technol. Smart Educ. 14(2), 159–180 (2017)30. Jackson, S.D.: “Connection is the antidote”: psychological distress, emotional processing, andvirtual community building amongLGBTQ students after theOrlando shooting. Psychol. Sex.Orientat. Gend. Divers. 4(2), 160–168 (2017)31. Kretzschmar, K., Tyroll, H., Pavarini, G., Manzini, A., Singh, I.: Can your phone be yourtherapist? Young people’s ethical perspectives on the use of fully automated conversationalagents (chatbots) in mental health support. Biomed. Informatics Insights 11, 1–9 (2019)Understanding the Intention to Use Mental Health Chatbots 9932. Lee, Y.C., Yamashita, N., Huang, Y.: Designing a chatbot as a mediator for promoting deepself-disclosure to a real mental health professional. Proc. ACM Human-Comput. Interact.4(CSCW1), 1–27 (2020)33. Lee, Y.C., Yamashita, N., Huang, Y., Fu, W.: “I Hear You, I Feel You”: encouraging deepself-disclosure through a chatbot. In: Proceedings of the 2020 CHI Conference on HumanFactors in Computing Systems (2020)34. Lee, J.D., See, K.A.: Trust in automation: Designing for appropriate reliance. Hum. Factors46(1), 50–80 (2004)35. Lipschitz, J., et al.: Adoption ofmobile apps for depression and anxiety: cross-sectional surveystudy on patient interest and barriers to engagement. JMIRMental Health 6(1), e11334 (2019)36. Liu, K., Tao, D.: The roles of trust, personalization, loss of privacy, and anthropomorphismin public acceptance of smart healthcare services. Comput. Hum. Behav. 127, 107026 (2022)37. Lucas, G.M., Gratch, J., King,A.,Morency, L.P.: It’s only a computer: virtual humans increasewillingness to disclose. Comput. Hum. Behav. 37, 94–100 (2014)38. Ly, K.H., Ly, A.M., Andersson, G.: A fully automated conversational agent for promotingmental well-being: a pilot RCT using mixed methods. Internet Interv. 10, 39–46 (2017)39. Magsamen-Conrad, K., Upadhyaya, S., Joa, C.Y., Dowd, J.: Bridging the divide: usingUTAUT to predict multigenerational tablet adoption practices. Comput. Hum. Behav. 50,186–196 (2015)40. Mandal, D., McQueen, R.J.: Extending UTAUT to explain social media adoption bymicrobusinesses. Int. J. Managing Inf. Technol. (IJMIT) 4(4), 1–11 (2012)41. McInroy, L.B., Craig, S.L., Leung, V.W.Y.: Platforms and patterns for practice: LGBTQ+youths’ use of information and communication technologies. Child Adolesc. Soc. Work J.36(5), 507–520 (2018)42. Melián-González, S., Gutiérrez-Taño, D., Bulchand-Gidumal, J.: Predicting the intentions touse chatbots for travel and tourism. Curr. Issue Tour. 24(2), 192–210 (2019)43. Mostafa, R.B., Kasamani, T.: Antecedents and consequences of chatbot initial trust. Eur. J.Mark. 56, 1748–1771 (2021)44. Nadarzynski, T., Miles, O., Cowie, A., Ridge, D.: Acceptability of artificial intelli-gence (AI)-led chatbot services in healthcare: a mixed-methods study. Digital Health 5,2055207619871808 (2019)45. Neufeld, D.J., Dong, L., Higgins, C.: Charismatic leadership and user acceptance ofinformation technology. Eur. J. Inf. Syst. 16(4), 494–510 (2007)46. Pennebaker, J.W.: Emotion, disclosure, and health: an overview. In: Emotion, Disclosure, &Health, pp. 3–10 (1995)47. Powell, J.: Trust me, I’m a chatbot: how artificial intelligence in health care fails the Turingtest. J. Med. Internet Res. 21(10), e16222 (2019)48. Prakash, A.V., Das, S.: Intelligent conversational agents in mental healthcare services: athematic analysis of user perceptions. Pacific Asia J. Assoc. Inf. Syst. 12(2), 1–34 (2020)49. Rogers, E. Diffusion of Innovations. Free Press, New York (1995)50. Russell, S.T., Fish, J.N.: Mental health in lesbian, gay, bisexual, and transgender (LGBT)youth. Annu. Rev. Clin. Psychol. 12(1), 465–487 (2016)51. Schroeder, J., Schroeder, M.: Trusting in machines: how mode of interaction affects will-ingness to share personal information with machines. In: Proceedings of the 51st HawaiiInternational Conference on System Sciences, Hawaii (2018)52. Schueller, S.M., Neary, M., O’Loughlin, K., Adkins, E.C.: Discovery of and interest in healthapps among those with mental health needs: survey and focus group study. J. Med. InternetRes. 20(6), e10141 (2018)53. Sheehan, K.B.: Toward a typology of Internet users and online privacy concerns. Inf. Soc.18(1), 21–32 (2002)100 T. Henkel et al.54. Steele, L.S., et al.: LGBT identity, untreated depression, and unmet need for mental healthservices by sexual minority women and trans-identified people. J. Women’s Health 26(2),116–127 (2017)55. Taddicken, M.: The ‘privacy paradox’ in the social web: the impact of privacy concerns, indi-vidual characteristics, and the perceived social relevance on different forms of self-disclosure.J. Comput.-Mediat. Commun. 19(2), 248–273 (2013)56. Tarhini, A., El-Masri, M., Ali, M., Serrano, A.: Extending the UTAUT model to understandthe customers’ acceptance and use of internet banking in Lebanon. Inf. Technol. People 29(4),830–849 (2016)57. Toch, E., Wang, Y., Cranor, L.F.: Personalization and privacy: a survey of privacy risks andremedies in personalization-based systems. User Model. User-Adap. Inter. 22(1–2), 203–220(2012)58. van Wezel, M.M.C., Croes, E.A.J., Antheunis, M.L.: “I’m here for you”: can social chatbotstruly support their users? A literature review. In: Følstad, A. et al. (eds.) Chatbot Researchand Design: Fourth International Workshop, CONVERSATIONS 2020, pp. 96–113 (2021)59. Venkatesh, V., Morris, M.G., Ackerman, P.L.: A longitudinal field investigation of gender dif-ferences in individual technology adoption decision-making processes. Organ. Behav. Hum.Decis. Process. 83(1), 33–60 (2000)60. Venkatesh, V., Morris, M.G., Davis, G.B., Davis, F.D.: User acceptance of informationtechnology: toward a unified view. MIS Q. 27(3), 425–478 (2003)61. Venkatesh, V., Sykes, T.A., Zhang, X.: “Just what the doctor ordered”: a revised UTAUT forEMR system adoption and use by doctors. In: 2011 44th Hawaii International Conference onSystem Sciences (2011)62. Venkatesh, V., Thong, J.Y.L., Xu, X.: Consumer acceptance and use of information technol-ogy: extending the unified theory of acceptance and use of technology.MISQ. 36(1), 157–178(2012)63. Woebot Health. Relational agent for mental health, 12 January 2022. https://woebothealth.com/. Accessed 31 Jan 202264. Mental health support, for everyone. Wysa (2021). https://wysa.io/. Retrieved 27 Jan 202265. Yarns, B.C., Abrams, J.M., Meeks, T.W., Sewell, D.D.: The mental health of older LGBTadults. Curr. Psychiatry Rep. 18(6), 1–11 (2016)66. Yi,M.Y., Jackson, J.D., Park, J.S., Probst, J.C.: Understanding information technology accep-tance by individual professionals: toward an integrative view. Inf. Manage. 43(3), 350–363(2006)",
    "id": 571475019,
    "identifiers": {
        "doi": "10.1007/978-3-031-25581-6_6",
        "oai": "oai:dare.uva.nl:publications/d576c339-9d80-40bd-8112-eb55c1e694c7"
    },
    "title": "Understanding the intention to use mental health chatbots among LGBTQIA+ individuals:Testing and extending the UTAUT",
    "language": {
        "code": "en",
        "name": "English"
    },
    "publishedDate": "2023-01-01T00:00:00+00:00",
    "publisher": null,
    "references": [],
    "sourceFulltextUrls": [
        "https://pure.uva.nl/ws/files/122966118/Understanding_the_intention_to_use_mental_health_chatbots_among_LGBTQIA_individuals.pdf"
    ],
    "updatedDate": "",
    "yearPublished": "2023",
    "links": [
        {
            "type": "download",
            "url": "https://core.ac.uk/download/571475019.pdf"
        },
        {
            "type": "reader",
            "url": "https://core.ac.uk/reader/571475019"
        },
        {
            "type": "thumbnail_m",
            "url": "https://core.ac.uk/image/571475019/medium"
        },
        {
            "type": "thumbnail_l",
            "url": "https://core.ac.uk/image/571475019/large"
        },
        {
            "type": "display",
            "url": "https://core.ac.uk/outputs/571475019"
        }
    ],
    "abstract": "This empirical study aims to test and extend the unified theory of acceptance and use of technology (UTAUT) in the context of mental health chatbot usage among LGBTQIA+ individuals. The proposed model uses UTAUT variables (performance expectancy, effort expectancy and social influence) as well as chatbot-related variables (willingness to self-disclose, perceived loss of privacy, and trust) to predict the intention to use a mental health chatbot. The online survey (N = 305) indicates that performance expectancy, social influence, and willingness to self-disclose positively predict chatbot usage intention, whereas effort expectancy negatively influences this intention. Moreover, previous experience with healthcare chatbots moderated the relationship between social influence and intention, age moderated the relationship between willingness to self-disclose and intention, and gender identity moderated the relationship between perceived loss of privacy and intention. Overall, the extended UTAUT proved to be useful in explaining technology acceptance of mental health chatbots among the LGBTQIA+ community",
    "tags": [
        "bookPart"
    ],
    "fulltextStatus": "enabled",
    "subjects": [
        "bookPart"
    ],
    "oai": "oai:dare.uva.nl:publications/d576c339-9d80-40bd-8112-eb55c1e694c7",
    "deleted": "ALLOWED",
    "disabled": false,
    "journals": null,
    "repositories": {
        "id": "15629",
        "openDoarId": 0,
        "name": "International Migration, Integration and Social Cohesion online publications",
        "urlHomepage": null,
        "uriJournals": null,
        "physicalName": "noname",
        "roarId": 0,
        "baseId": 0,
        "pdfStatus": null,
        "nrUpdates": 0,
        "lastUpdateTime": null
    },
    "repositoryDocument": {
        "id": 571475019,
        "depositedDate": null,
        "publishedDate": "2023-01-01T00:00:00+00:00",
        "updatedDate": "2024-01-23T23:49:46+00:00",
        "acceptedDate": null,
        "createdDate": "2023-07-08T23:21:47+01:00"
    },
    "urls": [
        "https://dare.uva.nl/personal/pure/en/publications/understanding-the-intention-to-use-mental-health-chatbots-among-lgbtqia-individuals(d576c339-9d80-40bd-8112-eb55c1e694c7).html",
        "https://doi.org/10.1007/978-3-031-25581-6_6",
        "https://hdl.handle.net/11245.1/d576c339-9d80-40bd-8112-eb55c1e694c7",
        "https://pure.uva.nl/ws/files/122966118/Understanding_the_intention_to_use_mental_health_chatbots_among_LGBTQIA_individuals.pdf"
    ],
    "lastUpdate": "2024-01-23T23:49:46+00:00",
    "setSpecs": []
}